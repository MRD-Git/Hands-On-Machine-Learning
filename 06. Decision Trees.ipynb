{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "page 167<br>\n",
    "See\n",
    "- https://github.com/ageron/handson-ml/blob/master/06_decision_trees.ipynb for code and further information,\n",
    "- https://guide.macports.org/#using.common-tasks for the installation of graphviz (here particularly \"sudo port install ...\" and \"sudo port search ...\"),\n",
    "- https://www.graphviz.org and page 168 of the book for instructions on how to use graphviz, and\n",
    "- https://stackoverflow.com/questions/10628262/inserting-image-into-ipython-notebook-markdown for information on how to include an image in Jupyter Markdown. The relevant command, starting from the folder that contains the Jupyter notebook is *<\"XXX\">*, where \"XXX\" is *img src=\"imagepath\"* (see example below).\n",
    "\n",
    "Like SVMs, decision trees are very versatile and can be used for classifcation as well as for Regression. They are fundamental component of random forests, see chapter 7.\n",
    "## Training and Visualizing a Decision Tree\n",
    "page 167<br>\n",
    "Let's build and display a decision tree so we can better understand how they work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is trained. Now, let's visualize it using graphviz! The code below first defines the path for saving the file \"iris_tree.dot\", which will be produced in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine where to save the figures\n",
    "import os\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id)\n",
    "# from sklearn.tree, use the export_graphviz command to export the structure of the decision tree into a dot file ...\n",
    "# ... under the path determined above\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=image_path(\"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal and use the \"cd ...\" and \"ls\" commands to navigate to the folder containing the file \"iris_tree.dot\". Then, run the command \"dot -Tpng iris_tree.dot -o iris_tree.png\" to have graphviz make a png file out of the dot file. That dot file can be displayed using the code explained at the top of this notebook. \n",
    "<img src=\"images/iris_tree.png\">\n",
    "## Making Predictions\n",
    "The classification process starts at the top *root node* (depth=0) and proceeds to its child nodes. One of them is a *leaf node*, i.e., it does not split further. The other nodes splits into two child nodes, both of which are leaf nodes. The nodes' attributes' meanings are as follows. The number of samples that fall into the node are specified by *samples*. Their distribution over all possible classes is described by *values*. The *gini* attribute measures the *gini impurity*,\n",
    "$$G_i=1-\\sum_{k=1}^np_{i,k}^2\\,,$$\n",
    "of the node. Here, $p_{i,k}=\\text{value}_{k,i}/\\text{samples}_i$ is the ratio of class $k$ instances among the training instances of the $i$-th node.<br><br>\n",
    "**General note**<br>\n",
    "One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don't require feature scaling or centering at all.<br><br>\n",
    "**General note**<br>\n",
    "Scikit-Learn uses the CART algorithm, which produces only *binary trees*: nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can procuce Decision Trees with nodes that haver more than two children.<br><br>\n",
    "**Model Interpretation: White Box Versus Black Box**<br>\n",
    "As you can see Decisison Trees are fairly intuitive and their decisions are easy to interpret. Such models are often called *white box modles*. In contrast, as we will see, Random Forests or neural networks are generally considered *black box models*. They make great predictions, and you can easily check the calculation that they performed to make  these predictions; nevertheless, it is usually hard to explain in simple terms why the predictions were made. For example, if a neural network says that a particular person appears on a picture, it is hard to know what actually contributed to this  prediction: did the model recognize that person's eyes? Her mouth? Her nose? Her shoes? Or even the couch that she was sitting on? Converesy, Decision Trees provide nice and simple classification rules that can even be applied manually if need be (e.g., for flower classification).\n",
    "## Estimating Class Probabilities\n",
    "The probability $p_{i,k}=\\text{value}_{k,i}/\\text{samples}_i$ can not only be used for the Gini impurity but also to output the probability that some - possibly new - instance belongs to a certain class: just go through the trained Decision Tree using the features of the new instance and arrive at node $i$. Then $p_{i,k}$ are the class probabilities for that instance. This is coded as easy as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.90740741 0.09259259]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "new_instance = [5, 1.5]                       # new instance with petal length = 5 cm and petal width = 1.5 cm\n",
    "print(tree_clf.predict_proba([new_instance])) # probabilities for Iris-Setosa, Iris-Versicolor, Iris-Virginica\n",
    "print(tree_clf.predict([new_instance]))       # classify according to highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm, Computational Complexity, Gini Impurity vs. Entropy, and Regularization Hyperparameters\n",
    "page 171-174<br>\n",
    "In each node, the Classification And Regression Tree (CART, standard algorithm for Scikit-Learn) splits the set into two subsets, i.e., there will be two direct child nodes. It always uses only one feature $k$ and splits it for the feature value $t_k$. Both $k$ and $t_k$ are chosen such that the training set is split into the purest possible subsets, as measured by the Gini impurities. The subsets are weighted with their size. So at each step, it tries to minimize the cost function\n",
    "$$J(k,t_k)=\\frac{m_{\\rm left}}{m}G_{\\rm left}+\\frac{m_{\\rm right}}{m}G_{\\rm right}\\,,$$\n",
    "where $G_{\\rm left/right}$ ($m_{\\rm left/right}$) is the Gini impurity (number of instances) in the left/right subset. The algorithm stops once it reaches the maximum depth or once it does not manage to reduce the impurity further.<br>\n",
    "Hyperparameters are *max_depth*, *min_samples_split*, *min_samples_leaf*, *min_weight_fraction_leaf*, and *max_leaf_nodes*. Unfortunately, finding the optimal tree is an $NP$-complete problem and takes $\\mathcal{O}(exp(m))$ time ($m$ is the number of instance).<br><br>\n",
    "**Warning / caution**<br>\n",
    "As you can see, the CART algrotihm is a *greedy algorithm*: it greedily searces for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.<br><br>\n",
    "Decision Trees are usually rather balanced so traversing th eDecision Tree requires going through roughly $\\mathcal{O}(log_2m)$ nodes. Since each node requires checking only one single feature, ther overall complexity is also $\\mathcal{O}(log_2m)$. However, training requires checking all features of all instances at each node and thus has the complexity $\\mathcal{O}(n\\times m\\times log_2m)$. For small training sets up to about a thousand features Scikit-Learn can speed training up by presorting the data (set *presort=True*).<br><br>\n",
    "By default, the CART algorithm tries to minimize the *Gini impurity* but by setting the hyperparameter *criterion* to *entropy*, one can switch from the Gini impurity to the entropy $H_i$ of the $i$-th node,\n",
    "$$H_i=-\\sum_{p_{i,k}\\neq0,k=1}^np_{i,k}\\,log\\,p_{i,k}\\,.$$\n",
    "$H_i$ will be 0 for $p_{i,k}\\in\\{0,1\\}$ (perfect classification) but greater than zero ($H_i=-log\\frac{1}{2}$) for $p_{i,0}=p_{i,1}=1/2$ (example for non-perfect classification). In Shannon's *information theory*, the entropy measures the average information content of a message: entropy is zero when all messages are identical. A reduction of entropy is often called an *information gain*.<br>\n",
    "Both *Gini impurity* and *entropy* grow similar trees. *Gini impurity* is slightly faster, so it is a good starting point. However, if they differ, *entropy* tends to produce slightly more balanced results.<br><br>\n",
    "Decision Trees make very few assumptions about the training data. Such models are often called *nonparatmetric models*, because the number of parameters is not determined prior to training. In contrast, a *parametric* model, e.g., a linear model, has a fixed number of parameters prior to training. To counteract overfitting one should use regularization. In Scikit-Learn, regularization can be achieved by restricting/setting the hyperparameters *max_depth* (maximum depth, see figure above), *min_samples_split* (minimum number of instances before splitting), *min_samples_leaf* (minimum number of instances in a leaf node), *min_weight_fraction_leaf* (same as *min_samples_leaf* but for class fractions), *max_leaf_nodes* (obvious), and *max_features* (max number of features randomly considered within a node). Increasing *min_\\** or reducing *max_\\** hyperparameters will regularize the model, see also Figure 6-3 in the book.<br><br>\n",
    "**General note**<br>\n",
    "Other algorithms work by first training the Decision Tree without restrictions, then *pruning* (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not *statistically significant*. Standard statistical tests, such as the $\\chi^2$ *test*, are used to estimate the probability that the improvement is purely the result of chance (which is called the *null hypothesis*). If this probability, called the *p-value*, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned.\n",
    "## Regression\n",
    "page 175<br>\n",
    "Decision Trees can also be used for regression tasks. Here, we use noisy quadratic data. Instead of minimizing the *Gini impurity* or the *entropy*, the CART algorithm now tries to minimize the MSE on the $y$-coordinates by splitting the data at some $x$-feature and fitting constant $y$-coordinates left and right of this threshold. The MSEs on both sides of the threshold are weighted by the number of instances on their side. Thus, the cost function is given by\n",
    "$$J(k,t_k)=\\frac{m_{\\rm left}}{m}{\\rm MSE}_{\\rm left}+\\frac{m_{\\rm right}}{m}{\\rm MSE}_{\\rm right}\\,,$$\n",
    "where ${\\rm MSE}_{\\rm node}=\\sum_{i\\in\\rm{node}}(y_{\\rm node}-y^{(i)})^2$ and $y_{\\rm node}=\\frac{1}{m_{\\rm node}}\\sum_{i\\in{\\rm node}}y^{(i)}$.<br>\n",
    "Scikit-Learn's *DecisionTreeRegressor* class works in a similar way: in each node, the *value* attribute gives the mean of the current node. Then the node splits the test set at some threshold. How this threshold is found is not discussed in the book. Possibly, the goal is to minimize the MSE in the child nodes in a similar way as CART does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=1,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# see Github link\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)           # create 200 instance of x- ...\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10 # ... and y-coordinates (the latter are noisy)\n",
    "# train a DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "print(tree_reg.fit(X, y))\n",
    "# export using graphviz (similar to above procedure but with different filename)\n",
    "export_graphviz(\n",
    "        tree_reg,\n",
    "        out_file=image_path(\"regression_tree.dot\"),\n",
    "        feature_names=[\"x1\"],\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability\n",
    "page 177<br>\n",
    "Decision Trees usually employ orthogonal decision boundaries. This is because the number of features is often equal or at least closely related to the dimension of the data. However, classes are often identified by combinations of features such that the decision boundaries might not be orthogonal.<br>\n",
    "Assume two classes can be perfectly separated with exactly one decision boundary. Then, a simple rotation of the data by 45° makes it hard for Decision Trees to find the line that separates the two classes because this diagonal line has to be constructed by many perpendicular lines and corners that approximate that diagonal. Prinicipal Component Analysis (PCA, see chapter 8) is often succesfull in removing such instabilities.<br>\n",
    "Moreover, removing only a single instance can lead to a very different tree. So there is some *instability* with respect to added or removed data. Random Forests (chapter 7) can reduce this instability by training many trees and averaging over their predictions.\n",
    "## Exercises\n",
    "page 178\n",
    "### 1.-6.\n",
    "Solutions are shown in Appendix A of the book and in the separate notebook *ExercisesWithoutCode*.\n",
    "### 7.\n",
    "Train and fine-tune a DecisionTree for the moons dataset.\n",
    "- Generate a moons dataset using *make_moons(n_samples=10000, noise=0.4)*.\n",
    "- Split it into a training set and a test set using *train_test_split()*.\n",
    "- Use grid search with cross-validation (with the help of the *GridSearchCV* class) to find good hyperparemeter values for a *DecisionTreeClassifier*. Hint: try various values for *max_leaf_nodes*.\n",
    "- Train it on the full training set using these hyperparameters, and measure your model's performance on the test set. You should get roughly 85% to 87% accuracy.\n",
    "\n",
    "Start with generating some moons data, splitting it into training and testing sets, and using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 882 out of 882 | elapsed:   18.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=42,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "                                            31, ...],\n",
       "                         'min_samples_split': [2, 3, 4]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate moons data (see chapter 5)\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "# split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# import grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_leaf_nodes': list(range(2, 100)), # vary the number of nodes and the minimum ...\n",
    "          'min_samples_split': [2, 3, 4]}        # ... number of samples necessary in a node for splitting\n",
    "# train a DecisionTreeClassifier using grid search\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1, verbose=1)\n",
    "grid_search_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best model and check its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=17,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best')\n",
      "0.8695\n"
     ]
    }
   ],
   "source": [
    "# take the best model\n",
    "print(grid_search_cv.best_estimator_)\n",
    "# import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "# make predictions\n",
    "y_pred = grid_search_cv.predict(X_test)\n",
    "# check the accuracy\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the accuracy is above 85% and almost 87%.\n",
    "### 8.\n",
    "Grow a forest.\n",
    "- Continuing the previous exercise, generate 1000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn's *ShuffleSplit* class for this.\n",
    "- Train one Decision Tree on each subset, using the best hyperparameter values found above. Evaluate these 1000 Decision Trees on the test set. Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy.\n",
    "- Now comes the magic. For each test set instance, generate the predicions of the 1000 Decision Trees, and keep only the most fequent prediction (you can use SciPy's *mode()* function for this). This gives you *majority-vote predictions * over the test set.\n",
    "- Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier!\n",
    "\n",
    "Start with the first two tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8054499999999999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "n_trees = 1000    # total number of mini sets\n",
    "n_instances = 100 # number of instances per mini set\n",
    "mini_sets = []    # container for the mini sets\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42) # keep the training ...\n",
    "for mini_train_index, mini_test_index in rs.split(X_train): # ... instances out of the test set; loop through ...\n",
    "    X_mini_train = X_train[mini_train_index]                # ... the 1000 lists with indices for the training ...\n",
    "    y_mini_train = y_train[mini_train_index]                # ... sets (features and labels) ...\n",
    "    mini_sets.append((X_mini_train, y_mini_train))          # ... and put them in the container for the mini sets\n",
    "# establish a forest by taking a thousand (untrained) copies of the best algorithm found in the previous exercise\n",
    "from sklearn.base import clone\n",
    "forest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)] # this is the initial forest\n",
    "accuracy_scores = []                                                     # container for accuracy scores\n",
    "# training each tree in the forest and storing their accuracy scores\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n",
    "    tree.fit(X_mini_train, y_mini_train)    \n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "np.mean(accuracy_scores)                                                 # average accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy is only slightly above 80%. That's not that great. Now, take care of the remaining two tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish a container for storing all the predictions of all the classifiers ...\n",
    "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "# ... and put all the predictions of all the classifiers inside\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index] = tree.predict(X_test)\n",
    "    # use SciPy's mode() function to classify according to the majority vote from the 1000 classifiers ...\n",
    "from scipy.stats import mode\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n",
    "# ... and check the accuracy of this \"ensemble\" classification\n",
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is significantly better: more than 87% are correct!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
