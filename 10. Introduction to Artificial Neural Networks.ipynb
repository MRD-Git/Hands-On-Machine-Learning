{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks\n",
    "page 253<br>\n",
    "See\n",
    "- https://github.com/ageron/handson-ml/blob/master/10_introduction_to_artificial_neural_networks.ipynb,\n",
    "- https://link.springer.com/article/10.1007/BF02478259,\n",
    "- https://en.wikipedia.org/wiki/Neuron,\n",
    "- https://en.wikipedia.org/wiki/Dual_number,\n",
    "- http://oscar.calldesk.ai, and\n",
    "- https://www.tensorflow.org/api_docs/python/tf/math/in_top_k for details.\n",
    "\n",
    "The study of birds inspired humans to build an airplane and fly. Similarly, the study of the brain inspired the development of artificial neural networks (ANNs), allowing man to build intelligent machines. Yet, just like airplanes do not need to flap their wings in order to fly, ANNs work also different from biological neurons. ANNs are the central ingredient of Deep Learning. They are versatile, powerful, and scalable. Many great feats have been achieved with programs based on ANNs: beating humans at the game of \"Go\", in the TV game show \"Jeopardy\", at identifying cancerous tissue, at classifying billions of images, and so on.\n",
    "\n",
    "This chapter will give a tour of basic ANN architectures, introduce *Mulit-Layer Perceptrons* (MLPs), and use TensorFlow to implement a program that uses ANNs to tackle the MNIST dataset (see also Chapter 3).\n",
    "\n",
    "## From Biological to Artificial Neurons\n",
    "page 254<br>\n",
    "In 1943, Warren McCulloch (neurophysiologist) and Walter Pitts (mathematician) invented the first ANN architecture (see the second link above). Many architectures have followed since. Due to early successes, there was significant belief that humans would soon converse with truly intelligent machines. But funding dropped around 1960 due to stagnation of the field. It recovered around 1980 due to new network architectures and better training techniques but by 1990, alternative machine learning techniques like support vector machines seemed more promising. Today, we are once more facing enthusiasm around ANNs. There are several reasons why this time, ANNs will have a lasting impact on our lives:\n",
    "- There is now a huge quantity of data available to train nerual networks, and ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "- The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time. This is in part due to Moore's Law, but also thanks to the gaming industy, which has produced powerful GPU cards by the millions.\n",
    "- The training algorithms have been improved. To be fair they are only slightly different from the ones used in the 1990s, but these relatively small tweaks have a huge positive impact.\n",
    "- Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algrothims were doomed because they were likely to get stuck in local optima, but it turns out that this is rather rare in practice (or when it is the case, they are usually fairly close to the global optimum).\n",
    "- ANNs seem to have entered a virtuous circle of funding and progress. Amazing products based on ANNs regularly make the headline news, which pulls more and more attention and funding toward them, resulting in more and more progress, and even more amazing products.\n",
    "\n",
    "### Biological Neurons\n",
    "page 255<br>\n",
    "Biological neurons are composed of a cell body -  or *soma* - that contains the cell's complex components (the Golgi apparatus, nucleus, endoplastic reticulum, and the mitochondrion), *dendrites*, and an *axon*. An axon (or *nerve fiber*) can reach a length of 1 meter and at its end it splits into many branches called *telodendria* that have *synaptic terminals* (or simply *synapses*) at their ends. Many axons are often gathered into a bundle / nerve. Dendrites reach lengths of hundreds of micrometers and branch into complex dendritic trees. Most neurons receive signals via the dendrites and send signals out via the axon. Electric impulses called *signals* are transmitted between neurons via the synapses. When a neuron receives a certain signal intensity within a few milliseconds, it will emit an own signal via its axon (see the third link above for details).<br>\n",
    "So biological neurons function in a rather simple way. But they are organised in a vast network of billions of neurons, each neuron typically connected to thousands of other neurons. This allows the biological neural networks (BNNs - *neural networks* or NNs are usually assumed to be artificial, at least in the context of machine learning) to perform highly complex computations. Although our understanding of these networks is far from complete, it seems that neurons are often organized in consecutive layers.\n",
    "\n",
    "### Logical Computations with Neurons\n",
    "page 256<br>\n",
    "McCulloch and Pitts introduced a model that later became known as the *artificial neuron*: it has one ore more binary inputs and one binary output. It only fires its binary output when the number of activated inputs exceeds a certain threshold. Importantly, McCulloch and Pitts showed that even with this simple model, a big enough network will be able to compute any logical function. Figure 10-3 of the book shows how minimal networks of such artificial neurons perfrom *AND, OR, NOT* and *NAND* operations.\n",
    "\n",
    "### The Perceptron\n",
    "page 257<br>\n",
    "In 1957, Frank Rosenblatt invented the *perceptron*, one of the most simple ANN architectures. Its artificial neuron is a *linear threshold unit* (LTU), which weighs each scalar and real input $x_i$ with a weight $w_i$ and adds the weighted inputs up to $z(x)=\\sum_ix_iw_i$ and then applies a step function (e.g., the Heaviside, sign, or arctan functions), thus yielding the result\n",
    "$$h_w(x)={\\rm step}(z)={\\rm step}(w^T\\cdot x)\\,.$$\n",
    "<br>\n",
    "In previous chapters, we had also included a bias term $w_0$. This can be included in the perceptron by using an additional constant input $x_0=1$ via an additional neuron, the *bias neuron*. Then, a single LTU can be used as a binary classifier, e.g., on the iris dataset: use a constant bias feature $w_0=1$ and features $w_1$ and $w_2$ for petal length and petal width.<br>\n",
    "Sometimes, the word *perceptron* is used to mean a single LTU, and sometimes it meant to represent a simple network that consists of only one single layer of LTUs. The perceptron diagram in Figure 10-5 of the book shows a single layer network with 3 parallel LTUs, all of which receive the same inputs, $x_0=1$, $x_1$, and $x_2$. The three outputs could be used to classify the three classes of the iris dataset. But to do so, the network needs to be trained. How is this done?<br><br>\n",
    "According to Donald Hebb and Siegrid LÃ¶wel, \"cells that fire together, wire together\" (*Hebbian learning* or Hebb's rule), that is the connecting weight between two neurons is increased whenever they produce the same output. Perceptrons use a variant of this rule: they only reinforce connections that lead to the correct output. The according **perceptron learning rule** (weight update) is<br><br>\n",
    "$$w_{i,j}^{\\rm(next\\,step)}=w_{i,j}+\\eta(y_j-y^{'}_j)x_i\\,,$$\n",
    "<br>\n",
    "where $w_{i,j}$ is the weight between input neuron $i$ and output neuron $j$, $x_i$ is the output of input neuron $i$ (i.e., the $i$-th feature of the current instance), $y^{'}_j$ is the output of the output neuron $j$, $y_j$ is the target output (class) of the current instance, and $\\eta$ is the learning rate. In short, the weight $w_{i,j}$ can only be updated if the input neuron $i$ has fired ($x_i=1$) and if it has, then the weight is updated according to the deviation from the desired class and according to the learning rate. Perceptrons are linear models with linear decision boundaries (see, e.g., Chapter 4 on linear models like logistic regression classifiers). But according to Rosenblatt's *perceptron convergence theorem*, this algorithm will converge towards a solution.<br><br>\n",
    "Let's try perceptrons out with Scikit-Learn's perceptron class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "# prepare the training data\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)] # petal length, petal width\n",
    "y = (iris.target==0).astype(np.int) # Iris Setosa\n",
    "print(iris.target)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above outputs, the positive class (yes, it is a Iris Setosa) is characterized by the prediction 1. Below, the features (petal length = 2cm, petal width = 0.5cm) lead to the prediction that such specimen is indeed an Iris Setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# training of the perceptron classifier\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "# inference on a new instance\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the perceptron learning rule displayed above (Equation 10-2 in the book), the perceptron learning rule resembles strongly stochastic gradient descent (SGD). Indeed, Scikit-Learn's perceptron class is simply an SGD classifier with hyperparameters loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization). Unlike logistic regression classifiers, perceptrons do not output a class probability. Instead, they just make predictions based on a hard threshold, which is one of many good reasons to prefer logistic regression over perceptrons.<br>\n",
    "In 1969, Marvin Minsky and Seymour Papert showed that perceptrons have serious weaknesses, e.g., the inability to compute the exclusive OR (XOR). This is true for any linear classification model (e.g., logistic regression classifiers) but was a disappointment for researchers, nevertheless. As a consequence, many researchers stopped their efforts towards *connectionism*, i.e., of neural networks.<br>\n",
    "But meanwhile, people found out that many shortcomings can be resolved by stacking several perceptrons. An XOR gate based on stacked perceptrons is demonstrated by the code below, were the network shown in the right part of Figure 10-6 of the book is implemented. However, the penalty due to firing of the left (first) LTU in the bottom (first) layer has been increased from -1 to -3. Of course, this does not change the fact that stacked perceptrons can indeed perform XOR operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = 0, 0\n",
    "in1 = x1 + x2 - 1.5                       # input to left LTU in first layer\n",
    "in2 = x1 + (x2*(x2+1)) - 0.5              # input to right LTU in first layer\n",
    "out1 = np.heaviside(in1, 0.5)             # left LTU\n",
    "out2 = np.heaviside(in2, 0.5)             # right LTU\n",
    "in3 = out1*(out1-3) + out2*(out2+1) - 0.5 # input LTU in second layer\n",
    "out3 = np.heaviside(in3, 0.5)             # top LTU\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron and Backpropagation\n",
    "page 261<br>\n",
    "An MLP (Mulit-Layer Perceptron) consists of an *input layer*, one or more *hidden layers*, and an additional *output layer*, see also Figure 10-7 in the book. Apart from the output layer, every layer also has a bias neuron. When there is more than one hidden layer, it is called a *deep neural network* (DNN). Training MLPs was a struggle until 1986, when D. E. Rumelhart introduced the *backpropagation* training algorithm. In short, it is gradient descent with rervers-mode autodiff (see chapter 4 and 9, as well as Appendinx D). In order for this to work, it was helpful to use a differentiabla activation function instead of a discontinuous step function. Rumelhart *et al.* chose a *logistic function*, $\\sigma(z)=(1+e^{-z})^{-1}$ but other differentiable step functions are also possible. When $\\tanh(z)$ is used, the output values lie between Â±1 such that the input from on layer of LTUs to an LTU of the next layer will on average be 0, at least in the beginning of training. Notably, this can speed up convergence. The *rectified linear unit*, ${\\rm ReLU}(z)={\\rm max}(0,z)$ â introduced in chapter 9 â has a discontinuous derivative but can help reduce issues with gradient descent due to its lack of an upper bound (more on that in chapter 11).<br><br>\n",
    "MLPs are often used for classification tasks, with each output corresponding to a different binary class (spam/ham, urgent/not urgent, ..., or 0/not 0, 1/not 1, 2/not 2, ...). When these classes are exclusive, it makes sense to further process the output of the output neurons with a softmax function (see chapter 4) so that the actual outputs are probabilities for that class. This makes sense when classes are exclusive since then, the outputs should add up to one, such that an interpretation as probability is meaningful.<br><br>\n",
    "**General note**<br>\n",
    "Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck to sigmoid functions for a very long time. But it turns out that the ReLU activation function generally works better in ANNs. This is one of the cases where the biological analogy was misleading.<br><br>\n",
    "Tensorflow's reverse-mode autodiff algorithm is nicely explained in Appendix D. The parts \"Forward-Mode Autodiff\" and \"Reverse-Mode Autodiff\" are really worth reading. The concept of dual numbers can be understood from the wikipedia page, https://en.wikipedia.org/wiki/Dual_number, in particular from the parts on \"Linear representation\" and \"Differentiation\". A \"physicist's\" alternative to the linear formulation using matrices would be to introduce the \"dual element\" as an infinitesimal that squares to zero, $x+{\\rm d}x\\to(x+{\\rm d}x)/x=1+\\epsilon$ where ${\\rm d}x^2=\\epsilon^2=0$, while both ${\\rm d}x$ and $\\epsilon$ are nonzero.<br>\n",
    "With these tools, the Taylor expansion of $f(a+b\\epsilon)$ around $a$ (see also the linked wikipedia page),\n",
    "$$f(a+b\\epsilon)=\\sum_n\\frac{f^{(n)}(a)}{n!}(b\\epsilon)^n=f(a)+f'(a)b\\epsilon\\,,$$\n",
    "explains differentiation via dual numbers very well ($\\epsilon^2=0$). Figure D-3 on page 516 shows how reverse-mode autodiff really requires going through the graph only twice. Importantly, the node $n_2$ highlights that in the chain rule, $\\partial_xf=(\\partial_{n_i}\\,f)/(\\partial_xn_i)$, the sum remains necessary. In the individual nodes, $x$ is replaced by $n_j$. A short part from that appendix section shall be quoted, here: \"How much does $f$ vary when $n_5$ varies? The answer is $\\partial_{n_5}\\,f=(\\partial_{n_7}\\,f)/(\\partial_{n_5}n_7)$.\"\n",
    "\n",
    "## Training an MLP with TensorFlow's High-Level API\n",
    "page 264<br>\n",
    "The simplest way to train an MLP with TensorFlow is to use the high-level API tf.learn, which is compatible with Scikit-Learn. Below, we use the DNNClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/j2/hf6944zn74l9y35sr4mgbzb00000gn/T/tmp1i3slmao\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/j2/hf6944zn74l9y35sr4mgbzb00000gn/T/tmp1i3slmao', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x125f85e48>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/j2/hf6944zn74l9y35sr4mgbzb00000gn/T/tmp1i3slmao/model.ckpt.\n",
      "INFO:tensorflow:loss = 111.2738, step = 1\n",
      "INFO:tensorflow:global_step/sec: 74.6508\n",
      "INFO:tensorflow:loss = 12.76069, step = 101 (1.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.1829\n",
      "INFO:tensorflow:loss = 14.68929, step = 201 (1.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.718\n",
      "INFO:tensorflow:loss = 1.1395445, step = 301 (1.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.7452\n",
      "INFO:tensorflow:loss = 2.9517152, step = 401 (1.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.3724\n",
      "INFO:tensorflow:loss = 6.310776, step = 501 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3406\n",
      "INFO:tensorflow:loss = 6.848994, step = 601 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.3589\n",
      "INFO:tensorflow:loss = 11.0270405, step = 701 (1.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.838\n",
      "INFO:tensorflow:loss = 6.032175, step = 801 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.8845\n",
      "INFO:tensorflow:loss = 11.023594, step = 901 (1.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.6134\n",
      "INFO:tensorflow:loss = 4.596037, step = 1001 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2656\n",
      "INFO:tensorflow:loss = 10.222907, step = 1101 (1.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.2988\n",
      "INFO:tensorflow:loss = 4.779354, step = 1201 (1.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.451\n",
      "INFO:tensorflow:loss = 1.9556199, step = 1301 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4359\n",
      "INFO:tensorflow:loss = 4.278097, step = 1401 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9962\n",
      "INFO:tensorflow:loss = 1.0817053, step = 1501 (1.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.6097\n",
      "INFO:tensorflow:loss = 0.65827906, step = 1601 (1.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7762\n",
      "INFO:tensorflow:loss = 2.7803035, step = 1701 (1.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.6513\n",
      "INFO:tensorflow:loss = 10.9118185, step = 1801 (1.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5532\n",
      "INFO:tensorflow:loss = 6.124473, step = 1901 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.549\n",
      "INFO:tensorflow:loss = 2.828579, step = 2001 (1.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.4759\n",
      "INFO:tensorflow:loss = 3.429254, step = 2101 (1.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0086\n",
      "INFO:tensorflow:loss = 3.2200236, step = 2201 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5771\n",
      "INFO:tensorflow:loss = 10.646117, step = 2301 (1.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1061\n",
      "INFO:tensorflow:loss = 3.2420974, step = 2401 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3102\n",
      "INFO:tensorflow:loss = 9.892055, step = 2501 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0873\n",
      "INFO:tensorflow:loss = 3.1105788, step = 2601 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0621\n",
      "INFO:tensorflow:loss = 9.83966, step = 2701 (1.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.6204\n",
      "INFO:tensorflow:loss = 5.081539, step = 2801 (1.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0077\n",
      "INFO:tensorflow:loss = 0.44344962, step = 2901 (1.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.881\n",
      "INFO:tensorflow:loss = 3.69627, step = 3001 (1.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8005\n",
      "INFO:tensorflow:loss = 1.6342572, step = 3101 (1.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2918\n",
      "INFO:tensorflow:loss = 1.4848675, step = 3201 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5544\n",
      "INFO:tensorflow:loss = 1.1151501, step = 3301 (1.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6935\n",
      "INFO:tensorflow:loss = 2.2965953, step = 3401 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.2776\n",
      "INFO:tensorflow:loss = 1.1819438, step = 3501 (1.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.1546\n",
      "INFO:tensorflow:loss = 1.4089923, step = 3601 (1.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.066\n",
      "INFO:tensorflow:loss = 5.5587955, step = 3701 (1.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1612\n",
      "INFO:tensorflow:loss = 1.4535192, step = 3801 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0167\n",
      "INFO:tensorflow:loss = 1.0642236, step = 3901 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1581\n",
      "INFO:tensorflow:loss = 1.2977616, step = 4001 (1.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0303\n",
      "INFO:tensorflow:loss = 1.546344, step = 4101 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4966\n",
      "INFO:tensorflow:loss = 1.4909941, step = 4201 (1.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1303\n",
      "INFO:tensorflow:loss = 3.3459952, step = 4301 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8869\n",
      "INFO:tensorflow:loss = 1.51694, step = 4401 (1.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.1904\n",
      "INFO:tensorflow:loss = 1.157588, step = 4501 (1.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0051\n",
      "INFO:tensorflow:loss = 0.15771133, step = 4601 (1.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.3427\n",
      "INFO:tensorflow:loss = 3.154062, step = 4701 (1.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5652\n",
      "INFO:tensorflow:loss = 0.45003107, step = 4801 (1.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.7735\n",
      "INFO:tensorflow:loss = 1.9772934, step = 4901 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3849\n",
      "INFO:tensorflow:loss = 2.061543, step = 5001 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7765\n",
      "INFO:tensorflow:loss = 4.361506, step = 5101 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.9742\n",
      "INFO:tensorflow:loss = 0.84030753, step = 5201 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.4469\n",
      "INFO:tensorflow:loss = 0.3778469, step = 5301 (1.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3019\n",
      "INFO:tensorflow:loss = 2.6963127, step = 5401 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.3854\n",
      "INFO:tensorflow:loss = 1.1289711, step = 5501 (1.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.3525\n",
      "INFO:tensorflow:loss = 0.34328118, step = 5601 (1.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.936\n",
      "INFO:tensorflow:loss = 0.349436, step = 5701 (1.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.5818\n",
      "INFO:tensorflow:loss = 0.22225896, step = 5801 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4973\n",
      "INFO:tensorflow:loss = 1.2969157, step = 5901 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.3095\n",
      "INFO:tensorflow:loss = 5.7495937, step = 6001 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.1835\n",
      "INFO:tensorflow:loss = 0.300655, step = 6101 (1.402 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.7331\n",
      "INFO:tensorflow:loss = 2.8219504, step = 6201 (1.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.3381\n",
      "INFO:tensorflow:loss = 0.13137963, step = 6301 (1.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1536\n",
      "INFO:tensorflow:loss = 0.17558542, step = 6401 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1549\n",
      "INFO:tensorflow:loss = 0.3150937, step = 6501 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.9244\n",
      "INFO:tensorflow:loss = 0.33722508, step = 6601 (1.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5155\n",
      "INFO:tensorflow:loss = 0.34174636, step = 6701 (1.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.2044\n",
      "INFO:tensorflow:loss = 1.0527476, step = 6801 (1.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2409\n",
      "INFO:tensorflow:loss = 3.0550797, step = 6901 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9112\n",
      "INFO:tensorflow:loss = 0.14217164, step = 7001 (1.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0138\n",
      "INFO:tensorflow:loss = 9.665017, step = 7101 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2207\n",
      "INFO:tensorflow:loss = 0.32472464, step = 7201 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.683\n",
      "INFO:tensorflow:loss = 2.1864486, step = 7301 (1.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2631\n",
      "INFO:tensorflow:loss = 1.0688549, step = 7401 (1.199 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 79.5877\n",
      "INFO:tensorflow:loss = 2.466872, step = 7501 (1.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.9726\n",
      "INFO:tensorflow:loss = 1.2315158, step = 7601 (1.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9786\n",
      "INFO:tensorflow:loss = 5.6266356, step = 7701 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5624\n",
      "INFO:tensorflow:loss = 0.09036364, step = 7801 (1.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9089\n",
      "INFO:tensorflow:loss = 0.2519115, step = 7901 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4359\n",
      "INFO:tensorflow:loss = 6.8407655, step = 8001 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2174\n",
      "INFO:tensorflow:loss = 0.39095354, step = 8101 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.053\n",
      "INFO:tensorflow:loss = 1.024648, step = 8201 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9161\n",
      "INFO:tensorflow:loss = 0.54383826, step = 8301 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3709\n",
      "INFO:tensorflow:loss = 0.496621, step = 8401 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.6958\n",
      "INFO:tensorflow:loss = 0.37743285, step = 8501 (1.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.4173\n",
      "INFO:tensorflow:loss = 0.3709712, step = 8601 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3539\n",
      "INFO:tensorflow:loss = 0.8342526, step = 8701 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1705\n",
      "INFO:tensorflow:loss = 0.19410913, step = 8801 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0051\n",
      "INFO:tensorflow:loss = 2.1783013, step = 8901 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2446\n",
      "INFO:tensorflow:loss = 0.089563586, step = 9001 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3166\n",
      "INFO:tensorflow:loss = 0.44877827, step = 9101 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9768\n",
      "INFO:tensorflow:loss = 1.9118333, step = 9201 (1.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2955\n",
      "INFO:tensorflow:loss = 2.508504, step = 9301 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1305\n",
      "INFO:tensorflow:loss = 0.532036, step = 9401 (1.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8461\n",
      "INFO:tensorflow:loss = 0.12576014, step = 9501 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2142\n",
      "INFO:tensorflow:loss = 0.26162094, step = 9601 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8719\n",
      "INFO:tensorflow:loss = 0.26009867, step = 9701 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9325\n",
      "INFO:tensorflow:loss = 0.105855934, step = 9801 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.42\n",
      "INFO:tensorflow:loss = 0.1213243, step = 9901 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7067\n",
      "INFO:tensorflow:loss = 0.11950545, step = 10001 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3123\n",
      "INFO:tensorflow:loss = 0.0578376, step = 10101 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8763\n",
      "INFO:tensorflow:loss = 0.4810086, step = 10201 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0932\n",
      "INFO:tensorflow:loss = 0.14835353, step = 10301 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9679\n",
      "INFO:tensorflow:loss = 0.4944289, step = 10401 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7649\n",
      "INFO:tensorflow:loss = 0.13131957, step = 10501 (1.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.401\n",
      "INFO:tensorflow:loss = 0.043584548, step = 10601 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2315\n",
      "INFO:tensorflow:loss = 0.09620434, step = 10701 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9116\n",
      "INFO:tensorflow:loss = 0.07561545, step = 10801 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7747\n",
      "INFO:tensorflow:loss = 0.25403655, step = 10901 (1.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4873\n",
      "INFO:tensorflow:loss = 0.43601537, step = 11001 (1.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9962\n",
      "INFO:tensorflow:loss = 0.03324336, step = 11101 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4233\n",
      "INFO:tensorflow:loss = 0.32432258, step = 11201 (1.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8224\n",
      "INFO:tensorflow:loss = 0.55754507, step = 11301 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.6506\n",
      "INFO:tensorflow:loss = 0.16218217, step = 11401 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5334\n",
      "INFO:tensorflow:loss = 0.08203812, step = 11501 (1.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0182\n",
      "INFO:tensorflow:loss = 0.06655315, step = 11601 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1119\n",
      "INFO:tensorflow:loss = 0.1198401, step = 11701 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4006\n",
      "INFO:tensorflow:loss = 0.055984933, step = 11801 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8783\n",
      "INFO:tensorflow:loss = 0.89595973, step = 11901 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6677\n",
      "INFO:tensorflow:loss = 0.06372848, step = 12001 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4022\n",
      "INFO:tensorflow:loss = 0.18164273, step = 12101 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3888\n",
      "INFO:tensorflow:loss = 0.15554927, step = 12201 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9634\n",
      "INFO:tensorflow:loss = 0.25869513, step = 12301 (1.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8063\n",
      "INFO:tensorflow:loss = 0.13087879, step = 12401 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3409\n",
      "INFO:tensorflow:loss = 0.24307796, step = 12501 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9207\n",
      "INFO:tensorflow:loss = 1.0114056, step = 12601 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1104\n",
      "INFO:tensorflow:loss = 0.022238621, step = 12701 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1749\n",
      "INFO:tensorflow:loss = 0.28177446, step = 12801 (1.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0463\n",
      "INFO:tensorflow:loss = 0.36530417, step = 12901 (1.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4569\n",
      "INFO:tensorflow:loss = 0.9210401, step = 13001 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1883\n",
      "INFO:tensorflow:loss = 0.17159285, step = 13101 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1296\n",
      "INFO:tensorflow:loss = 0.26202047, step = 13201 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4131\n",
      "INFO:tensorflow:loss = 0.111649305, step = 13301 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6496\n",
      "INFO:tensorflow:loss = 0.08611577, step = 13401 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.7456\n",
      "INFO:tensorflow:loss = 0.024185626, step = 13501 (1.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.002\n",
      "INFO:tensorflow:loss = 0.008384367, step = 13601 (1.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.8558\n",
      "INFO:tensorflow:loss = 0.18494424, step = 13701 (1.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.558\n",
      "INFO:tensorflow:loss = 0.015972849, step = 13801 (1.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3179\n",
      "INFO:tensorflow:loss = 0.27430934, step = 13901 (1.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3033\n",
      "INFO:tensorflow:loss = 0.07242732, step = 14001 (1.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7769\n",
      "INFO:tensorflow:loss = 0.043688998, step = 14101 (1.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.6194\n",
      "INFO:tensorflow:loss = 0.44438717, step = 14201 (1.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.583\n",
      "INFO:tensorflow:loss = 0.042654637, step = 14301 (1.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.0895\n",
      "INFO:tensorflow:loss = 0.102077246, step = 14401 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5867\n",
      "INFO:tensorflow:loss = 0.13144605, step = 14501 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5881\n",
      "INFO:tensorflow:loss = 2.0638673, step = 14601 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8071\n",
      "INFO:tensorflow:loss = 0.28274807, step = 14701 (1.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.3804\n",
      "INFO:tensorflow:loss = 0.22758387, step = 14801 (1.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7005\n",
      "INFO:tensorflow:loss = 0.08484335, step = 14901 (1.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.3738\n",
      "INFO:tensorflow:loss = 0.044514652, step = 15001 (1.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.4156\n",
      "INFO:tensorflow:loss = 0.18818675, step = 15101 (1.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7067\n",
      "INFO:tensorflow:loss = 0.021060389, step = 15201 (1.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8005\n",
      "INFO:tensorflow:loss = 0.12151311, step = 15301 (1.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.6505\n",
      "INFO:tensorflow:loss = 0.056342345, step = 15401 (1.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.369\n",
      "INFO:tensorflow:loss = 0.09441045, step = 15501 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.192\n",
      "INFO:tensorflow:loss = 0.12915552, step = 15601 (1.297 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 80.7374\n",
      "INFO:tensorflow:loss = 0.018453483, step = 15701 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.2792\n",
      "INFO:tensorflow:loss = 0.005063211, step = 15801 (1.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.165\n",
      "INFO:tensorflow:loss = 0.045956515, step = 15901 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.044\n",
      "INFO:tensorflow:loss = 0.14933804, step = 16001 (1.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9524\n",
      "INFO:tensorflow:loss = 0.02612287, step = 16101 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.993\n",
      "INFO:tensorflow:loss = 0.1607581, step = 16201 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0595\n",
      "INFO:tensorflow:loss = 0.19471687, step = 16301 (1.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5668\n",
      "INFO:tensorflow:loss = 0.03702511, step = 16401 (1.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.9001\n",
      "INFO:tensorflow:loss = 0.13329874, step = 16501 (1.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2804\n",
      "INFO:tensorflow:loss = 0.048678815, step = 16601 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0492\n",
      "INFO:tensorflow:loss = 0.17134306, step = 16701 (1.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0743\n",
      "INFO:tensorflow:loss = 0.04473674, step = 16801 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2852\n",
      "INFO:tensorflow:loss = 0.095665194, step = 16901 (1.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5368\n",
      "INFO:tensorflow:loss = 0.12867521, step = 17001 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7302\n",
      "INFO:tensorflow:loss = 0.047330666, step = 17101 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4703\n",
      "INFO:tensorflow:loss = 0.014834153, step = 17201 (1.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5897\n",
      "INFO:tensorflow:loss = 0.06467925, step = 17301 (1.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9762\n",
      "INFO:tensorflow:loss = 0.07807407, step = 17401 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.3182\n",
      "INFO:tensorflow:loss = 0.07383317, step = 17501 (1.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.7457\n",
      "INFO:tensorflow:loss = 0.012287584, step = 17601 (1.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9008\n",
      "INFO:tensorflow:loss = 0.10476042, step = 17701 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3655\n",
      "INFO:tensorflow:loss = 0.032276668, step = 17801 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0261\n",
      "INFO:tensorflow:loss = 0.20848611, step = 17901 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8644\n",
      "INFO:tensorflow:loss = 0.0015099463, step = 18001 (1.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4071\n",
      "INFO:tensorflow:loss = 0.22623366, step = 18101 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5945\n",
      "INFO:tensorflow:loss = 0.19011146, step = 18201 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0011\n",
      "INFO:tensorflow:loss = 0.08643807, step = 18301 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.409\n",
      "INFO:tensorflow:loss = 0.04517721, step = 18401 (1.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1273\n",
      "INFO:tensorflow:loss = 0.33064857, step = 18501 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2365\n",
      "INFO:tensorflow:loss = 0.016247667, step = 18601 (1.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1322\n",
      "INFO:tensorflow:loss = 0.13547449, step = 18701 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8166\n",
      "INFO:tensorflow:loss = 0.047903415, step = 18801 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.9483\n",
      "INFO:tensorflow:loss = 0.32433832, step = 18901 (1.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.829\n",
      "INFO:tensorflow:loss = 0.1455228, step = 19001 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8336\n",
      "INFO:tensorflow:loss = 0.02467233, step = 19101 (1.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7481\n",
      "INFO:tensorflow:loss = 0.015358709, step = 19201 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.0003\n",
      "INFO:tensorflow:loss = 0.11522218, step = 19301 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8248\n",
      "INFO:tensorflow:loss = 0.08646712, step = 19401 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4788\n",
      "INFO:tensorflow:loss = 0.031394947, step = 19501 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4064\n",
      "INFO:tensorflow:loss = 0.020355847, step = 19601 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8602\n",
      "INFO:tensorflow:loss = 0.09336012, step = 19701 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1124\n",
      "INFO:tensorflow:loss = 0.07185194, step = 19801 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6929\n",
      "INFO:tensorflow:loss = 0.07411547, step = 19901 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4884\n",
      "INFO:tensorflow:loss = 0.015418166, step = 20001 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2308\n",
      "INFO:tensorflow:loss = 0.38133544, step = 20101 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1692\n",
      "INFO:tensorflow:loss = 0.0047113216, step = 20201 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.921\n",
      "INFO:tensorflow:loss = 0.09235419, step = 20301 (1.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4016\n",
      "INFO:tensorflow:loss = 0.10528539, step = 20401 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2442\n",
      "INFO:tensorflow:loss = 0.050937515, step = 20501 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.766\n",
      "INFO:tensorflow:loss = 0.036151163, step = 20601 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1742\n",
      "INFO:tensorflow:loss = 0.156943, step = 20701 (1.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3317\n",
      "INFO:tensorflow:loss = 0.076747626, step = 20801 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1686\n",
      "INFO:tensorflow:loss = 0.024817174, step = 20901 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3319\n",
      "INFO:tensorflow:loss = 0.0058679734, step = 21001 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1313\n",
      "INFO:tensorflow:loss = 0.03976801, step = 21101 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6728\n",
      "INFO:tensorflow:loss = 0.20151867, step = 21201 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9022\n",
      "INFO:tensorflow:loss = 0.078326605, step = 21301 (1.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6054\n",
      "INFO:tensorflow:loss = 0.08472477, step = 21401 (1.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8309\n",
      "INFO:tensorflow:loss = 0.0070510334, step = 21501 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2961\n",
      "INFO:tensorflow:loss = 0.009196147, step = 21601 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4393\n",
      "INFO:tensorflow:loss = 0.06622837, step = 21701 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.0277\n",
      "INFO:tensorflow:loss = 0.17552204, step = 21801 (1.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3223\n",
      "INFO:tensorflow:loss = 0.060996115, step = 21901 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.3768\n",
      "INFO:tensorflow:loss = 0.02194377, step = 22001 (1.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3453\n",
      "INFO:tensorflow:loss = 0.24251615, step = 22101 (1.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2782\n",
      "INFO:tensorflow:loss = 0.009592633, step = 22201 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0893\n",
      "INFO:tensorflow:loss = 0.04121512, step = 22301 (1.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.8088\n",
      "INFO:tensorflow:loss = 0.1245791, step = 22401 (1.285 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2548\n",
      "INFO:tensorflow:loss = 0.027543623, step = 22501 (1.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.025\n",
      "INFO:tensorflow:loss = 0.027458556, step = 22601 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3172\n",
      "INFO:tensorflow:loss = 0.033026613, step = 22701 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7074\n",
      "INFO:tensorflow:loss = 0.058511354, step = 22801 (1.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8066\n",
      "INFO:tensorflow:loss = 0.08483229, step = 22901 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8537\n",
      "INFO:tensorflow:loss = 0.083467096, step = 23001 (1.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7896\n",
      "INFO:tensorflow:loss = 0.65255654, step = 23101 (1.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9304\n",
      "INFO:tensorflow:loss = 0.23470883, step = 23201 (1.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.7518\n",
      "INFO:tensorflow:loss = 0.07156044, step = 23301 (1.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3828\n",
      "INFO:tensorflow:loss = 0.096007854, step = 23401 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3796\n",
      "INFO:tensorflow:loss = 0.011772792, step = 23501 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1565\n",
      "INFO:tensorflow:loss = 0.021448262, step = 23601 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2295\n",
      "INFO:tensorflow:loss = 0.010777782, step = 23701 (1.262 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 80.7882\n",
      "INFO:tensorflow:loss = 0.047273476, step = 23801 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6435\n",
      "INFO:tensorflow:loss = 0.07347919, step = 23901 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4893\n",
      "INFO:tensorflow:loss = 0.09225047, step = 24001 (1.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8638\n",
      "INFO:tensorflow:loss = 0.010594401, step = 24101 (1.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0595\n",
      "INFO:tensorflow:loss = 0.07059836, step = 24201 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5136\n",
      "INFO:tensorflow:loss = 0.0026259483, step = 24301 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2328\n",
      "INFO:tensorflow:loss = 0.12016117, step = 24401 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8898\n",
      "INFO:tensorflow:loss = 0.07194876, step = 24501 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6262\n",
      "INFO:tensorflow:loss = 0.0043789004, step = 24601 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7332\n",
      "INFO:tensorflow:loss = 0.037498884, step = 24701 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2468\n",
      "INFO:tensorflow:loss = 0.00247204, step = 24801 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8259\n",
      "INFO:tensorflow:loss = 0.029444555, step = 24901 (1.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.851\n",
      "INFO:tensorflow:loss = 0.23646057, step = 25001 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2742\n",
      "INFO:tensorflow:loss = 0.026937965, step = 25101 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5693\n",
      "INFO:tensorflow:loss = 0.03092077, step = 25201 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8792\n",
      "INFO:tensorflow:loss = 0.0658566, step = 25301 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8679\n",
      "INFO:tensorflow:loss = 0.015321378, step = 25401 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4151\n",
      "INFO:tensorflow:loss = 0.05467838, step = 25501 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.5297\n",
      "INFO:tensorflow:loss = 0.057886083, step = 25601 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5049\n",
      "INFO:tensorflow:loss = 0.016510237, step = 25701 (1.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0147\n",
      "INFO:tensorflow:loss = 0.007831563, step = 25801 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0398\n",
      "INFO:tensorflow:loss = 0.0398921, step = 25901 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.2179\n",
      "INFO:tensorflow:loss = 0.05165513, step = 26001 (1.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.6364\n",
      "INFO:tensorflow:loss = 0.12689798, step = 26101 (1.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9836\n",
      "INFO:tensorflow:loss = 0.028356662, step = 26201 (1.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3449\n",
      "INFO:tensorflow:loss = 0.089665644, step = 26301 (1.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.3742\n",
      "INFO:tensorflow:loss = 0.003963859, step = 26401 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.0848\n",
      "INFO:tensorflow:loss = 0.022569276, step = 26501 (1.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.1395\n",
      "INFO:tensorflow:loss = 0.042013977, step = 26601 (1.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.4809\n",
      "INFO:tensorflow:loss = 0.056900516, step = 26701 (1.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4948\n",
      "INFO:tensorflow:loss = 0.08049942, step = 26801 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4824\n",
      "INFO:tensorflow:loss = 0.031306792, step = 26901 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9205\n",
      "INFO:tensorflow:loss = 0.11036873, step = 27001 (1.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.9738\n",
      "INFO:tensorflow:loss = 0.040012825, step = 27101 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5941\n",
      "INFO:tensorflow:loss = 0.020551316, step = 27201 (1.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.7495\n",
      "INFO:tensorflow:loss = 0.058765803, step = 27301 (1.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5672\n",
      "INFO:tensorflow:loss = 0.045098986, step = 27401 (1.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.9995\n",
      "INFO:tensorflow:loss = 0.23641717, step = 27501 (1.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.9507\n",
      "INFO:tensorflow:loss = 0.124025404, step = 27601 (1.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.4526\n",
      "INFO:tensorflow:loss = 0.04933518, step = 27701 (1.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.3977\n",
      "INFO:tensorflow:loss = 0.006285055, step = 27801 (1.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.0256\n",
      "INFO:tensorflow:loss = 0.040940445, step = 27901 (1.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.4065\n",
      "INFO:tensorflow:loss = 0.02677204, step = 28001 (1.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.8221\n",
      "INFO:tensorflow:loss = 0.0881826, step = 28101 (1.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.7737\n",
      "INFO:tensorflow:loss = 0.04797277, step = 28201 (1.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1735\n",
      "INFO:tensorflow:loss = 0.018203013, step = 28301 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3053\n",
      "INFO:tensorflow:loss = 0.003426635, step = 28401 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.4129\n",
      "INFO:tensorflow:loss = 0.011320366, step = 28501 (1.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.176\n",
      "INFO:tensorflow:loss = 0.020981675, step = 28601 (1.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.442\n",
      "INFO:tensorflow:loss = 0.021516783, step = 28701 (1.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.0295\n",
      "INFO:tensorflow:loss = 0.0051730536, step = 28801 (1.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.2903\n",
      "INFO:tensorflow:loss = 0.05438015, step = 28901 (1.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.587\n",
      "INFO:tensorflow:loss = 0.016050799, step = 29001 (1.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4602\n",
      "INFO:tensorflow:loss = 0.0065233065, step = 29101 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.8097\n",
      "INFO:tensorflow:loss = 0.07651726, step = 29201 (1.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.666\n",
      "INFO:tensorflow:loss = 0.01824307, step = 29301 (1.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7916\n",
      "INFO:tensorflow:loss = 0.07180848, step = 29401 (1.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0487\n",
      "INFO:tensorflow:loss = 0.021293448, step = 29501 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.4362\n",
      "INFO:tensorflow:loss = 0.029783987, step = 29601 (1.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2628\n",
      "INFO:tensorflow:loss = 0.04467256, step = 29701 (1.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4147\n",
      "INFO:tensorflow:loss = 0.026046569, step = 29801 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4861\n",
      "INFO:tensorflow:loss = 0.023825413, step = 29901 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6923\n",
      "INFO:tensorflow:loss = 0.0022229727, step = 30001 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9626\n",
      "INFO:tensorflow:loss = 0.037826985, step = 30101 (1.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3843\n",
      "INFO:tensorflow:loss = 0.025910638, step = 30201 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.2295\n",
      "INFO:tensorflow:loss = 0.04173332, step = 30301 (1.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.0412\n",
      "INFO:tensorflow:loss = 0.061972316, step = 30401 (1.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.4874\n",
      "INFO:tensorflow:loss = 0.0098960735, step = 30501 (1.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.8835\n",
      "INFO:tensorflow:loss = 0.051329114, step = 30601 (1.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.4112\n",
      "INFO:tensorflow:loss = 0.0242947, step = 30701 (1.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.888\n",
      "INFO:tensorflow:loss = 0.009911525, step = 30801 (1.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6646\n",
      "INFO:tensorflow:loss = 0.031046756, step = 30901 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.5141\n",
      "INFO:tensorflow:loss = 0.043618537, step = 31001 (1.308 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0306\n",
      "INFO:tensorflow:loss = 0.020776892, step = 31101 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6888\n",
      "INFO:tensorflow:loss = 0.00016164464, step = 31201 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0491\n",
      "INFO:tensorflow:loss = 0.041657902, step = 31301 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8053\n",
      "INFO:tensorflow:loss = 0.011215458, step = 31401 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4229\n",
      "INFO:tensorflow:loss = 0.035549667, step = 31501 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5109\n",
      "INFO:tensorflow:loss = 0.0038416982, step = 31601 (1.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.0418\n",
      "INFO:tensorflow:loss = 0.017003324, step = 31701 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6883\n",
      "INFO:tensorflow:loss = 0.03656887, step = 31801 (1.256 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 84.5704\n",
      "INFO:tensorflow:loss = 0.073672, step = 31901 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8997\n",
      "INFO:tensorflow:loss = 0.14368615, step = 32001 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7406\n",
      "INFO:tensorflow:loss = 0.016972434, step = 32101 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3682\n",
      "INFO:tensorflow:loss = 0.015100708, step = 32201 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5397\n",
      "INFO:tensorflow:loss = 0.07284905, step = 32301 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.118\n",
      "INFO:tensorflow:loss = 0.07950735, step = 32401 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2871\n",
      "INFO:tensorflow:loss = 0.018703043, step = 32501 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.249\n",
      "INFO:tensorflow:loss = 0.0143075995, step = 32601 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2776\n",
      "INFO:tensorflow:loss = 0.013998631, step = 32701 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1945\n",
      "INFO:tensorflow:loss = 0.0028412684, step = 32801 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5599\n",
      "INFO:tensorflow:loss = 0.015245351, step = 32901 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4902\n",
      "INFO:tensorflow:loss = 0.009603469, step = 33001 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0879\n",
      "INFO:tensorflow:loss = 0.07862167, step = 33101 (1.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4909\n",
      "INFO:tensorflow:loss = 0.018261587, step = 33201 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5735\n",
      "INFO:tensorflow:loss = 0.03673226, step = 33301 (1.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2539\n",
      "INFO:tensorflow:loss = 0.04438508, step = 33401 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4779\n",
      "INFO:tensorflow:loss = 0.033487357, step = 33501 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.6745\n",
      "INFO:tensorflow:loss = 0.010107671, step = 33601 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3553\n",
      "INFO:tensorflow:loss = 0.02442771, step = 33701 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4613\n",
      "INFO:tensorflow:loss = 0.012505997, step = 33801 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9712\n",
      "INFO:tensorflow:loss = 0.004795216, step = 33901 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9955\n",
      "INFO:tensorflow:loss = 0.015555283, step = 34001 (1.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5964\n",
      "INFO:tensorflow:loss = 0.011541799, step = 34101 (1.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7622\n",
      "INFO:tensorflow:loss = 0.017194198, step = 34201 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5622\n",
      "INFO:tensorflow:loss = 0.0039206226, step = 34301 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7013\n",
      "INFO:tensorflow:loss = 0.0142773455, step = 34401 (1.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.784\n",
      "INFO:tensorflow:loss = 0.005930137, step = 34501 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0148\n",
      "INFO:tensorflow:loss = 0.0397031, step = 34601 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0757\n",
      "INFO:tensorflow:loss = 0.013378773, step = 34701 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8644\n",
      "INFO:tensorflow:loss = 0.06400747, step = 34801 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3751\n",
      "INFO:tensorflow:loss = 0.0044639045, step = 34901 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0697\n",
      "INFO:tensorflow:loss = 0.054466907, step = 35001 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8672\n",
      "INFO:tensorflow:loss = 0.013762716, step = 35101 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.5692\n",
      "INFO:tensorflow:loss = 0.005094194, step = 35201 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.487\n",
      "INFO:tensorflow:loss = 0.010737883, step = 35301 (1.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1308\n",
      "INFO:tensorflow:loss = 0.011152844, step = 35401 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5082\n",
      "INFO:tensorflow:loss = 0.0023300757, step = 35501 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.6577\n",
      "INFO:tensorflow:loss = 0.029462822, step = 35601 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1542\n",
      "INFO:tensorflow:loss = 0.015789699, step = 35701 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.5239\n",
      "INFO:tensorflow:loss = 0.019254934, step = 35801 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4064\n",
      "INFO:tensorflow:loss = 0.017151909, step = 35901 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.857\n",
      "INFO:tensorflow:loss = 0.013494091, step = 36001 (1.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1011\n",
      "INFO:tensorflow:loss = 0.0041250926, step = 36101 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8449\n",
      "INFO:tensorflow:loss = 0.013945938, step = 36201 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4669\n",
      "INFO:tensorflow:loss = 0.050150223, step = 36301 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7292\n",
      "INFO:tensorflow:loss = 0.013357587, step = 36401 (1.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7646\n",
      "INFO:tensorflow:loss = 0.07076648, step = 36501 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3036\n",
      "INFO:tensorflow:loss = 0.01223808, step = 36601 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3227\n",
      "INFO:tensorflow:loss = 0.0228591, step = 36701 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3738\n",
      "INFO:tensorflow:loss = 0.02274356, step = 36801 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1628\n",
      "INFO:tensorflow:loss = 0.040040545, step = 36901 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5613\n",
      "INFO:tensorflow:loss = 0.015890189, step = 37001 (1.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4789\n",
      "INFO:tensorflow:loss = 0.001046628, step = 37101 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9085\n",
      "INFO:tensorflow:loss = 0.00016462205, step = 37201 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3744\n",
      "INFO:tensorflow:loss = 0.0018176101, step = 37301 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1748\n",
      "INFO:tensorflow:loss = 0.016662592, step = 37401 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.771\n",
      "INFO:tensorflow:loss = 0.029896596, step = 37501 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2574\n",
      "INFO:tensorflow:loss = 0.01613763, step = 37601 (1.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3698\n",
      "INFO:tensorflow:loss = 0.0048067737, step = 37701 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2219\n",
      "INFO:tensorflow:loss = 0.006995528, step = 37801 (1.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3047\n",
      "INFO:tensorflow:loss = 0.003155488, step = 37901 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0005\n",
      "INFO:tensorflow:loss = 0.014837191, step = 38001 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6215\n",
      "INFO:tensorflow:loss = 0.01631521, step = 38101 (1.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9388\n",
      "INFO:tensorflow:loss = 0.027662482, step = 38201 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9972\n",
      "INFO:tensorflow:loss = 0.021863932, step = 38301 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2732\n",
      "INFO:tensorflow:loss = 0.0034438646, step = 38401 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9594\n",
      "INFO:tensorflow:loss = 0.021333318, step = 38501 (1.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3599\n",
      "INFO:tensorflow:loss = 0.014096165, step = 38601 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0932\n",
      "INFO:tensorflow:loss = 0.03244923, step = 38701 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.734\n",
      "INFO:tensorflow:loss = 0.01536277, step = 38801 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9032\n",
      "INFO:tensorflow:loss = 0.0040220087, step = 38901 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1828\n",
      "INFO:tensorflow:loss = 0.015977891, step = 39001 (1.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6515\n",
      "INFO:tensorflow:loss = 0.008436643, step = 39101 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0058\n",
      "INFO:tensorflow:loss = 0.0066813733, step = 39201 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2173\n",
      "INFO:tensorflow:loss = 0.026134502, step = 39301 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8993\n",
      "INFO:tensorflow:loss = 0.006511007, step = 39401 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0197\n",
      "INFO:tensorflow:loss = 0.0238501, step = 39501 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0864\n",
      "INFO:tensorflow:loss = 0.005313979, step = 39601 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1075\n",
      "INFO:tensorflow:loss = 0.041563537, step = 39701 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4369\n",
      "INFO:tensorflow:loss = 0.038040966, step = 39801 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9635\n",
      "INFO:tensorflow:loss = 0.013220831, step = 39901 (1.252 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 80.1984\n",
      "INFO:tensorflow:loss = 0.02137714, step = 40001 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2889\n",
      "INFO:tensorflow:loss = 0.010872903, step = 40101 (1.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1327\n",
      "INFO:tensorflow:loss = 0.0070594614, step = 40201 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2819\n",
      "INFO:tensorflow:loss = 0.00930143, step = 40301 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2444\n",
      "INFO:tensorflow:loss = 0.017696235, step = 40401 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2309\n",
      "INFO:tensorflow:loss = 0.016471116, step = 40501 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3296\n",
      "INFO:tensorflow:loss = 0.025980936, step = 40601 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.448\n",
      "INFO:tensorflow:loss = 0.013709705, step = 40701 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.1169\n",
      "INFO:tensorflow:loss = 0.009738188, step = 40801 (1.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1722\n",
      "INFO:tensorflow:loss = 0.00778044, step = 40901 (1.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1555\n",
      "INFO:tensorflow:loss = 0.06365645, step = 41001 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.766\n",
      "INFO:tensorflow:loss = 0.03887704, step = 41101 (1.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9862\n",
      "INFO:tensorflow:loss = 0.019503634, step = 41201 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9593\n",
      "INFO:tensorflow:loss = 0.005993445, step = 41301 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4069\n",
      "INFO:tensorflow:loss = 0.02921221, step = 41401 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5378\n",
      "INFO:tensorflow:loss = 0.0070136334, step = 41501 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.969\n",
      "INFO:tensorflow:loss = 0.009069021, step = 41601 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6448\n",
      "INFO:tensorflow:loss = 0.0008985869, step = 41701 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2301\n",
      "INFO:tensorflow:loss = 0.013653607, step = 41801 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3486\n",
      "INFO:tensorflow:loss = 0.017121667, step = 41901 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.6977\n",
      "INFO:tensorflow:loss = 0.0046294793, step = 42001 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5507\n",
      "INFO:tensorflow:loss = 0.0038013814, step = 42101 (1.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3762\n",
      "INFO:tensorflow:loss = 0.021133069, step = 42201 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5238\n",
      "INFO:tensorflow:loss = 0.032709178, step = 42301 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6738\n",
      "INFO:tensorflow:loss = 0.002517969, step = 42401 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2893\n",
      "INFO:tensorflow:loss = 0.018309664, step = 42501 (1.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9545\n",
      "INFO:tensorflow:loss = 0.020632453, step = 42601 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.385\n",
      "INFO:tensorflow:loss = 0.0022074585, step = 42701 (1.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.3355\n",
      "INFO:tensorflow:loss = 0.007308812, step = 42801 (1.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.5166\n",
      "INFO:tensorflow:loss = 0.026709292, step = 42901 (1.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2152\n",
      "INFO:tensorflow:loss = 0.03550164, step = 43001 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0516\n",
      "INFO:tensorflow:loss = 0.023390785, step = 43101 (1.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1239\n",
      "INFO:tensorflow:loss = 0.008911824, step = 43201 (1.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.0652\n",
      "INFO:tensorflow:loss = 0.025761817, step = 43301 (1.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.8073\n",
      "INFO:tensorflow:loss = 0.012324747, step = 43401 (1.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4241\n",
      "INFO:tensorflow:loss = 0.009924102, step = 43501 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5735\n",
      "INFO:tensorflow:loss = 0.020297796, step = 43601 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.2082\n",
      "INFO:tensorflow:loss = 0.011987172, step = 43701 (1.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.2777\n",
      "INFO:tensorflow:loss = 0.041097093, step = 43801 (1.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5139\n",
      "INFO:tensorflow:loss = 0.021713996, step = 43901 (1.324 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 44000 into /var/folders/j2/hf6944zn74l9y35sr4mgbzb00000gn/T/tmp1i3slmao/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.014681991.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1100e33c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# the below code is taken from the github link above, since tensorflow has evolved since publication of the book\n",
    "# features and labels\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "# training\n",
    "feature_cols = [tf.feature_column.numeric_column(\"X\", shape=[28 * 28])]\n",
    "dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10, feature_columns=feature_cols)\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(x={\"X\": X_train}, y=y_train, num_epochs=40, batch_size=50, shuffle=True)\n",
    "dnn_clf.train(input_fn=input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first creates the feature and label data for training, testing, and validation, followed by the DNNClassifier. Then, the classifier is trained. With tensorflow version 1.5, the code as in the book needed to be adapted, see the github link above. To get the accuracy, we also deviate from the code from the book (see github link above) and obtain the following. The accuracy is around 98%, indeed a little bit higher than what we found with a k-nearest neighbor classifier in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-09-19-13:39:04\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/j2/hf6944zn74l9y35sr4mgbzb00000gn/T/tmp1i3slmao/model.ckpt-44000\n",
      "INFO:tensorflow:Finished evaluation at 2019-09-19-13:39:06\n",
      "INFO:tensorflow:Saving dict for global step 44000: accuracy = 0.9779, average_loss = 0.11287731, global_step = 44000, loss = 14.288267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9779,\n",
       " 'average_loss': 0.11287731,\n",
       " 'loss': 14.288267,\n",
       " 'global_step': 44000}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"X\": X_test}, y=y_test, shuffle=False)\n",
    "eval_results = dnn_clf.evaluate(input_fn=test_input_fn)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning / caution**<br>\n",
    "The tensorflow.contirb package contains many useful functions, bit it is a place for experimental code that has not yet graduated to be part of the core TensorFlow API. So the DNNClassifier class (and any other contrib code) may change wihtout notice in the future. [This is exactly why the code had to be adapted, see the github link above].\n",
    "\n",
    "## Traning a DNN Using Plain TensorFlow\n",
    "page 265<br>\n",
    "Here, we use TensorFlow's lower-level python API (see chapter 9) to build and the same network as above and train it on the MNIST dataset using batch gradient descent.\n",
    "\n",
    "### Construction Phase and Execution Phase\n",
    "page 265<br>\n",
    "For the construction of the DNN, we first specify its overall structure. Here, we use two fully connected layers: the first with 300 neurons and the second with 100 neurons. The output layer has 10 neurons, in accordance with the 10 binary classes of the MNIST dataset. Then, we need placeholder nodes that will receive the data (features and labels). Next, we build the layers. There are two options: (i) construction \"by hand\" with an own function and (ii) using function provided by TensorFlow. Here, we try out both but one network needs to be deleted (or something like that) before the other one can be used. Restarting the kernel might not be necessary but is sufficient to switch from own construction to construction via TensorFlow.<br>\n",
    "Let's go through the function that is used for own construction of the layers:\n",
    "1. In order to keep the graph neat, a name scope for the layer is defined.\n",
    "2. The number of inputs is the number of features is the size of the second dimension of the data (first dimension = instances).\n",
    "3. W â or the *kernel* of that layer â will be a node for the matrix holding the weights for this layer. Its shape is (n_inputs, n_neurons). The weights shall be initialized randomly (to avoid certain symmetries - e.g. all weights are 0 - that might compromise training) and truncated (i.e., have a cutoff, which reduces the probability of slow training). A normal Gaussian distribution with a standard deviation of $2/\\sqrt{n_{\\rm inputs}}$ (and witha cutoff) is used. This standard deviation can speed up training tremendously for reasons that are going to be discussed in chapter 11.\n",
    "4. Node \"b\" will hold the biases.\n",
    "5. Compute $Z=X\\cdot W+b$. In this vectorized form, all instances (of the current batch) are processed in one go.\n",
    "6. Finally, pass $Z$ to an activation function, if provided, and return the result. If no activation function is provided just return $Z$.\n",
    "\n",
    "After construction of the layers - via the own function or directly via TensorFlow - is finished, we use softmax regression to assign probabilities to the classes and cross entropy to penalize probabilities that differ from perfect prediction of the correct class.<br><br>\n",
    "**General note**<br>\n",
    "The spares_softmax_cross_entropy_with_logits() function is equivalent to applying the softmax activation function and then computing the cross entropy, but it is more efficient, and it properly takes care of corner cases like logits equal to 0. This is why we did not apply the softmax activation function earlier. There is also another function called softmax_cross_entropy_with_logits(), which takes labels in the form of one-hot vectors (instead of ints from 0 to the number of classes minus 1).<br><br>\n",
    "With the cost function ready, we next implement training via gradient descent with the \"train\" node. We will also want to evaluate the quality of the predictions. Here, we use the network's overall accuracy within the \"eval\" node. The node also uses the \"in_top_k()\" function. With $k=1$, it checks whether the prediction with the highest assigned probability is correct. Finally, there is a global variable initializer node and a saver node.<br><br>\n",
    "The execution phase starts by loading the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# CONSTRUCTION\n",
    "# DNN overall structure\n",
    "n_inputs = 28 * 28 # MNIST features size\n",
    "n_hidden1 = 300    # neurons in hidden layer 1\n",
    "n_hidden2 = 100    # neurons in hidden layer 2\n",
    "n_outputs = 10     # outputs\n",
    "# data placeholder nodes\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # placeholder for features\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")             # placeholder for labels\n",
    "# choose on option to build the two hidden layer and the output layer\n",
    "dnn_choice = 2\n",
    "if dnn_choice is 1:\n",
    "    # function for building a layer\n",
    "    def neuron_layer(X, n_neurons, name, activation=None):\n",
    "        with tf.name_scope(name):\n",
    "            n_inputs = int(X.get_shape()[1])                                 # number of features\n",
    "            stddev = 2 / np.sqrt(n_inputs)                                   # standard deviation, see text above\n",
    "            init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) # initial matrix with random weigths\n",
    "            W = tf.Variable(init, name=\"kernel\")                             # node holding the weights\n",
    "            b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")              # node holding the biases\n",
    "            Z = tf.matmul(X, W) + b                                          # input to activation function\n",
    "            if activation is not None:                                       # return activation(Z) or Z\n",
    "                return activation(Z)\n",
    "            else:\n",
    "                return Z\n",
    "    # build the layers via the function\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu) # take X as input use ReLU\n",
    "        hidden2 = neuron_layer(X, n_hidden2, name=\"hidden2\", activation=tf.nn.relu) # the same, but input is \"hidden1\"\n",
    "        logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")                   # now \"hidden2\", keep output as is\n",
    "else:\n",
    "    # build the layers directly from tensorflow\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)      # everything clear ...\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",activation=tf.nn.relu) # ... no furhter ...\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")                        # ... explanation required\n",
    "# define a loss function and node\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits) # labels = classes (0-9) ...\n",
    "                                                                                    # ... logits = previous output\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")                                    # apply cross entropy to the ...\n",
    "                                                                                    # ... returned probabilities\n",
    "# build the optimizer and give it a learning rate\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):                                     # training node\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) # use gradient descent ...\n",
    "    training_op = optimizer.minimize(loss)                       # ... on the loss function (cross entropy)\n",
    "# specify evaluation\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)                  # check whether the topmost prediction is correct\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # get accuracy\n",
    "# initialize all variables and use a saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# EXECUTION\n",
    "# get the MNIST dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data # load the data ...\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")            # ... from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the number of epochs and the number of instances in a batch. Then start a session, run the initializer and loop throuhg the epochs. In each epoch, loop through all the batches and succesively, train the alogrithm on all batches. At the end of an epoch, evaluate the accuracy for the current batch and for the test set and print the results on screen. After all epochs are run, save the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9 Test accuracy: 0.9045\n",
      "1 Train accuracy: 0.94 Test accuracy: 0.924\n",
      "2 Train accuracy: 0.96 Test accuracy: 0.9335\n",
      "3 Train accuracy: 0.96 Test accuracy: 0.9354\n",
      "4 Train accuracy: 0.92 Test accuracy: 0.9441\n",
      "5 Train accuracy: 0.98 Test accuracy: 0.9476\n",
      "6 Train accuracy: 0.98 Test accuracy: 0.9516\n",
      "7 Train accuracy: 1.0 Test accuracy: 0.9527\n",
      "8 Train accuracy: 0.94 Test accuracy: 0.9564\n",
      "9 Train accuracy: 0.98 Test accuracy: 0.9589\n",
      "10 Train accuracy: 0.94 Test accuracy: 0.9603\n",
      "11 Train accuracy: 0.98 Test accuracy: 0.9645\n",
      "12 Train accuracy: 0.98 Test accuracy: 0.9642\n",
      "13 Train accuracy: 0.96 Test accuracy: 0.9658\n",
      "14 Train accuracy: 0.98 Test accuracy: 0.9667\n",
      "15 Train accuracy: 0.96 Test accuracy: 0.9693\n",
      "16 Train accuracy: 0.94 Test accuracy: 0.969\n",
      "17 Train accuracy: 1.0 Test accuracy: 0.9703\n",
      "18 Train accuracy: 1.0 Test accuracy: 0.9707\n",
      "19 Train accuracy: 0.96 Test accuracy: 0.9703\n",
      "20 Train accuracy: 0.98 Test accuracy: 0.9721\n",
      "21 Train accuracy: 1.0 Test accuracy: 0.9723\n",
      "22 Train accuracy: 0.98 Test accuracy: 0.9725\n",
      "23 Train accuracy: 1.0 Test accuracy: 0.9736\n",
      "24 Train accuracy: 0.98 Test accuracy: 0.9734\n",
      "25 Train accuracy: 1.0 Test accuracy: 0.9732\n",
      "26 Train accuracy: 0.98 Test accuracy: 0.9755\n",
      "27 Train accuracy: 1.0 Test accuracy: 0.9749\n",
      "28 Train accuracy: 1.0 Test accuracy: 0.974\n",
      "29 Train accuracy: 0.98 Test accuracy: 0.9755\n",
      "30 Train accuracy: 1.0 Test accuracy: 0.9749\n",
      "31 Train accuracy: 0.98 Test accuracy: 0.9748\n",
      "32 Train accuracy: 1.0 Test accuracy: 0.975\n",
      "33 Train accuracy: 1.0 Test accuracy: 0.9756\n",
      "34 Train accuracy: 1.0 Test accuracy: 0.9761\n",
      "35 Train accuracy: 1.0 Test accuracy: 0.9767\n",
      "36 Train accuracy: 1.0 Test accuracy: 0.9767\n",
      "37 Train accuracy: 1.0 Test accuracy: 0.9769\n",
      "38 Train accuracy: 0.98 Test accuracy: 0.9769\n",
      "39 Train accuracy: 1.0 Test accuracy: 0.9772\n"
     ]
    }
   ],
   "source": [
    "# specify epochs and batches\n",
    "n_epochs = 40   # number of epochs\n",
    "batch_size = 50 # batch size\n",
    "# train the algorithm for all epochs on all batches\n",
    "with tf.Session() as sess:                                                               # session start\n",
    "    init.run()                                                                           # initialize variables\n",
    "    for epoch in range(n_epochs):                                                        # loop through epochs\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):                  # loop through batches\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)                        # get a batch\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})                    # train on the batch\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})                    # accuracy on the batch\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels}) # accuracy on the test set\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)           # print the results\n",
    "    save_path = saver.save(sess, \"./tf_logs/10_NN_Intro/my_model_final.ckpt\")            # save the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Neural Network\n",
    "page 269<br>\n",
    "With the model being trained, we can now infer instances. Therefore, we first restore the model, then get a few new instances from the test set, calculate the logits for each instance, and pick the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/10_NN_Intro/my_model_final.ckpt\n",
      "Predicted classes: [7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "Actual classes:    [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_instances = 20                                              # number of instances from the test set\n",
    "with tf.Session() as sess:                                            # start a session\n",
    "    saver.restore(sess, \"./tf_logs/10_NN_Intro/my_model_final.ckpt\")  # restore the saved model\n",
    "    X_new_scaled = X_test[:number_of_instances]                       # specify new instances\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})                      # calculate the logits for each instance\n",
    "    y_pred = np.argmax(Z, axis=1)                                     # pick the highest probability for each instance\n",
    "# print the predicted classes and check whether all instances have been processed\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", y_test[:20])\n",
    "len(y_pred) is number_of_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters\n",
    "page 270<br>\n",
    "The flexibility of neural networks poses also a problem: there are just so many possibilities for a network structure (number of layers, number of neurons in a layer, connections, activation functions, initialization of weights and biases, learning rate, etc.) that the choice of hyperparameters still remains some kind of black magic. As for hyperparameter search, randomized search is much more efficient than grid search (see also chapter 2). Another option is to use tools like Oscar: http://oscar.calldesk.ai.\n",
    "\n",
    "### Number of Hidden Layers\n",
    "page 270<br>\n",
    "It has been shown that a neural network with only one hidden layer can calculate *any* function. It only takes a sufficient number of neurons in this layer. For this reason, researchers initially did not give much attention to multilayer networks. However, the *parameter efficiency* is usually much better in networks with more layers. This can be understood as follows: imagine the task of drawing a forest in a graphics program (e.g. inkscape). Without being allowed to copy, one has to draw every leaf to get a twig, every twig to get a branch, every branch to get a tree, and every tree to get the forest. Every time, all the leaves, twigs, etc. have to be drawn from scratch (no copying). Obviously, it would be much faster to draw a few leaves, copy them in a random manner to get a twig, and do the same with twigs, branches, and trees. One would be finished much, much sooner since repetitive tasks have to be done only once.\n",
    "\n",
    "In deep neural networks, different layers match different patterns: low-level patterns like edges etc. in lower layers, then possibly shapes in intermediate layers, and objects at a higher layer. It would be inefficient if every object needed to be matched by *its* neurons in a single layer since most of these neurons recognize things that other neurons also recognize for another object. Using more layers speeds up training (due to higher parameter efficiency) but can also be helpful for generalization. Tasks that mainly differ at a high level can share the low and intermediate level layers, at least as a starting point. As a start, one layer should usually be fine. From there, one may incrementally add layers until the model starts to overfit. It is also very common to start with a pretrained model instead of starting from scratch.\n",
    "\n",
    "### Number of Neurons per Hidden Layer\n",
    "page 271<br>\n",
    "The number of input neurons obviously should match the number of features in the data. Likewise, 10 output neurons are obviously appropriate for the MNIST dataset with 10 exclusive classes. It has been consensus that a funnel structure for the hidden layers is appropriate, the idea being that many low-level features, coalesce into fewer high-level features. On the other hand, all the richness of the universe seems to emerge from only 16 particles in the standard model. Those 16 particles give rise to much more than 16 objects. Anyway, the former consensus of using a funnel-shaped layout is not widely shared, anymore. And for the MNIST dataset, one may also get good results with two layers of 150 neurons each. Just like with the number of layers, one can increase the number of neurons per layer until overfitting sets in. In general, increasing the number of layers is improves the results more than increasing the number of neurons per layer.\n",
    "\n",
    "Another, more brute force, way to get a good model is to use more layers and neurons than one actually expects to need and then use early stopping to prevent the model from overfitting. This is also called the *stretch pants* approach, akin to buying large stretch pants and waiting until they shrink in to fit: it avoids having to deal with a tedious search.\n",
    "\n",
    "### Activation Functions\n",
    "page 272<br>\n",
    "The ReLU function (or one of its variants, see chapter 11) is a good default activation function because it is easier to compute than many other activation functions and because gradient descent usually does not get stuck on plateaus due to the fact that ReLU does not saturate for large input values (as opposed to the logistic function or hyperbolic tangent function that saturate at 1). As for output, the softmax activation function is usually a good choice for classification (when the classes are mutually exclusive). For regresseion, one may simply use the last layers outputs, without any further activation function.\n",
    "\n",
    "## Exercises\n",
    "page 272\n",
    "### 1.-8.\n",
    "Solutions are shown in Appendix A of the book and in the separate notebook *ExercisesWithoutCode*.\n",
    "### 9.\n",
    "Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of Chapter 9, try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 90.240% \tLoss: 0.35380\n",
      "Epoch: 5 \tValidation accuracy: 95.120% \tLoss: 0.17921\n",
      "Epoch: 10 \tValidation accuracy: 96.520% \tLoss: 0.12785\n",
      "Epoch: 15 \tValidation accuracy: 97.180% \tLoss: 0.10320\n",
      "Epoch: 20 \tValidation accuracy: 97.480% \tLoss: 0.09166\n",
      "Epoch: 25 \tValidation accuracy: 97.620% \tLoss: 0.08206\n",
      "Epoch: 30 \tValidation accuracy: 97.740% \tLoss: 0.07877\n",
      "Epoch: 35 \tValidation accuracy: 97.820% \tLoss: 0.07419\n",
      "Epoch: 40 \tValidation accuracy: 97.840% \tLoss: 0.07160\n",
      "Epoch: 45 \tValidation accuracy: 98.080% \tLoss: 0.06740\n",
      "Epoch: 50 \tValidation accuracy: 98.040% \tLoss: 0.06734\n",
      "Epoch: 55 \tValidation accuracy: 98.000% \tLoss: 0.06683\n",
      "Epoch: 60 \tValidation accuracy: 98.040% \tLoss: 0.06728\n",
      "Epoch: 65 \tValidation accuracy: 98.180% \tLoss: 0.06668\n",
      "Epoch: 70 \tValidation accuracy: 98.180% \tLoss: 0.06600\n",
      "Epoch: 75 \tValidation accuracy: 98.140% \tLoss: 0.06639\n",
      "Epoch: 80 \tValidation accuracy: 98.120% \tLoss: 0.06659\n",
      "Epoch: 85 \tValidation accuracy: 98.220% \tLoss: 0.06595\n",
      "Epoch: 90 \tValidation accuracy: 98.200% \tLoss: 0.06724\n",
      "Epoch: 95 \tValidation accuracy: 98.160% \tLoss: 0.06879\n",
      "Epoch: 100 \tValidation accuracy: 98.220% \tLoss: 0.06863\n",
      "Epoch: 105 \tValidation accuracy: 98.200% \tLoss: 0.07053\n",
      "Epoch: 110 \tValidation accuracy: 98.220% \tLoss: 0.07035\n",
      "Epoch: 115 \tValidation accuracy: 98.320% \tLoss: 0.07038\n",
      "Epoch: 120 \tValidation accuracy: 98.220% \tLoss: 0.07294\n",
      "Epoch: 125 \tValidation accuracy: 98.300% \tLoss: 0.07122\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# The entire solution of exercise 9 is from the github link above.\n",
    "# firts, define the architecture\n",
    "n_inputs = 28*28  # MNIST features\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "# function to reset the graph\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "# feature and label placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# neural network within node \"dnn\"\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "# loss node with softmax (=> probabilities) and cross entropy (=> penalty for wrong probabilities)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "# training node and learning rate\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# evaluation node uses only the highest probability and calculates the accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)                     # check whether the topmost prediction is correct\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    # get the accuracy\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy) # append the accuracy to the summary\n",
    "# initialization node\n",
    "init = tf.global_variables_initializer()\n",
    "# saver node\n",
    "saver = tf.train.Saver()\n",
    "# import datetime and use it to build a function that return a path for saving the logged data\n",
    "from datetime import datetime\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") # date string\n",
    "    root_logdir = \"./tf_logs/10_NN_Intro\"            # default directory\n",
    "    if prefix:                                       # if a prefix has been specified ...\n",
    "        prefix += \"-\"                                # ... append a \"-\" sign\n",
    "    name = prefix + \"run-\" + now                     # in any case, append \"run-\" and the date string from above\n",
    "    return \"{}/{}/\".format(root_logdir, name)        # build the complete path and return it\n",
    "# specify a directory\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "# create a file writer\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "# get the number of instances (m) and features (n)\n",
    "m, n = X_train.shape\n",
    "# training schedule\n",
    "n_epochs = 1001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "# checkpoints etc.\n",
    "checkpoint_path = \"./tf_logs/10_NN_Intro/tmp/my_deep_mnist_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./tf_logs/10_NN_Intro/my_deep_mnist_model\"\n",
    "# iniitalize some quantities\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "# build a function that randomly puts instances into a batch\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "# And now open a session and run this thing!\n",
    "with tf.Session() as sess:\n",
    "    # if the checkpoint file exists, restore the model, including the session, and load the epoch number\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    # if no checkpoint file exists, start from the beginning (with a new session)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "    # now loop through the (remaining epochs)\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # in each epoch, loop through all the (randomized) batches and run the training operation on each batch\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # caluclate metrics\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary,\n",
    "                                                                                   loss_summary],\n",
    "                                                                                  feed_dict={X: X_valid, y: y_valid})\n",
    "        # save the results\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        # print a report every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,                                             # current epoch\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100), # current accuracy on validation set\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))                           # current loss on validation set\n",
    "            saver.save(sess, checkpoint_path)                                  # save a checkpoint for the session\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:                       # open epoch checkpoints and ...\n",
    "                f.write(b\"%d\" % (epoch + 1))                                   # update the epoch\n",
    "            # if the loss has decreased (good), update the according quantities\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            # if it has nos decreased over the last 5 epochs, ...\n",
    "            else:\n",
    "                # ... then count the epochs without progress ...\n",
    "                epochs_without_progress += 5\n",
    "                # ... and when this count exceeds a certain threshold, ...\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    # ... then it is time to stop (early, i.e., before the model totally goes south overfitting)\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, all the model is eplained via comments. It stops early once the loss function on the validation set seems to permanently rise again (overfitting). Below, the best model before the onset of overfitting is restored and evaluated with the accuracy on the **test set** (since this is the *final* model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/10_NN_Intro/my_deep_mnist_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9796"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove(checkpoint_epoch_path)                                    # get the checkpoint of the \"best\" epoch\n",
    "with tf.Session() as sess:                                          # restart the session ...\n",
    "    saver.restore(sess, final_model_path)                           # ... of that checkpoint and restore the model \n",
    "    accuracy_test = accuracy.eval(feed_dict={X: X_test, y: y_test}) # get the accuracy ...\n",
    "accuracy_test                                                       # ... and print it to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have already learned quite a lot about artificial neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
