{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises and Appendix A\n",
    "The idea of this notebook is to gather all the exercises (and answers) that do not rquire coding. Those exercises that do require code are planned to be covered in the notebook of the according chapter. All answers shown here can also be found in Appendix A (page 475) of the book.\n",
    "## Exercises in Chapter 1\n",
    "page 31\n",
    "### 1. How would you define Machine Learning?\n",
    "A machine is said to learn a taks T with increasing experience E if its performance on this task T, as measured by some performance measure P, increases with E.\n",
    "<br>Book solution: Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure.\n",
    "### 2. Can you name four types of problems where it shines?\n",
    "- Problems that require a lot of hand-tuning or long lists of rules: these tasks can be automated by using a Machine Learning algorithm.\n",
    "- Complex problems for which there seem to be no traditional solutions. Machine Learning might find a good solution.\n",
    "- Significantly fluctuating input data: a Machine Learning algorithm can adapt to new input data.\n",
    "- Getting insights about complex problems and large amounts of data.\n",
    "\n",
    "<br>Book solution: Machine Learning is great for complex problems for which we have no algorithmic solution, to replace long lists of hand-tuned rules, to build systems that adopt to fluctuating environments, and finally to help humans learn (e.g., data mining).\n",
    "### 3. What is a labeled training set?\n",
    "A training set where all instances also have the target feature. Labeled training sets are used for supervised Machine Learning.\n",
    "<br>Book solution: A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance.\n",
    "### 4. What are the two most common supervised tasks?\n",
    "Classification and regression.\n",
    "<br>Book solution: The two most common supervised tasks are regression and classification.\n",
    "### 5. Can you name four common unsupervised tasks?\n",
    "Clustering (e.g., clustering customers into groups), visualization (of networks or graphs), dimensionality reduction (get rid of redundant features), and anomaly detection (e.g., credit card fraud).\n",
    "<br>Book solution: Common unsupervised tasks inlude clustering, visualization, dimensionality reduction, and association rule learning.\n",
    "### 6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains?\n",
    "An algorithm that uses \"reinforcement learning\", i.e., an algorithm that accumulates \"rewards\" for desired behavior and \"penalties\" for undesired behavior.\n",
    "<br>Book solution: Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to expres the problem as a supervised or semisupervised learning problem, but it would be less natural.\n",
    "### 7. What type of algorithm would you use to segment your customers into multiple groups?\n",
    "Unsupervised Machine Learning.\n",
    "<br>Book solution: If you don't know how to define the groups, then you can use a clustering algorithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classifcation algorithm (supervised learning), and it will classify all your customers into these groups.\n",
    "### 8. Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem?\n",
    "As a supervised learning problem.\n",
    "<br>Book solution: Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam).\n",
    "### 9. What is an online learning system?\n",
    "One that (i) automatically includes newly available data in its learning process or (ii) employs out-of-core learning.\n",
    "<br>Book solution: An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data.\n",
    "### 10. What is out-of-core learning?\n",
    "Cutting huge amounts of data (so huge that it cannot be fed to the system at once) in pieces (\"mini-batches\"), training the system on the first piece, and then feeding the system the remaining data piece-by-piece as if these pieces were new data in an online system. The difference to actual online learning is that the \"new\" pieces are not new and the \"online\" learning happens actually offline.\n",
    "<br>Book solution: Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches.\n",
    "### 11. What type of learning algorithm relies on a similarity measure to make predictions?\n",
    "Instance-based learning.\n",
    "<br>Book solution: An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learnd instances and uses them to make predictions.\n",
    "### 12. What is the difference between a model parameter and a learning algorithm's hyperparameter?\n",
    "A model parameter is optimized in the process of training a model-based learning algorithm. This is not possible in an instance-based learning algorithm simply because there are no model parameters. But both model-based and instance-based algorithms use hyperparameters that determine the overall structure of the algorithm. Examples:\n",
    "- Model-based algorithm<br>\n",
    "The order of a polynomial for polynomial regression is a hyperparameter while the prefactor to a certain power is a model parameter.\n",
    "- Instance-based algorithm<br>\n",
    "The kind of norm used for a similarity measure is also a hyperparameter.\n",
    "\n",
    "<br>Book solution: A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperaparameter is a parameter of the learning algorithm  itslef, not of the model (e.g.,  the amount of regularization to apply).\n",
    "### 13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n",
    "Model-based algorithms aim to optimize the model parameters versus some performance measure. The most common strategy is regression, aiming to minimize a cost function. These algorithms make predictions by applying the model to a new instance, the output being usually a number (or several numbers).\n",
    "<br>Book solution: Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions on the training data, plus a penalty for model complexity if the model is regularized. To make predicitons, we feed the new instance's features into the model's  prediction function, using the parameter values found by the learning algorithm.\n",
    "### 14. Can you name four of the main challenges in Machine Learning?\n",
    "Insufficient quantity of training data (results in insufficient learning), erroneous data (results in wrong predictions), irrelevant features in data (slows down the training process), overfitting (good on training data but bad on new data), and underfitting (model does not capture the complexity and makes na√Øve i.e. poor predictions).\n",
    "<br>Book solution: Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple models that underfit the training data, and excessively complex models that overfit the data.\n",
    "### 15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?\n",
    "Probably, overfitting is happening. Get more data, higher quality, or regularize the model via hyperparameters.\n",
    "<br>Book solution: If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data.\n",
    "### 16. What is a test set and why would you want to use it?\n",
    "To simulate new instances and test the established model on these \"new\" instances.\n",
    "<br>Book solution: A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production.\n",
    "### 17. What is the purpose of a validation set?\n",
    "To test several version of several algorithm. First, shortlist a few promising algorithms, then optimize these, and finally choose the best. All this is done using a validation set (or via cross-validation). The test set remains untouched in this entire process.\n",
    "<br>Book solution: A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters.\n",
    "### 18. What can go wrong if you tune hyperparameters using the test set?\n",
    "The final model might end up as a fit to the test set. Being some kind of fit to this data, the model might not generalize well to all-new data.\n",
    "<br>Book solution: If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect).\n",
    "### 19. What is cross-validation and why would you prefer it to a validation set?\n",
    "Cross validation does not split the training set (the counterpart of the test set) further into sets for training and validation but uses the entire set for training. Validation is still possible as follows.\n",
    "- The training set is cut in k (usually k=5 or k=10) folds, all of which should preserve representative ratios of features.\n",
    "- The model is trained on k-1 folds and validated on the remaining fold.\n",
    "- This previous step is performed k times so every fold has been used for validation once.\n",
    "\n",
    "The advantages of cross-validation are that, first, the model is trained on all the training data, i.e., on more data. Secondly, performing the validation k times allows for a measure on the insecurity of the model, e.g., by calculating the standard deviation of the performance on the k validation folds.\n",
    "<br>Book solution: Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparamter tuning) without the need for a separate validation set. This saves precious training data.\n",
    "## Exercises in Chapter 2\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 3\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 4\n",
    "page 142\n",
    "### 1. What Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
    "If you have a training set with millions of features you can use Stochastic Gradient Descent  or Mini-batch Gradient Descent, and perhaps Batch Gradient Descent if the training set fits in memory. But you cannot use the Normal Equation because the computational complexity grows quickly (more than quadratically) with the number of features.\n",
    "### 2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
    "If the features in your training set have very different scales, the cost function will have the shape of an elongated bowl, so the Gradient Descent algorithms will take a long time to converge. To solve this you should scale the data before training the model. Note that the Normal Equation will work just fine without scaling. Moreover, regularized models may converge to a suboptimal solution if the features are not scaled: indeed, since regularzation penalizes large weights, features with smaller values will tend to be ignored compared to features with larger values.\n",
    "### 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "Gradient Descent cannot get stuck in a local minimum when training a Logistic Regression model because the cost function is convex.\n",
    "### 4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "If the optimization problem is convex (such as Linear Regression or Logistic Regression), and assuming the learning rate is not too high, then all Gradient Descent algorithms will apporach the global optimum and end up producing fairly similar models. However, unless you gradually reduce the learning rate, Stochastic GD and mini-batch GD will never truly converge; instead, they will keep jumping back and forth around the global optimum. This means that even if you let them run for a very long time, these Gradient Descent algorithms will porudce slightly different models.\n",
    "### 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "If the validation error consistently goes up after every eopoch, then one possibility is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training.\n",
    "### 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "Due to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training iteration. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals, and when it has not improved for a long time (meaning it will porbably never beat the record), you can revert to the best saved model.\n",
    "### 7. Which Gadient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "Stochastic Gradient Descent has the fastest training iteration since it considers only one training instance at a time, so it is generally the first to reach the vicinity of the global optimum (or Mini-batch GD with a very small min-batch size). However, only Batch Gradient Descent will actually converge, given enough training time. As mentioned, Stocastic GD and Mini-batch GD will bounce around the optimum, unless you gradually reduce the learning rate.\n",
    "### 8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation eror. What is happening? What are three ways to solve this?\n",
    "If the validation error is much higher than the training error, this is likely because your model is overfitting the training set. One way to try to fix this is to reduce the polynomial degree: a model with fewer degrees of freedom is less likely to overfit. Another thing you can try is to reularize the model -- for example, by adding an $L_2$ penalty (Ridge) or an $L_1$ penalty (Lasso) to the cost function. This will also reduce the degrees of freedom of the model. Lastly, you can try to increase the size of the training set.\n",
    "### 9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameeter $\\alpha$ or reduce it?\n",
    "If both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter $\\alpha$.\n",
    "### 10. Why would you want to use:\n",
    "- **Ridge Regression  instead of plain Linear Regression (i.e., without any regularization)?**\n",
    "- **Lasso instead of Ridge Regression?**\n",
    "- **Elastic Net instead of Lasso?**\n",
    "\n",
    "Let's see:\n",
    "- A model with some regulaization typically performs better than a model without any regularization, so you should generally prefer Ridge Regression over plain Linear Regression.\n",
    "- Lasso Regression uses an $L_1$ penalty, which tends to push the weights down to exactly zero. This leads to sparse models, where all weights are zero except for the most important weights. This is a way to perfrom feature selection automatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression.\n",
    "- Elastic Net is generally preferred over Lasso since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training intances). However, it does add an extra hpyerparameter to tune. If you just want Lasso without the erratic behavior, you can just use Elasic Net with an $L_1$_ratio close to 1.\n",
    "\n",
    "### 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "If you want to classify pictures as outdoor/indoor and daytime/nighttime, since these are not exclusive classes (i.e., all four combinations are possible) you should train two Logistic Regression classifiers.\n",
    "### 12.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 5\n",
    "page 165\n",
    "### 1. What is the fundamental idea behind Support Vector Machines?\n",
    "The fundamental idea behind Support Vector Machines is to fit the widest possible \"street\" between the classes. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. When performing soft margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets.\n",
    "### 2. What is a support vector?\n",
    "After training an SVM, a *support vector* is any instance located on the \"street\" (see the previous answer), including its border. The decision boundary is entirely determined by the support vectors. Any instance that is *not* a support vector (i.e., off the street) has no influence whatsoever; you could remove them, add more instances, or move them around, and as long as they stay off the street they won't affect the decision boundary. Computing the predictions only involves the support vectors, not the whole training set.\n",
    "### 3. Why is it important to scale the inputs when using SVMs?\n",
    "SVMs try to fit the largest posible \"street\" between the classes (see the first answer), so if the training set is not scaled, the SVM will tend to neglect small features (see Figure 5-2).<br>\n",
    "Own note: The possible neglection of small features comes about from the fact that machine algorithms may implicitly (machine limit) or explicitly (*tolerance* etc.) fixed boundaries on their accuracy.\n",
    "### 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set  *probability=True* when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logisitc Regression on the SVM's scores (trained by an addtitional five-fold cross-validation on the training data). This will add the *predict_proba()* and *predict_log_proba()* methods to the SVM.\n",
    "### 5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
    "This question applies only to linear SVMs since kernelized can only use the dual form. The computational coplexity of the primal form of th SVM problem is proportional to the number of training instances $m$, while the computational complexity of the dual form is proportional to a number between $m^2$ and $m^3$. So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow.\n",
    "### 6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the traninig set: should you increase or decrease $\\gamma$ (gamma)? What about $C$?\n",
    "If an SVM classifier trained with an RBF kernel underfits the training set, there might be too much regularization. To decrease it, you need to increase $\\gamma$ or $C$ (or both).\n",
    "### 7. How should you set the QP parameters ($H$, $f$, $A$, and $b$) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
    "\n",
    "Let's call the QP parameters for the hard-margin problem $H'$, $f'$, $A'$ and $b'$ (see \"Quadratic Programming\" on page 159). The QP parameters for the soft margin problem have $m$ additional parameters ($n_p=n+1+m$) and $m$ additional constraints ($n_c=2m$). They can be defined like so:\n",
    "\n",
    "- $H$ is equaal to $H'$, plus $m$ columns of 0s on the right and $m$ rows of 0s at the bottom:\n",
    "$H=\\begin{pmatrix}H'&0&\\cdots\\\\0&0&\\\\\\vdots&&\\ddots\\end{pmatrix}$.\n",
    "\n",
    "- $f$ is equal to $f'$ with $m$ additional elements, all equal to the value of the hyperparameter $C$.\n",
    "- $b$ is equal to $b'$ with $m$ additional elements, all equal to 0.\n",
    "- $A$ is equal to $A'$, with an extra $m\\times m$ identitiy matrix $I_m$ appended to the right, $-I_m$ just below it, and the rest filled with zeros: $A=\\begin{pmatrix}A'&I_m\\\\0&-I_m\\end{pmatrix}$.\n",
    "\n",
    "A confirmation of the above relations is shown in the file \"Gradients.pdf\".\n",
    "### 8.-10.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 6\n",
    "page 178\n",
    "### 1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million  instances?\n",
    "If every leaf node will contain only one instance and all above nodes split the set in a balanced way, $m=m_{\\rm leaves}=2^{n_{\\rm nodes}}$ is the number of instances. So $n_{\\rm nodes}=log_2m$ is the approximate depth.<br>\n",
    "Book solution: The depth of a well-balanced binary tree containing *m* leaves is equal to $log_2m^3$, rounded up. A binary Decision Tree (one that makes only binary decisions, as is the case of all trees in Scikit-Learn) will end up more or less well balanced a the end of training, with one leaf per training instance if it is trained without restrictions. Thus, if the training set contains one million instances, the Decision Tree will have a depth of $log_210^6\\approx20$ (actually a bit more since the tree will generally not be perfectly well balanced).\n",
    "\n",
    "### 2. Is a node's Gini impurity generally lower or greater than its parent's? Is it *generally* lower/greater, or *always* lower/greater?\n",
    "A node's Gini impurity is generally lower than its parent's. This is due to the CART training algorithm's cost function, which splits each node in a way that minimizes the weighted sum of its children's Gini impurities. However, it is possible for a node to have a higher Gini impurity than its parent, as long as this increase is more than compensated for by a decrease of the other child's impurity. For example, consider a node containing four instances of class A and 1 of class B. Its Gini impurity is $1-\\frac{2^2}{5}-\\frac{3^2}{5}=0.32$. Now suppose the dataset is one-dimensinal and the instances are lined up in the following order: A,B, A, A, A. You can verify that the algorithm will split this node after the second instance, producing one child note with instances A, B, and the other child node with instances A, A, A. The first child node's Gini impurity is $1-\\frac{1^2}{2}-\\frac{1^2}{2}=0.5$, which is higher than its parent. This is compensated for by the fact that the other node is pure, so the overall weighted Gini impurity is $\\frac{2}{5}\\times 0.5+\\frac{3}{5}\\times0=0.2$, which is lower than the parent's Gini impurity.\n",
    "\n",
    "### 3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing *max_depth*?\n",
    "If a Decision Tree is overfitting the training set, it may be a good idea to decrease *max_depth*, since this will constrain the model, regularizing it.\n",
    "\n",
    "### 4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "Scaling the input features is irrelevant for Decision Trees as the treat all features independently.<br>\n",
    "Book solution: Decision Trees don't care whether or not the training data is scaled or centered; that's one of the nice things about them. So if a Decision Tree underfits the training set, scaling the input features will just be a waste of time.\n",
    "\n",
    "### 5. If it takes one hour to train a Decisoin Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?\n",
    "The computational complexity of training a Decision Tree is $\\mathcal{O}(n\\times m\\log(m))$. So if you multiply the training set size by 10, the training time will be multiplied by $K=(n\\times10m\\times\\log(10m))/(n\\times m\\times\\log(m))=10\\times\\log(10m)/\\log(m)$. If $m=10^6$, then $K\\approx11.7$, so you can expect the training time to be roughly 11.7 hours.<br>\n",
    "Note that the basis of the logarithm is 10, here: $\\log(10)=1$.\n",
    "### 6. If your training set contains 100000 instances, will setting *presort=True* speed up training?\n",
    "No, presorting with Scikit-Learn is only effective up to about a thousand instances.<br>\n",
    "Book solution: Presorting the training set speeds up training only if the dataset is smaller than a few thousand instances. If it contains 100000 instances, setting *presort=True* will considerably slow down training.\n",
    "### 7.-8.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 7\n",
    "page 202\n",
    "### 1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "If you have trained five different models and they all achieve 95% precision, you can try combining them into a voting ensemble, which will often give you even better results. It works better if the models are very different (e.g., an SVM classifier, A Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are trained on different training instances (that's the whole point of bagging and pasting ensembles), but if not it will still work as long as the models are very different.\n",
    "### 2. What is the difference between hard and soft voting classifiers?\n",
    "A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets the most votes. A soft voting classifier computes the average estimated class probability of each class and picks the class with the highest probability. This gives high-confidence votes more weight and often perfroms better, but it works only if every classifier is able to estimate class probabilities (e.g., for the sVM classifiers in Scikit-Learn you must set \"probability=True\").\n",
    "### 3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the same reason. However, each predictor in a boosting ensemble is built based on the previous predcitor, so training is necessarily sequential, and you will not gain anything by distributing training across multiple servers. Regarding stacking ensembles, all the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.\n",
    "### 4. What is the benefit of out-of-bag evaluation?\n",
    "With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated using instances that it was not trained on (they were held out). This makes it possible to have a fairly unbiased evaluation of the ensemble without the need for an additional validation set. Thus, you have more instances available for training, and your ensemble can perform slightly better.\n",
    "### 5. What makes Extra-Trees more random then regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    "When you are growing a tree in a Random Forest, only a random subset of the features is considered for splitting at each node. This is true as well for Extra-Trees, but they go one step further: rather than searching for the best possible thresholds, like regular Decision Trees do, they use random thresholds for each feature. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perfrom better. Moreover, since Extra-Trees don't search for the best possible thresholds, they are much faster to train than Random Forests. However, they are neither faster nor slower than Random Forests when making predictions.\n",
    "### 6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    "If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.\n",
    "### 7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many).\n",
    "### 8.-9.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 8\n",
    "page 224\n",
    "### 1. What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?\n",
    "The main motivations for dimensionality reduction are:\n",
    "- To speed up a subsequent training algorithm (in some cases it may even remove noise and redundant featrures, making the training algorithm perform better).\n",
    "- To visualize the data and gain insights on the most important features.\n",
    "- Simply to save space (compression).\n",
    "\n",
    "The main drawbacks are:\n",
    "- Some information is lost, possibly degrading the performance of subsequent training algorithms.\n",
    "- It can be computationally intensive.\n",
    "- It adds some complexity to your Machine Learning pipelines.\n",
    "- Transformed features are often hard to interpret.\n",
    "\n",
    "### 2. What is the curse of dimensionality?\n",
    "The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high-dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data.\n",
    "### 3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n",
    "Once a dataset's dimensionality has been reduced using on of the algorithms we discussed, it is almost always impossible to perfectly reverse the operation, because some information gets lost during dimensionality reduction. Moreover, while some algorithms (such as PCA) have a simple reverse transformation procedure that can reconstruct a dataset relatively similar to the original, other algorithms (such as T-SNE) do not.\n",
    "### 4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
    "PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear, because it can at least get rid of useless dimensions. However, if there are no useless dimensions - for example, the Swiss roll - then reducinng dimensionality with PCA will lose too much information. You want to unroll the Swiss roll, not squash it.\n",
    "### 5. Suppose you perfrom PCA on a 1000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
    "That's a trick question: it depends on the dataset. Let's look at two extreme examples. First, suppose the dataset is composed of points that are almost perfectly aligned. In this case, PCA can reduce the dataset down to just one dimesnion while still perserving 95% of the variance. Now imagine that the dataset is composed of perfectly random points, scatterd all around the 1000 dimensions. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950. Plotting the explained variance as a function of the number of dimensions is one way to get a roguh idea of the dataset's intrinsic dimensionality.\n",
    "### 6. In what cases would you use vanilla PCA, Icremental PCA, Randomized PCA, or Kernel PCA?\n",
    "Regular PCA is the default, but it works only if the dataset fits in memory. Incremental PCA is useful for large datasets that don't fit in memory, but it is slower than regular PCA, so if the dataset fits in memory you should prefer regular PCA. Incremental PCA is also useful for online tasks, when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datsets.<br>Remark: The word \"vanilla\" refers to the default version of an algorithm. The analogy comes from ice cream, where the vanilla flavour can be considered a \"default\" choice of flavour because basically everybody likes it. In the end, it might not be the best choice but it is a good choice if you do not know where to start.\n",
    "### 7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?\n",
    "Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the orignal dataset.\n",
    "### 8. Does it make any sense to chain two different dimensionality reduction algorithms?\n",
    "It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algirthm, such as LLE. This two-step approach will likely yield the same performance as using LLE only, but in a fraction of the time.\n",
    "### 9.-10.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 9\n",
    "page 251\n",
    "### 1. What are the main benefits of creating a computation graph rather than directly executing the computations? What are the main drawbacks?\n",
    "Main benefits and drawbacks of creating a computation graph rather than directly executing the computations:<br>\n",
    "*Main benefits:*\n",
    "- TensorFlow can automatically compute the gradients for you (using reverse-mode autodiff).\n",
    "- TensorFlow can take care of running the operations in parallel in different threads.\n",
    "- It makes it easier to run the same model across different devices.\n",
    "- I simplifies introspection -- for example, to view the model in TensorBoard.\n",
    "\n",
    "*Main drawbacks:*\n",
    "- It makes the learning curve steeper.\n",
    "- It makes step-by-step debugging harder.\n",
    "\n",
    "### 2. Is the statement \"a_val = a.eval(session=sess)\" equivalent to \"a_val = sess.run(a)\"?\n",
    "Yes, the statement \"a_val = a.eval(session=sess)\" is indeed equivalent to \"a_val = sess.run(a)\".\n",
    "### 3. Is the statement \"a_val, b_val = a.eval(session=sess), b.eval(session=sess)\" equivalent to \"a_val, b_val = sess.run([a, b])\"?\n",
    "No, the statement \"a_val, b_val = a.eval(session=sess), b.eval(session=sess)\" is not equivalent to \"a_val, b_val = sess.run([a, b])\". Indeed, the first statement runs the graph twice (once to compute \"a\", once to compute \"b\"), while the second statement runs the graph only once. If any of these operations (or the ops they depend on) have side effects (e.g., a variable is modified, an item is inserted in a queue, or a reader reads a file), then the effects will be different. If they don't have side effects, both statements will return the same result, but the second statement will be faster than the first.\n",
    "### 4. Can you run two graphs in the same session?\n",
    "No, you cannot run two graphs in the same session. You would have to merge the graphs into a single graph first.\n",
    "### 5. If you create a graph \"g\" containing a variable \"w\", then start two threads and open a session in each thread, both using the same graph \"g\", will each session have its own copy of the variable \"w\" or will it be shared?\n",
    "In local TensorFlow, sessions manage variable values, so if you create a graph \"g\" containing a variable \"w\", then start two threads and open a local session in each thread, both using the samge graph \"g\", then each session will have its own copy of the variable \"w\". However, in distributed TensorFlow, variable values are stored in containers mangaged by the cluster, so if both sessions connect to the same cluster and use the same container, then they will share the same variable values for \"w\".\n",
    "### 6. When is a variable initialized? When is it destroyed?\n",
    "A variable is initialized when you call its initializer, and it is destroyed when the session ends. In distributed TensorFlow, variables live in containers on the cluster, so closing a session will not destroy the variable. To destroy a variable, you need to clear its container.\n",
    "### 7. What is the difference between a placeholder and a variable?\n",
    "Variables and placeholders are extremely different, but beginners often confuse them:\n",
    "- A variable is an operation that holds a value. If you run the variable, it returns that value. Before you can run it, you need to initialize it. You can change the variable's value (for example, by using an assignement operation). It is stateful: the variable keeps the same value upon successive runs of the graph. It is typically used to hold model parameters but also for other purposes (e.g., to count the global training step).\n",
    "- Placeholders technically don't do much: they just hold information about the type and shape of the tensor they represent, but they have no value. In fact, if you try to evaluate an operation that depends on a placeholder, you must feed TensorFlow the value of the placeholder (using the \"feed_dict\" argument) or else you will get an exception. Placeholders are typically used to feed training or test data to TensorFlow during the execution phase. They are also useful to pass a value to an assignment node, to change the value of a variable (e.g., model weights).\n",
    "\n",
    "### 8. What happens when you run the graph to evaluate an operation that depends on a placeholder but you don't feed its value? What happens if the operation does not depend on the placeholder?\n",
    "If you run the graph to evaluate an operation that depends on a placeholder but you don't feed its value, you get an exception. If the operation does not depend on the placeholder, then no exception is raised.\n",
    "### 9. When you run a graph, can you feed the output value of any operation, or just the value of placeholders?\n",
    "When you run a graph, you can feed the output value of any operation, not just the value of placeholders. In practice, however, this is rather rare (it can be useful, for example, when you are caching the output of frozen layers; see Chapter 11).\n",
    "### 10. How can you set a variable to any value you want (during the execution phase)?\n",
    "You can specify a variable's initial value when constructing the graph, and it will be initialized later when you run the variable's initializer during the execution phase. If you want to change that variable's value to anything you want during the execution phase, then the simplest option is to create an assignment node (during the graph construction phase) using the \"tf.assign()\" function, passing the variable and a placeholder as parameters. During the execution phase, you can run the assignment operation and feed the variable's new value using the placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2518282\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                                             # import tensorflow\n",
    "x = tf.Variable(tf.random_uniform(shape=(), minval=0.0,maxval=1.0)) # variable x\n",
    "x_new_val = tf.placeholder(shape=(), dtype=tf.float32)              # placeholder x_new_val\n",
    "x_assign = tf.assign(x, x_new_val)                                  # assignment node x_assign assigns x_new_val to x\n",
    "with tf.Session():                                                  # start a tensorflow session\n",
    "    x.initializer.run()                                             # initialize the variable x\n",
    "    print(x.eval())                                                 # evaluate x (random number in [0,5]) and print it\n",
    "    x_assign.eval(feed_dict={x_new_val: 5.0})                       # run x_assign while feeding a value to the ...\n",
    "                                                                    # ... placeholder => updated x\n",
    "    print(x.eval())                                                 # evaluate x again and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation?\n",
    "Reverse-mode autodiff (implemented by TensorFlow) needs to traverse the graph only twice in order to compute the gradients of the cost function with regards to any number of variables. On the other hand, forward-mode autodiff would need to run once for each variable (so 10 times if we want the gradients with regards to 10 different variables). As for symbolic differentiation, it would build a different graph to compute the gradients, so it would not traverse the original graph at all (except when building the new gradients graph). A highly optimized symbolic differentiation system could potentially run the new gradients graph only once to compute the gradients with regards to all variables, but that new graph may be horribly complex and inefficient compared to the orginigal graph.\n",
    "### 12.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 10\n",
    "page 272\n",
    "### 1. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes $A\\oplus B$ (where $\\oplus$ represents the XOR operation). Hint: $A\\oplus B=(A\\wedge\\neg B)\\vee(\\neg A\\wedge B)$.\n",
    "<img src=\"images/XOR_network_cut.jpg\">\n",
    "### 2. Why is it generally preferable to use a Logistic regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Pereptron training algorithm)? How can you tweak a Perceptron to make it equilvalent to a Logistic Regression classifier?\n",
    "A classical Perceptron will converge only if the dataset is linearly separable, and it won't be able to estimate class probabilities. In contrast, a Logistic Regression classifier will converge to a good solution even if the dataset is not linearly separable, and itw will output class probabilities. If you change the Perceptron's activation function to the logistic activation function (or the softmax activation function if there are multiple neurons), and if you train it using Gradient Descent (or some other optimization algorithm minimizing the cost function, typically cross entropy), then it becomes equivalent to a Logistic Regression classifier.\n",
    "### 3. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "The logistic activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all.\n",
    "### 4. Name three poplular activation functions. Can you draw them?\n",
    "The step function, the logistic function, the hyperbolic tangent, the rectified linear unit (see Figure 10-8). See Chapter 11 for other examples, such as ELU and variants of ReLU.\n",
    "### 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial nerurons. All artificial neurons use the ReLU activation function.\n",
    "- What is the shape of the input matrix $X$?\n",
    "- What about the shape of the hidden layer's weight vector $W_h$, and the shape of its bias vecor $b_h$?\n",
    "- What is the shape of the output layer's weight vector $W_o$, and its bias vector $b_o$?\n",
    "- What is the shape of the network's output matrix $Y$?\n",
    "- Write the equation that computes the network's output matrix $Y$ as a function of $X$, $W_h$, $b_h$, $W_o$, and $b_0$.\n",
    "\n",
    "Considering the MLP described in the question: suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "- The shape of the input matrix $X$ is $m\\times10$, where $m$ represents the training batch size.\n",
    "- The shape of the hidden layer's weight vector $W_h$ is $10\\times50$ and the length of its bias vector $b_h$ is 50.\n",
    "- The shape of the output layer's weight vector $W_0$ is $50\\times3$ and the length of its bias vector $b_0$ is 3.\n",
    "- The shape of the network's output matrix $Y$ is $m\\times3$.\n",
    "- $Y={\\rm ReLU}\\left({\\rm ReLU}(X\\cdot W_h+b_h)\\cdot W_0+b_0\\right).$ Recall that the ReLU fucntion just sets every negative number in the matrix to zero. Also note that when you are adding a bias vector to a mtrrix, it is added to every single row in the matrix, which is called *broadcasting*.\n",
    "\n",
    "### 6. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "To classifiy email into spam or ham, you just need one neuron in the output layer of a neural network ‚Äì for example, indicating the probability that the email is spam. You would typically use the logistic activation function in the output layer when estimating a probability. If instead you want to tackle MNIST, you ndeed 10 neurons in the output layer, and you must replace the logistic funciton with the softamx activation function, which can handle mutliple classes, outputting one probability per class. Now, if you want your neural netork to predict housing prices like in Chapter 2, then you need one output neuron, using no activation function at all in the output layer.<br>\n",
    "When the values to predict can vary by many orders of magnitude, then you may want to predict the logarighm of the target value rather than the target value directly. Simply comuting the exponentiral of the nerual network's output will give you the estimated value (since $\\exp(\\log v)=v$).\n",
    "### 7. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parameter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that (hopefully) minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodfiff (although it wasn't called that when backpropagation was invented, and it has been reinvented several times). Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once (see Appendix D for more details). So what's the differnce? Well, backpropagation refers to the whole process of training an artificial neural network using mutliple backpropagation steps, each of which computes gradients and uses them to perfrom a Gradient Descent step. In contrast, reverse-mode autodiff is simply a technique to compute gradients efficiently, and it happens to be used by backpropagation.\n",
    "### 8. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "Here is a list of all the hyperparameters you can tweak in a basic MLP: the number of hidden layers, the number of neurons in each hidden layer, and the activation function used in each hidden layer and in the output layer. In general, the ReLU activation function (or one of its variants; see Chapter 11) is a good default for the hidden layers. For the output layer, in general you will want the logistic activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression.<br>\n",
    "If the MLP overfits thr training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden  layer.<br>\n",
    "In Chapter 11, we discuss many techniques than introduce additional hyperparameters: type of weight initialization, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping threshold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparemeters (e.g., dropout rate when using dropout) and so on.\n",
    "### 9.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "## Exercises in Chapter 11\n",
    "page 313\n",
    "### 1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetries: if all the weights have the same initial value, even if that value is not zero, then symmetry isn not broken (i.e., all neurons in a given layer are equivalent), and backpropagation well be unable to break it. Concretely, this means that all the neruons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution.<br>\n",
    "Own answer: No, it is still very symmetric.\n",
    "### 2. Is it okay to initialize the bias terms to 0?\n",
    "It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that's okay too; it does not make much difference.\n",
    "### 3. Name three advantages of the ELU activation function over ReLU.\n",
    "A few advantages of the ELU function  over the ReLU function are:\n",
    "- It can take on negative values, so the average output of the neurons in any given layer is typically closer to 0 than when using the ReLU activation function (which never outputs negative values). This helps alleviate the vanishing gradients problem.\n",
    "- It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units.\n",
    "- It is smooth everywhere, wheareas the ReLU's slope abruplty jumps form 0 to 1 at $z=0$. Such an abrupt change can slow down Gradient Descent because it will bounce around $z=0$.\n",
    "\n",
    "Own answer: It is differentiable, has a mean closer to zero, and a finite gradient also for negative values.\n",
    "### 4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "The ELU activation function is a good default. If you need the nerual network to be as fast as possible, you can use the leaky ReLU variants instead (e.g., a simple leaky ReLU using the default hyperparameter value). The simplicity of the ReLU activation function makes it many people's preferred option, despite the fact that they are generally outperformed by the ELU and leaky ReLU. However, the ReLU activation function's capability of outputting precisely zero can be useful in some cases (e.g., see Chapter 15). The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number between -1 and 1, but nowadays it is not used much in hidden layers. The logistic activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but it is also rarely used in hidden layers (there are exceptions--for example, for the coding layer of variational autoencoders; see Chapter 15). Finally, the softmax activation function is useful in the output layer to output probabilities for mutually exclusive classes, but other than that it is rarely (if ever) used in hidden layers.<br>\n",
    "Own answer: (i) ELU: best performance, (ii) leaky ReLU: good performance and fast, (iii) ReLU: simple, no hyperparameter, (iv) tanh: mean 0, and (v) logistic and softmax: classification.\n",
    "### 5. What may happen if you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using a `MomentumOptimizer`?\n",
    "If you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using a `MomentumOptimizer`, then the algorithm will likely pick up a lot of speed hopefully roughly toward the global minimum, but then it will shoot right past the minimum, due to its momentum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller `momentum` value.<br>\n",
    "Own answer: Due to the high momentum the algorithm will overshoot and converge very slowly (or even diverge).\n",
    "### 6. Name three ways you can produce a sparse model.\n",
    "One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply $l_1$ regularization during training, which pushes the optimizer toward sparsity. A third option is to combine $l_1$ regulrization with *dual averaging*, using TensorFlow's `FTRLOptimizer` class.<br>\n",
    "Own answer: (i) use l1 regularization, (ii) clip tiny weights to 0, (iii) use the *follow the regularized leader* (FTRL) algorithm, or (iv) use `tf.sparse` instead of `tf.keras.layers.Dense`.\n",
    "### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference since it is only turned on during training.<br>\n",
    "Own answer: training \"yes\", inference \"no\".\n",
    "### 8.-10.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "## Exercises in Chapter 12\n",
    "page 354\n",
    "### 1. If you get a `CUDA_ERROR_OUT_OF_MEMORY` when starting your TensorFlow program, what is probably going on? What can you do about it?\n",
    "When a TensorFlow process starts, it grabs all the available memory on all GPU devices that are visile to it, so if you get a `CUDA_ERROR_OUT_OF_MEMORY` when starting your TensorFlow program, it probably means that other processes are running that have already grabbed all the memory on at least one visible GPU device (most likely it is another TensorFlow process). To fix this problem, a trivial solution is to stop the other processes and try again. However, if you need all processes to run simultaneously, a simple option is to dedicate different devices to each process, by setting the `CUDA_VISIBLE_DEVICES` environement variable appropriately for each device. Another opiton is to configure TensorFlow to grab only part of the GPU memory, instead of all of it, by creating a `ConfigProto`, setting its `gpu_options.per_process_gpu_memory_fraction` to the proportion of the total memory that it should grab (e.g., 0.4), and using this `ConfigProto` when opening a session. The last option is to tell TensorFlow to grab memory only when it needs it by setting the `gpu_options.allow_growth` to `True`. However, this last option is usually not recommended because any memory that TensorFlow grabs is never released, and it is harder to guarantee a repeatable behavior (there  may be race conditions depending on which porcesses start first, how much memory they need during training, and so on).<br>\n",
    "Own answer: GPU does not have enough memory. Tell the GPU to use CPU memory, use the CPU entirely, use GPU with enough memory, reduce memory need (e.g., smaller network or smaller batch), allocate appropriate fraction of memory per device, shut down old TensorFlow session / graph, or reserve an entire GPU (with enough GPU RAM) for this additional process.\n",
    "### 2. What is the difference between pinning an operation on a device and placing an operation on a device?\n",
    "By pinning an operation on a device, you are telling TensorFlow that this is where you would like this operation to be placed. However, some constraints may prevent TensorFlow from honoring your request. For  example, the operation may have no implementation (called a *kernel*) for that particular type of device. In this case, TensorFlow will raise an exception by default, but you can configure it to fall back to the CPU instead (this is called *soft placement*). Another example is an operation that can modify a variable; this operation and the variable need to be colocated. So the difference between pinning an operation and placing an operation is that pinning is what you ask TensorFlow (\"Please place this operation o GPU #1\") while placement is what TensorFlow actually ends up doing (\"Sorry, falling back to the CPU\").<br>\n",
    "Own answer: Every operation is placed on a device (by TensorFlow). Placing may involve pinning, i.e., a preference specified by the user, e.g., via a `with tf.device(\"/gpu:1\"):` block.\n",
    "### 3. If you are running on a GPU-enabled TensorFlow installation, and you just use the default placement, will all operations be placed on the first GPU?\n",
    "If you are running on a GPU-enabled TensorFlow installation, and you just use the default placement, then if all operations have a GPU kernel (i.e., a GPU implementation), yes, they will all be placed on the first GPU. However, if one or more operations do not have a GPU kernel, then by default TensorFlow will raise an exception. If you configure TensorFlow to fall back to the CPU instead (soft placement), then all operations will be placed on the first GPU except the ones without a GPU kernel and all the operations that must be collocated with them (see the answer to the previous exercise).<br>\n",
    "Own answer: If some operation does not have a GPU kernel (=*implementation of the task* dedicated to the device), it will be placed on the CPU.\n",
    "### 4. If you pin a variable to `\"/gpu:0\"`, can it be used by operations placed on `\"/gpu:1\"`? Or by operations placed on `\"/cpu:0\"`? Or by operations pinned to devices located on other servers?\n",
    "Yes, if you pin a variable to `\"/gpu:0\"`, it can be used by operatons placed on `\"/gpu:1\"`. TensorFlow will automatically take care of adding the appropriate operations to transfer the varible's value across devices. The same goes for devices located on different servers (as long as they are part of the same cluster).<br>\n",
    "Own answer: Yes.\n",
    "### 5. Can two operations placed on the same device run in parallel?\n",
    "Yes, two operations placed on the same device can run in parallel: TensorFlow automatically takes care of running operations in parallel (on different CPU cores or different GPU threads), as long as no operation depends on another operation's output. Moreover, you can start multiple sessions in parallel threads (or processes), and evaluate operations in each thread. Since sessions are independent, TensorFlow will be able to evaluate any operation from one session in parallel with any operation from another session.<br>\n",
    "Own answer: Yes, as long as they do not depend on each other.\n",
    "### 6. What is a control dependency and when would you want to use one?\n",
    "Control dependencies are used when you want to postpone the evaluation of an operation `X` until after some other operations are run, even though these operations are not required to compute `X`. This is useful in particular when `X` would occupy a lot of memory and you only need it later in the computation graph, or if `X` uses up a lot of I/O (for example, it requires a large variable value located on a different device or server) and you don't want it to run at the same time as other I/O-hungry operations, to avoid saturating the bandwidth.<br>\n",
    "Own answer: Control dependencies allow the user to delay the execution of a certain step until something (needs to be specified) has happened. This can be useful to avoid bottlenecking. So it can be used for an overall speedup of the calculation (bottlenecking is very inefficient).\n",
    "### 7. Suppose you train a DNN for days on a TensorFlow cluster, and immediately after your training program ends you realize that you forgot to save the model using a `Saver`. Is your trained model lost?\n",
    "You're in luck! In distributed TensorFlow, the variable values live in containers managed by the cluster, so even if you close the session and exit the client program, the model parameters are still alive and well on the cluster. You simply need to open a new session to the cluster and save the model (make sure you don't call the variable initializers or restore a previous model, as this would destroy your previous new model!).<br>\n",
    "Own answer: It is not lost, yet. One may start a new session and save the parameters. (Without using an initializer in between!)\n",
    "### 8.-10.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "## Exercises in Chapter 13\n",
    "page 378\n",
    "### 1. What are the advantages of a CNN over a fully connected DNN for image classiciaction?\n",
    "These are the main advantages of a CNN pver afully connected DNN for image classification:\n",
    "- Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connevted DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
    "- When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since  images typically have very  repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
    "- Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisicve head start compared to DNNs.\n",
    "\n",
    "### 2. Consider a CNN composed of three convolutional layers, each with $3\\times3$ kernels, a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images 0f $200\\times300$ pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediciton for a single instance? what about when training on a mini-batch of 50 images?\n",
    "Let's compute how many parameters the CNN has. Since its first convolutional layer has 3x3 kernels, and the input has three channels (red, green, and blue), then each feature maps has 3x3x3 weights, plus a bias term. That's 28 parameters per feature map. Since this first convolutional layer has 100 feature maps, it has a total of 2800 parameters. The second convolutional layer has 3x3 kernels, and its input is the set of 100 feature maps of the previous layer, so each feature map has 3x3x100=900 weights, plus a bias term. Since it has 200 feature maps, this layer has 901x200=180200 parameters. Finally, the third and last convolutional layer also has 3x3 kernels, and its input is the set of 200 feature maps of the previous layers, so each feature map has 3x3x200=1800 weights, plus a bias term. Since it has 400 feature maps, this layer has a total of 1801x400=730400 parameters. All in all, the CNN has 2800+180200+720400=903400 parameters.<br><br>\n",
    "Now let's compute how much RAM this neural network will require (at least) when making a prediciton for a single instance. First let's compute the feature map size for each layer. Since we are using a stride of 2 and SAME padding, the horizontal and vertical size of the feature maps are divided by 2 at each layer (rounding up if necessary), so as the input channels are 200x300 pixels, the first layer's feature maps are 100x150, the second layer's feature maps are 50x75, and the third layer's feature maps are 25x38. Since 32 bits is 4 bytes and the first convolutional layer has 100 feature maps, this first layer takes up 4x100x150x100=6 million bytes (about 5.7MB, considering that 1MB=1024KB and 1KB =1024 bytes). The second layer takes up 4x50x75x200=3 million bytes (about 2.9MB). Finally, the third layer takes up 4x25x38x400=1520000 bytes (about 1.4MB). However, once a layer has been computed, the memory occupied by the previous layer can be released, so if everything is well optimized, only 6+9=15 million bytes (about 14.3MB) of RAM will be required (when the second layer has just been computed, but the memory occupied by the first layer is not released yet). But wait, you also need to add the memory occupied by the CNN's parameters. We computed earlier that it has 903400 parameters, each using up 4 bytes, so this adds 3613600 bytes (about 3.4MB). The total RAM required is (at least) 18613600 bytes (about 17.8MB).<br><br>\n",
    "Lastly, let's compute the minimum amount of RAM required when training the CNN on a mini-batch of 50 images. During training TensorFlow uses backpropagation, which requires keeping all values computed during the forward pass until the reverse pass begins. So we must compute the total RAM required by all layers for a single instance and mutliply that by 50! At that point let's start counting in megabytes rather than bytes. We computed before that the three layers require respectively 5.7, 2.9, and 1.4MB for each instance. That's a total of 10.0MB per instance. So for 50 instances the total RAM is 500MB. Add to that the RAM required by the input images, which is 50x4x200x300x3=36 million bytes (about 34.3MB, plus the RAM required for the model parameters, which is about 3.4 MB (computed earlier), plus some RAM for the gradents (we will neglect them since they can be released gradually as backpropagation goes down the layers during the reverse pass). We are up to a total of roughly 500.0+34.3+3.4=537.7MB. And that's really an optimistic bare minimum.\n",
    "### 3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "- Reduce the mini-batch size.\n",
    "- Reduce dimensionality using a larger stride in one or more layers.\n",
    "- Remove one or more layers.\n",
    "- Use 16-bit floats instead of 32-bit floats.\n",
    "- Disctribute the CNN across multiple devices.\n",
    "\n",
    "### 4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few (see the previous questions).<br>\n",
    "Moreover, pooling layers with stride greater than 1 reduce the size of the layer, thus lowering the number of paramters in following layers.\n",
    "### 5. When would you want to add a *local response normalization* layer?\n",
    "A *local response normalization* layer makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps, which encourages different feature maps to specialize and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon.<br>\n",
    "So local response normalization might be helpful if a network does not learn enough low-level features. Low performance on some tasks together with good performance on another taks (e.g. classification of different classes) will usually see an improvement from local response normalization.\n",
    "### 6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet and ResNet?\n",
    "The main innovations in AlexNet compared to LeNet-5 are (1) it is much larger and deeper, and (2) it stacks convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. The main innovation in GoogLeNet is the introducion of *inception modules*, which make it possible to have a much deeper net than previous CNN architectures, with fewer parameters. Finally, ResNet's main innovation is the introduction of skip connections, which make it possible to go well beyond 100 layers. Arguably, its simplicity  and consistency are alo rather innovative.<br>\n",
    "For AlexNet, the use of *local response normalization* should also be mentioned. If a neuron/position in a feature map has a high response the according neurons/positions in neighboring feature maps will be suppressed, this inciting the different layers to learn respond to features. This is usually applied to low-level layers. However, it might be that it has been used before AlexNet.\n",
    "### 7.-9.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 14\n",
    "page 412\n",
    "### 1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to vector RNN? And a vector-to-sequence RNN?\n",
    "Here are a few RNN applications:\n",
    "- For e a sequence-to-sequence RNN: predicting the weather (or any other time series), machine translation (using an encoder-decoder architecture), video captioning, speech to text, music generation (or other sequence generation), identifying the chords of a song.\n",
    "- For a sequence-to-vector RNN classifiyng music samples by music genre, analyzing the sentiment of a book review, predicting what word an aphasic patient is thinking of based on readings from brain implants, predicting the probability that a user will want to watch a movie based on her watch history (this is one of many possible implementations of *collaborative filtering*).\n",
    "- For a vector-to-sequence RNN: image captioning, creating a music playlist based on an embedding of the current artist, generating a melody based on a set of parameters, locating pedestrians in a picture (e.g., a video frame fom a self-driving car's camera).\n",
    "\n",
    "Own answer: Possible Seq2Seq applications include language translation, voice cloning, and time series prediction, e.g., on cryptocurrency prices. Seq2Vec: quantum state measurement classification and recommendation systems (recommend an instance with a similar embedding). Vec2Seq: creation of a sentence (or song) that is different but similar to an exsiting sentence (or song).\n",
    "### 2. Why do people use encoder-decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence \"Je vous en prie\" means \"You are welcome\", but if  you translate it one word at a time, you get \"I you in pray\". Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an encoder-decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain squence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast).<br>\n",
    "Own answer: One reason is that the lengths of the original and translated sentences will generally differ and another reason is that the first output would be generated before the entire input has been fed (otherwise it would effectively be an encoder-decoder network).\n",
    "### 3. How could you combine a convolutional neural network with an RNN to classify videos?\n",
    "To classify videos based on the visual content, one possible architecture could be to take (say) one frame per second, then run each frame through a convolutional neural network, feed the output of the CNN to a sequence-to-vector RNN, and finally run its output through a softmax layer, giving you all the class probabilities. For training you would just use cross entropy as the cost function. If you wanted to use the audio for classification as well, you could convert every second of audio to a spectrograph, feed this spectrograph to a CNN, and feed the output of this CNN to the RNN (along with the corresponding output of the other CNN).<br>\n",
    "Own answer: One possibility would be to use a CNN to assign individual frames (not necessarily all frames, one frame per second may work) a fixed number of labels, say three. Then, run an RNN on that time series of labels and put a softmax (multiclass-) classifier on top.\n",
    "### 4. What are the advantages of building an RNN using `dynamic_rnn()` rather than `static_rnn()`?\n",
    "Building an RNN using `dynamic_rnn()` rather than `static_rnn()` offers several advantages:\n",
    "- It is based on a `while_loop()` operation that is able to swap the GPU's memory to the CPU's memory during backpropagation, avoiding out-of-memory errors.\n",
    "- It is arguably easier to use, as it can directly take a single tensor as input and output (covering all time steps), rather than a list of tensors (one per time step). No need to stack, unstack, or transpose.\n",
    "- It generates a smaller graph, easier to visualize in TensorBoard.\n",
    "\n",
    "Own answer: By unrolling the RNN with `dynamic_rnn()`, one does not need to explicitly feed the cells' states as input at the next time step (which could become quite unpractical for large RNNs). Moreover, `dynamic_rnn()` offers the option to swap the memory from the GPU to the CPU via `swap_memory=True`. This way, a possible OOM (out of memory) error on the GPU can be avoided.\n",
    "### 5. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "To handle variable length input sequences, the simplest option is to set the *sequence_length* parameter when calling the `static_rnn()` or `dynamic_rnn()` functions. Another option is to pad the smaller inputs (e.g., with zeros) to make them the same size as the largest input (this may be faster than the first option if the input sequences all have very similar lengths). To handle variable-length output sequences, if you know in advance the length of each output sequence, you can use the `sequence_length` parameter (for example, consider a sequence-to-sequence RNN that labels every frame in a video with a violence score: the output sequence will be exactly the same length as the input sequence). If you don't know in advance the length of the output sequence, you can use the padding trick: always output the same size sequence, but ignore any outputs that come after the end-of-sequence token (by ignoring them when computing the cost function).<br>\n",
    "Own answer: Both of them can be padded with $\\text{<eos>}$ (end-of-sequence) tokens.\n",
    "### 6. What is a common way to distribute training and execution of a deep RNN across mulitple GPUs?\n",
    "To distribute training and execution of a deep RNN across multiple GPUs, a common technique is simply to place each layer on a different GPU (see Chapter 12).<br>\n",
    "Own answer: First of all, the problem is that `BasicRNNCell()` and the like a cell factories and not finished cells. And one wants to distribute finished cells not cell factories. One way to do this is to use a custom wrapper that uses separate cell factories for different layers. \n",
    "### 7.-9.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Exercise 9 is also addressed in the Natural Language Processing section of Chapter 14 (see own Jupyter notebook on Chapter 14).\n",
    "<br>\n",
    "## Exercises in Chapter 15\n",
    "page 438\n",
    "### 1. What are the main tasks that autoencoders are used for?\n",
    "Here are some of the main tasks that autoencoders are used for:\n",
    "- Feature extraction\n",
    "- Unsupervised pretraining\n",
    "- Dimenstionality reduction\n",
    "- Generative models\n",
    "- Anomaly detection (an autoencoder is generally bad at reconstructing outliers)\n",
    "\n",
    "Unsupervised pretraining, denoising data, and generating new instances of a certain kind of data (class).\n",
    "### 2. Suppose you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances. How can autoencoders help? How would you proceed?\n",
    "If you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances, then you could first train a deep autoencoder on the full dataset (labeled + unlabeld), then reuse its lower half for the classifier (i.e., reuse the layers up to the codings layer, included) and train the classifer using the labeld data. If you have little labeld data, you probablly want to freeze the reused layers when training the classifier.<br>\n",
    "With unsupervised pretraining of the lower layers of the classificators: these layers may be taken from the lower layers of an autoencoder up to (and including) the coding layer.\n",
    "### 3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?\n",
    "The fact that an autoencoder perfectly reconstructs its inputs does not neessarily mean that  it is a good autoencoder; perhaps it is simply an overcomplete autoencoder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible for a very deep autoencoder to learn to map each training instance to a different coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the third to 0.003, and so on), and it could learn \"by heart\" to reconstruct the right training instance for each coding. It would perfectly reconstruct its inputs without really learning any useful pattern in the data. In practice such a mapping is unlikely to happen, but it illustrates the fact that perfect reconstructions are not a guarantee that the autoencoder learned anything useful. However, if it produces very bad reconstructions, then it is almost guaranteed to be a bad autoencoder. To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss (e.g., compute the MSE, the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good sign that the autoencoder is bad, but a low reconstruction loss is not a guarentee that it is good. You should also evaluate the autoencoder according to what it will be used for. For example, if you are using it for unsupervised pretraining of a classifier, then you should also evaluate the classifier's performance.<br>\n",
    "It is not a useful autoencoder if it merely copies the input to the output. The autoencoder is useful if it reproduces the input well and has learned the patterns in the data. The latter is driven by constraining the autoencoder. If the autoencoder is used for unsupervised pretraining of a classification algorithm, the final performance of that classificator gives also an indication.\n",
    "### 4. What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?\n",
    "An undercomplete autoencoder is one whose codings layer is smaller than the input and output layers. If it is larger, then it is an overcomplete autoencoder. The main risk of an excessively undercomplete autoencoder is that it may fail to reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful feature.<br>\n",
    "Undercomplete [overcomplete] autoencoders have a coding layer with dimension lower [higher] than that of the input layer. Using an overcomplete autoencoder bears the risk of the autencoder just copying the inputs to the outputs without learning anything.\n",
    "### 5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
    "To tie the weights of an encoder layer and its corresponding decoder layer, you simply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making training converge faster with less training data, and reducing the risk of overfitting the training set.<br>\n",
    "Autoencoders are usually symmetric wrt the coding layer. So the weights of the encoder are usually just the transpose of the weights of the decoder. Reducing the number of trainable variables this way can speed up training.\n",
    "### 6. What is a common technique to visualize features learned by the lower layer of a stacked autoencoder? What about higher layers?\n",
    "To visualize the features learned by the lower layer of a stacked autoencoder, a common technique is simply to plot the weights of each neuron, by reshaping each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape `[784]` to `[28, 28]`). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. <br>\n",
    "The neurons of the topmost hidden layer can be visualized by arranging their weights in a 2D array and plotting these weights as pictures. The weights / features of lower layers can be visualized by starting with a random input and then training the input until it maximally activates the selected neuron.\n",
    "### 7. What is a generative model? Can you name a type of generative autoencoder?\n",
    "A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized ‚Äì for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder.<br>\n",
    "Variational autoencoders learn the means and variances of normal distributions instead of a weights in the coding layer. The idea is that a certain type of data (e.g., dog pictures) can be sufficiently approximated by its collection of means and standard deviations in the coding layer (\"latent space\" or our \"idea\" of the data ‚Äì without being too specific). Once the coding layer has been trained (\"latent space\" is established) one may just produce a random coding (data point in the \"latent space\") and have it decoded from the decoder to output a new instance.\n",
    "### 8.-10.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml.\n",
    "<br>\n",
    "## Exercises in Chapter 16\n",
    "page 473\n",
    "### 1. How would you define Reinforcement Learning? How is it different from regular supervised or unsupervised learning?\n",
    "Reinforcement Learning is an area of Machine Learning aimed at creating agents capable of taking actions in an environment in a way that maximizes rewards over time. There are many difference between RL and regular supervised and unsupervised learning. Here are a few:\n",
    "- In supervised and unsupervised learning, the goal is generally to find patterns in the data. In Reinforcemnt Learning, the goal is to find a good policy.\n",
    "- Unlike in superviesd learning, the agent is not explicitly given the \"right\" answer. It must learn by trial and error.\n",
    "- Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing.\n",
    "- A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsuperivsed learning systems generally don't need to worry about exploration; they just feed on the training data they are given.\n",
    "- In supervised and unsupervised learning, training instances are typically independent (in fact, they are generally shuffled). In Reinforcement Learning, consecutive observations are generally *not* independent. An agent may remain in the same region of the environment for a while before it moves on, so consecutive observations will be very correlated. In some cases a replay memory is used to ensure that the training algorithm gets fairly independent observations.\n",
    "\n",
    "### 2. Can you think of three possible applications of RL that were not mentined in this chapter? For each of them, what is the environment? What is the agent? What are ppossible action? What are the rewards?\n",
    "Here are a few possible applications of Reinfocrement Learning, other than those mentioned in Chapter 16:\n",
    "- *Music personalization:* The environment is a user's personalized web radio. The agent is the software deciding what song to play next for that user. Its possible actions are to play any song in the catalog (it must try to choose a song the user will enjoy) or to play an advertisement (it must try to choose an ad that the user will be interested in). It gets a small reward every time the user listens to a song, a larger reward every time the user listens to an ad, a negative reward when the user skips a song or an ad, and a very negative reward if the user leaves.\n",
    "- *Marketing:* The environment is your company's marketing departement. The agent is the software that defines which customers a mailing campaign should be sent to, given their profile and purchase history (for each customer it has two possible actions: send or don't send). It gets a negative reward for the cost of the mailing campaign, and a positive reward for estimead revenue generated from this campaign.\n",
    "- *Product delivery:* Let the agent control a fleet of delivery trucks, deciding what they should pick up at the depots, where they should go, what they should drop off, and so on. They would get positive rewards for each product delivered on time, and negative rewards for late deliveries.\n",
    "\n",
    "### 3. What is the discount rate? Can the optimal policy change if you modify the discount rate?\n",
    "When estimating the value of an action, Reinforcement Learning algorithms typically sum all the rewards that this action led to, giving more weight to immediate rewards, and less weight to later rewards (considering that an action has more influcence on the near future than on the distant future). To model this, a discount rate is typically applied at each time step. Fore example, with a discount rate of 0.9, a reward of 100 that is received two time steps later is counted as only $0.9^2\\times100=81$ when you are estmating the value of the action. You can think of the discount rate as a measure of how much the future is valued relative to the present: if it is very close to 1, then the future is valued almost as much as the present. If it is close to 0, then only immediate rewards matter. Of course, this impacts the optimal policy tremendously: if you value the future, you may be willing to put up with a lot of immediate pain for the prospect of eventual rewards, while if you don't value the future, you will just grab any immediate reward you can find, never investing in the future.\n",
    "### 4. How do you measure the performance of a Reinforcement Learning agent?\n",
    "To measure the performance of a Reinforcement Learning agent, you can  simply sum up the rewards it gets. In a simulated environment, you can run many episodes and look at the total rewards it gets on average (and possibly look at the min, max, standard deviation, and so on).\n",
    "### 5. What is the credit assignment problem? When does it occur? How can you alleviate it?\n",
    "The credit assignment problem is the fact that when a Reinforcement Learning agent receives a reward, it has no direct way of knowing which of its previous actions contributed to this reward. It typically occurs when there is a large delay between an action and the resulting rewards (e.g., during a game of Atari's *Pong*, there may be a few dozen time steps between the moment the agent hits the ball and the moment it wins the point). One way to alleviate it is to provide the agent with shorter-term rewards, when possible. This usually requires prior knowledge about the task. For example, if we want to build an agent that will learn to play chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent's pieces.\n",
    "### 6. What is the point of using a replay memory?\n",
    "An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can introduce some bias in the learning algorithm. It wmay tune its policy for this region of the envrironment, but it will not perform well as soon as it moves out ot this region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?).\n",
    "### 7. What is an off-policy RL algorithm?\n",
    "An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts optimally), independently of how the agent actually acts. Q-Learning is a good example os such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation.\n",
    "### 8.-10.\n",
    "See the Jupyter notebooks available at https://github.com/ageron/handson-ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
