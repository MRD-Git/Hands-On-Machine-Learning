{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and Running with TensorFlow\n",
    "page 229<br>\n",
    "See\n",
    "- https://github.com/ageron/handson-ml/blob/master/09_up_and_running_with_tensorflow.ipynb,\n",
    "- https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.allclose.html,\n",
    "- https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html,\n",
    "- https://www.tensorflow.org/api_docs/python/tf/matmul,\n",
    "- https://en.wikipedia.org/wiki/Automatic_differentiation,\n",
    "- https://www.tensorflow.org/guide/summaries_and_tensorboard, and\n",
    "- https://www.youtube.com/watch?v=eBbEDRsCmv4 for details.\n",
    "\n",
    "TensorFlow is a software by Google that allows the user to define a machine learning program structure and to run this structure efficiently in optimized C++ code. Parallelization to GPUs and CPUs is also possible. \n",
    "- It runs on Mac, Windows, and Linux.\n",
    "- It is compatible with Scikit-Learn.\n",
    "- It supports the use of many interesting APIs / packages (see e.g. page 231).\n",
    "- High flexibility: any neural network architecture can be constructed.\n",
    "- Highly efficient C++ implementation.\n",
    "- Automatically takes care of calculating gradients via \"automatic differentiation\" (or \"autodiff\").\n",
    "- The network structure can be visualized with its \"TensorBoard\".\n",
    "- Continuous development, cloud services, \"tensorflow.js\", etc. keep TensorFlow at the cutting edge.\n",
    "\n",
    "### Creating Your First Graph and Running It in a Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "# declare variables\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "# declare a function\n",
    "f = x*x*y + y + 2\n",
    "f\n",
    "# until now, no calculation has been executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code does not actually calculate anything! Although in particular the last line looks like it is performing a mathematical calculation, all the above code does is to create a compuational graph. Even the variables \"x\" and \"y\" are in fact not initialized, yet! In order to run the calculation that is represented by the graph, one has to establish a session and run it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# open a session\n",
    "sess = tf.Session()\n",
    "# initialize the variables (probably, this means that some memory is associated to them)\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "# run the function f in the session and store the outcome in result\n",
    "result = sess.run(f)\n",
    "# print the result\n",
    "print(result)\n",
    "# close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is equivalent and a bit quicker to type because inside the \"with\" block the session remains throughout and is closed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# shorten the code by using a \"with\" block so \"sess\" can be spared inside the block (less typing to do for the ...\n",
    "with tf.Session() as sess: # ... programmer\n",
    "    x.initializer.run()    # intialize x\n",
    "    y.initializer.run()    # intialize y\n",
    "    print(f.eval())        # evaluate f and print the result (still inside the session block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initializing every variable manually, initialization can be achieved globally with a node that performs the initialization. See the following code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# declare a global initializer\n",
    "init = tf.global_variables_initializer()\n",
    "# as above, the \"with\" environment eases the programmer's job (no need to type \"sess.\" almost everywhere) \n",
    "with tf.Session() as sess:\n",
    "    # initialize everything\n",
    "    init.run()\n",
    "    # perform the computation\n",
    "    result = f.eval()\n",
    "    # print the result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another possibility for simplifying the code is to use an \"InteractiveSession\". It automatically sets itself as the default session and thus requires no manual initialization of variables. However, manual closure at the end of the run is still required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# this is even shorter (but it might interfere with other sessions, so pay caution if other sessions are open)\n",
    "sess = tf.InteractiveSession()\n",
    "# as above, initialize and run everything, then print the result\n",
    "init.run()\n",
    "rsult = f.eval()\n",
    "print(result)\n",
    "# the downside of omitting the \"with\" environment is that such environment (or actually its end) cannot indicate ...\n",
    "# ... the end of the session; so it needs to be closed manually\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as no other session aside the default session from \"InteractiveSession\" is required, the last option above seems to be the most useful option: it requires no manual initialization of variables and avoids (possibly disturbing) indentation. Only manual closing is required.<br><br>\n",
    "Typically, a TensorFlow program consists of two parts: (i) first, it creates a graph that represents the model and its detailed computation steps and (ii) second, it runs a loop repeatedly for training, thus gradually improving its model parameters.\n",
    "## Managing Graphs\n",
    "page 234<br>\n",
    "Every node that is created will automatically be added to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)                # declare the variable x1\n",
    "x1.graph is tf.get_default_graph() # check if it is part of the default graph: yes, it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing wrong with the default graph. But if one wants to add a variable (now \"x2\") to another graph, this can be done by temporarily turning that other graph into the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()                        # new graph\n",
    "with graph.as_default():                  # the new graph is the default graph inside this \"with\" environment\n",
    "    x2 = tf.Variable(2)                   # add the variable \"x2\" and end the environment\n",
    "print(x2.graph is graph)                  # check if \"x2\" is part of \"graph\": yes, it is\n",
    "print(x2.graph is tf.get_default_graph()) # check if \"x2\" is part of the default graph: no, it is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion or Tip**<br>\n",
    "In Jupyter (or in a Python shell), it is common to run the same commands more than once while you are experimenting. As a result, you may end up with a default graph containing many duplicate nodes. One solution is to restart the Jupyter kernel (or the Python shell), but a more convenient solution is to just reset the default graph by running \"tf.reset_default_graph()\".\n",
    "## Lifecycle of a Node Value\n",
    "page 235<br>\n",
    "TensorFlow automatically checks on which nodes another node depends. Then, these nodes will be run before the dependent node. This is illustrated by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)         # node with a constant value\n",
    "x = w + 2                  # another node, dependent on \"w\"\n",
    "y = x + 5                  # another node, dependent on \"x\"\n",
    "z = x * 3                  # another node, also dependent on \"x\"\n",
    "with tf.Session() as sess: # run everything inside a \"with\" environment\n",
    "    print(y.eval())        # 10\n",
    "    print(z.eval())        # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above outputs are as expected: the dependent nodes have been run first. Otherwise, the results would be different. **Importantly**, TensorFlow does not reuse the \"old\" results for \"w\" and \"x\", which has already been calculated for \"y\". Instead, it evaluates \"w\" and \"x\" twice. All node values are dropped when the graph has been run. Only variables (and partially also queues and readers, see Chapter 12) are maintained beyond the end of the graph. A variable is \"born\" when it is initialized and \"dies\" when its session is closed.<br>\n",
    "Evaluating \"w\" and \"x\" twice is inefficient. To avoid such inefficiency, one can calculate \"y\" and \"z\" in one go, i.e., with one and the same graph as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val) # 10\n",
    "    print(z_val) # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning / caution**<br>\n",
    "In single-process TensorFlow, multiple sessions do not share any state, even if they reuse the same graph (each session would have its own copy of every variable). In distributed TensorFlow (see Chapter 12), variable state is sotred on the servers, not in the sessions, so multiple sessions can share the same variables.\n",
    "## Linear Regression with TensorFlow\n",
    "page 235<br>\n",
    "TensorFlow operations - or \"ops\" for short - can handle (almost) any number of inputs and outputs. It handles these inputs and outputs in the form of **tensors** (hence the name \"TensorFlow\"), i.e., multidimensional arrays. They are very similar to NumPy arrays or NumPy ndarrays, e.g., they also have a type and shape. They may carry floats or strings. Operations on the tensors like taking the transpose or multiplying them are rather intuitive. For a specific example, there is a link to TensorFlow's API documentation for \"tf.matmul\" at the top of this notebook. (There is also a link to the documentation on NumPy's \"np.c_\" operation.)<br><br>\n",
    "Here, we use all this to reiterate the optimization of the California Housing dataset via the Normal Equation in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.7465141e+01]\n",
      " [ 4.3573415e-01]\n",
      " [ 9.3382923e-03]\n",
      " [-1.0662201e-01]\n",
      " [ 6.4410698e-01]\n",
      " [-4.2513184e-06]\n",
      " [-3.7732250e-03]\n",
      " [-4.2664889e-01]\n",
      " [-4.4051403e-01]]\n"
     ]
    }
   ],
   "source": [
    "# relevant imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# get the data, its shape, and add a bias input feature x_0 = 1\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "# declare the features as variable \"x\" and the target as variable \"y\"\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "# build the transpose of the data and use it to deduce theta via the normal equation, see Equation 4-4 on page 108\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "# until now, we have just made declarations that are necessary for the tensorflow graph ...\n",
    "# ... now, we run the graph and print the resulting values for theta\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "page 237<br>\n",
    "Similar to our routine in Chapter 4, we now move beyond the Normal Equation to actual machine learning. First, we manually implement Batch Gradient Descent in TensorFlow. Then, we use TensorFlow's autodiff feature to compute the gradients automatically. And finally, we will use some of TensorFlow's out-of-the-box optimizers.<br><br>\n",
    "**Warning / caution**<br>\n",
    "When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may be much slower. You can do this using TensorFlow, NumPy, Scikit-Learn's StandardScaler, or any other solution you prefer. The following code assumes that this normalization has already been done.\n",
    "### Manually Computing the Gradients\n",
    "page 237<br>\n",
    "Taking into account the above warning, we first normalize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import StandardScaler from Scikit-Learn and establish an instance of it\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# normalize the data and then add a bias term x_0 = 1\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "scaled_housing_data_plus_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following procedure should seem familiar: we have already coded Batch Gradient Descent in Chapter 4. Now, we do it with TensorFlow. Where necessary, functions (e.g., \"tf.random_uniform()\" or \"tf.assign()\") can be looked up on the TensorFlow documentation linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 4.280337\n",
      "Epoch 100 MSE = 0.58247656\n",
      "Epoch 200 MSE = 0.53001195\n",
      "Epoch 300 MSE = 0.5278852\n",
      "Epoch 400 MSE = 0.52692956\n",
      "Epoch 500 MSE = 0.5262425\n",
      "Epoch 600 MSE = 0.52574044\n",
      "Epoch 700 MSE = 0.52537286\n",
      "Epoch 800 MSE = 0.52510303\n",
      "Epoch 900 MSE = 0.5249041\n",
      "[[ 2.0685523e+00]\n",
      " [ 8.3150929e-01]\n",
      " [ 1.2610334e-01]\n",
      " [-2.5601009e-01]\n",
      " [ 2.9207098e-01]\n",
      " [-1.8531521e-03]\n",
      " [-3.9988447e-02]\n",
      " [-8.4451884e-01]\n",
      " [-8.1476104e-01]]\n"
     ]
    }
   ],
   "source": [
    "# learning schedule\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "# declare variables and nodes\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")  # features\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")  # target values\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\") # initialize random bias and weights\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                            # make predictions\n",
    "error = y_pred - y                                                          # deviations from target values\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                          # cost function\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)                         # gradient on the cost function; see ...\n",
    "                                                                            # ... Equation 4-6 on page 115 and ...\n",
    "                                                                            # ... own .ipynb on Chapter 4\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)           # learning step: update theta\n",
    "# build global initializer (initializes all the variables)\n",
    "init = tf.global_variables_initializer()\n",
    "# run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)            # run the initializer\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:  # print the current epoch and mean squared error every 100 steps\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op) # run the learning step\n",
    "    best_theta = theta.eval() # assign the final theta to a variable outside tensorflow\n",
    "# print the result\n",
    "print(best_theta)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Simple as that.\n",
    "### Using autodiff\n",
    "page 238<br>\n",
    "Above, we have used the analytic formula given in Equation 4-6 of the book (see also code on Chapter 4) to express the gradient in a compact way. Usually, such a hands-on analytic approach should work, at least in principle. But in reality, it will not always be practical. For example the gradient (w.r.t. \"a\" and \"b\") of the following function ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "2.898088625641223\n",
      "-0.19869949020717656\n"
     ]
    }
   ],
   "source": [
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z\n",
    "print(my_func(0, 0))\n",
    "print(my_func(0, 1))\n",
    "print(my_func(1, 0))\n",
    "print(my_func(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... is very difficult to calculate analytically. (Hint given in the book: \"don't even try\".)<br>\n",
    "Fortunately, we can simply use TensorFlow's \"autodiff\" feature. Apart from \"reset_graph\" and \"gradients\", the following code is exactly the same as the one for Batch Gradient Descent above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.408029\n",
      "Epoch 100 MSE = 0.7551966\n",
      "Epoch 200 MSE = 0.5420869\n",
      "Epoch 300 MSE = 0.53316975\n",
      "Epoch 400 MSE = 0.53053766\n",
      "Epoch 500 MSE = 0.52879685\n",
      "Epoch 600 MSE = 0.52754843\n",
      "Epoch 700 MSE = 0.52664953\n",
      "Epoch 800 MSE = 0.52600086\n",
      "Epoch 900 MSE = 0.5255332\n",
      "[[ 2.0685525e+00]\n",
      " [ 8.1063598e-01]\n",
      " [ 1.2685777e-01]\n",
      " [-2.0784083e-01]\n",
      " [ 2.4839847e-01]\n",
      " [-1.3083883e-03]\n",
      " [-3.9607048e-02]\n",
      " [-8.5861266e-01]\n",
      " [-8.2600272e-01]]\n"
     ]
    }
   ],
   "source": [
    "# function taken from Github link above\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "# resetting the graph\n",
    "reset_graph()\n",
    "# code as for Batch Gradient Descent above\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# updated gradient node\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "# code as for Batch Gradient Descent above\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While not necessary for Batch Gradient Descent, the autodiff feature is going to be very useful so one more comment on it might be appropriate: \"tf.gradients()\" receives an operation (in this case \"mse\" as defined in the same block) as well as a list of variables (in this case just \"theta\") and then it calculated the gradient of that operation w.r.t. every variable.<br><br>\n",
    "With Table 9-2, the surrounding text, and Appendix D, the book provides some more details on \"main solutions to compute gradients automatically\". In case this should ever become interesting (e.g., for backpropagation), consulting the book and/or the wikipedia page on *automatic differentiation* (linked at the top) might be a good starting point for digging further into the topic. Here, it is considered sufficient to note that TensorFlow uses \"Reverse-mode autodiff\" which computes all gradients in just $n_{\\rm outputs}+1$ graph traversals. In our example, $n_{\\rm outputs}=1$, since \"mse\" is a scalar.\n",
    "### Using an Optimizer\n",
    "page 239<br>\n",
    "As has just been demonstrated, TensorFlow can compute the gradient efficiently. But there is even more in the box: instead of calculating the gradient, one may use a \"GradientDescentOptimizer\" for training. The below code is again the same as above but replaces \"gradients = ...\" and \"training_op = ...\" with other optimizing commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.408029\n",
      "Epoch 100 MSE = 0.7551966\n",
      "Epoch 200 MSE = 0.5420869\n",
      "Epoch 300 MSE = 0.53316975\n",
      "Epoch 400 MSE = 0.53053766\n",
      "Epoch 500 MSE = 0.52879685\n",
      "Epoch 600 MSE = 0.52754843\n",
      "Epoch 700 MSE = 0.52664953\n",
      "Epoch 800 MSE = 0.52600086\n",
      "Epoch 900 MSE = 0.5255332\n",
      "[[ 2.0685525e+00]\n",
      " [ 8.1063598e-01]\n",
      " [ 1.2685777e-01]\n",
      " [-2.0784083e-01]\n",
      " [ 2.4839847e-01]\n",
      " [-1.3083883e-03]\n",
      " [-3.9607048e-02]\n",
      " [-8.5861266e-01]\n",
      " [-8.2600272e-01]]\n"
     ]
    }
   ],
   "source": [
    "# code is the same as above\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# replace \"gradients\" and \"training_op\"\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "# code is the same as above\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works wonderfully. There are different optimizers available. Just to show off, the book also suggests the following routine, using a \"MomentumOptimizer\" instead of a \"GradientDescentOptimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.408029\n",
      "Epoch 100 MSE = 0.52520055\n",
      "Epoch 200 MSE = 0.52433306\n",
      "Epoch 300 MSE = 0.524321\n",
      "Epoch 400 MSE = 0.52432114\n",
      "Epoch 500 MSE = 0.5243208\n",
      "Epoch 600 MSE = 0.52432054\n",
      "Epoch 700 MSE = 0.52432054\n",
      "Epoch 800 MSE = 0.5243205\n",
      "Epoch 900 MSE = 0.5243205\n",
      "[[ 2.068558  ]\n",
      " [ 0.8296182 ]\n",
      " [ 0.11875144]\n",
      " [-0.26552498]\n",
      " [ 0.3056947 ]\n",
      " [-0.00450307]\n",
      " [-0.03932622]\n",
      " [-0.89988816]\n",
      " [-0.8705434 ]]\n"
     ]
    }
   ],
   "source": [
    "# code is the same as above\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# replace \"gradients\" and \"training_op\"\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "# code is the same as above\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shall be enough. Let's move forward to the next subject.\n",
    "## Feeding Data to the Training Algorithm\n",
    "page 239<br>\n",
    "When we want to use Mini-batch Gradient Descent instead of (full) Batch Gradient Descent, we need a way to feed the mini-batches to the algorithm. The following simple code illuminates how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "# reset the graph\n",
    "reset_graph()\n",
    "# define a placeholder node and define its output tensor's data type (necessary) and its shape (not necessary)\n",
    "# here, we use a matrix with any number of rows (specified as \"None\") and 3 columns\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "# open a session and run it\n",
    "with tf.Session() as sess:\n",
    "    # evaluate B by feeding the 1x3 matrix [[1, 2, 3]] to A\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    # evaluate B by feeding the 2x3 matrix [[4, 5, 6], [ 7, 8, 9]] to A\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [ 7, 8, 9]]})\n",
    "# print the results\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General note**<br>\n",
    "You can actually feed the output of *any* operations, not just placeholders. In this case TensorFlow does not try to evalutate these operations; is uses the values you feed it.<br><br>\n",
    "For **Mini-batch Gradient Descent**, we only need to tweak our above code for (full) Batch Gradient Descent slightly. Specifically, we take the code with a \"GradientDescentOptimizer\" and implement the following changes:\n",
    "- replace variables X and y by placeholderes X and y,\n",
    "- specify the batch size and infer the number of mini-batches,\n",
    "- add a function that returns mini-batches of features and the according labels,\n",
    "- loop through the number of mini-batches, and feed mini-batches to the placeholders while doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# code is the same as above\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "# replace previous definitions for X (features) and y (target values) by placeholders\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "# specify the batch size and infer the number of mini-batches\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "# code is the same as above\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "# add a function that returns mini-batches of features and the according labels (taken from the Github link above)\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  # random seed should be different for all mini-batches\n",
    "    indices = np.random.randint(m, size=batch_size)  # returns batch_size (100) random integers between 0 and m-1\n",
    "    X_batch = scaled_housing_data_plus_bias[indices] # 100 random features ...\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices] # ... and their corresponding target values\n",
    "    return X_batch, y_batch\n",
    "# code is the same as above apart from the for-loop (which now includes \"sess.run\") replacing the if-loop\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        # loop through the number of mini-batches\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size) # get a mini-batch, ...\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})      # ... feed it to the algorithm, and run it\n",
    "        # code is the same as above\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General note**<br>\n",
    "We don't need to pass the value of X and y when avalutating theta since it does not depend on either of them.\n",
    "## Saving and Restoring Models\n",
    "page 241<br>\n",
    "Once a model is trained, the parameters should be saved. But also during training, one should regularly save them so they can be restored in case the program crashes. TensorFlow makes it quite easy to save the model:\n",
    "- just create a \"tf.train.Saver()\" node at the end of construction (but before initialization) and\n",
    "- run this node during the execution phase whenever the model shall be saved, passing it the relevant session and a path.\n",
    "\n",
    "The below demonstration uses the code with TensorFlow's \"GradientDescentOptimizer\" from further above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 12.408029\n",
      "Epoch 100 MSE = 0.7551966\n",
      "Epoch 200 MSE = 0.5420869\n",
      "Epoch 300 MSE = 0.53316975\n",
      "Epoch 400 MSE = 0.53053766\n",
      "Epoch 500 MSE = 0.52879685\n",
      "Epoch 600 MSE = 0.52754843\n",
      "Epoch 700 MSE = 0.52664953\n",
      "Epoch 800 MSE = 0.52600086\n",
      "Epoch 900 MSE = 0.5255332\n",
      "[[ 2.0685525e+00]\n",
      " [ 8.1063598e-01]\n",
      " [ 1.2685777e-01]\n",
      " [-2.0784083e-01]\n",
      " [ 2.4839847e-01]\n",
      " [-1.3083883e-03]\n",
      " [-3.9607048e-02]\n",
      " [-8.5861266e-01]\n",
      " [-8.2600272e-01]]\n"
     ]
    }
   ],
   "source": [
    "# code is the same as above\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "# construction of the graph is almost complete: finally, create a \"saver\" node;   step 1/3\n",
    "saver = tf.train.Saver()\n",
    "# code is the same as above\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        # save the model every 100 steps (code adopted from further above);       step 2/3\n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\")\n",
    "        # code is the same as above\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    # save the final model still within the live session                          step 3/3\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    # code is the same as above\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy. And restoring a model is similarly easy:\n",
    "- build the model (not necessary here as we still have the graph defined above) and then\n",
    "- restore the variables **instead** of initializing them.\n",
    "\n",
    "In the below code, we attribute the name \"best_theta_restored\" to the restored variable \"theta\" so we can print it outside TensorFlow. NumPy's \"allclose\" function checks whether all entries of two variables are equal up to a small tolerance (\"all\" entries should be very \"close\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n",
      "[[ 2.0685525e+00]\n",
      " [ 8.1063598e-01]\n",
      " [ 1.2685777e-01]\n",
      " [-2.0784083e-01]\n",
      " [ 2.4839847e-01]\n",
      " [-1.3083883e-03]\n",
      " [-3.9607048e-02]\n",
      " [-8.5861266e-01]\n",
      " [-8.2600272e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()\n",
    "print(best_theta_restored)\n",
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow's \"tf.train.Saver()\" allows the user to specify which variables to save / restore and under what name. In the following example, only \"theta\" can be saved / restored, namely under the name \"weights\". In this context, it seems important to note that a variable's name specified within \"tf.train.Saver()\" is relevant for TensorFlow's namespace, but not necessarily for Python's namespace. See the discussion on https://stackoverflow.com/questions/33648167/why-do-we-name-variables-in-tensorflow for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_weights.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code is the same as above\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "init = tf.global_variables_initializer()\n",
    "# save only theta and save it under the name \"weights\"\n",
    "saver = tf.train.Saver({\"weights\": theta})\n",
    "# code is the same as above\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        # skip saving during training (otherwise, the code continues as above)\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_weights.ckpt\")\n",
    "# skip printing \"best_theta\", restore it from the checkpoint, and check if restoration was successful\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_weights.ckpt\")\n",
    "    best_theta_restored = theta.eval()\n",
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, TensorFlow's \"tf.train.Saver()\" also saves the graph's structure under a similarly named file with the extension \".meta\" at the end. So we can reset the graph completely and then reload it instead of building it once more from scratch. That's very handy! **Once the graph has been rebuilt, one can also restore its state.** This is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entire code taken from Github link above\n",
    "reset_graph()                                                       # notice that this resets the graph\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\") # load the graph structure from checkpoint\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")        # get the graph and infer \"theta\" from it\n",
    "with tf.Session() as sess:                                          # run a session\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")                 # restore the graph's state\n",
    "    best_theta_restored = theta.eval()                              # evaluate theta\n",
    "np.allclose(best_theta, best_theta_restored)                        # check if it (almost) equals the original value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! For the moment, this shall be enough on \"saving and restoring models\".\n",
    "## Visualizing the Graph and Training Curves Using TensorBoard\n",
    "page 242<br>\n",
    "Until now, we have been relying on the \"print()\" function to get an idea of the training progress. There is a better way: using *TensorBoard*! Feeding it some training statistics, TensorBoard will produce good-looking, interactive visualizations of training metrics (e.g., learning curves) in the web browser. If it has access to the graph used by TensorFlow, TensorBoard will also provide an interactive visualization of that. This last feature can be particularly useful to identify misconceptions in the model.<br><br>\n",
    "An important step is to write the graph's definition and some training statistics into a log directory that TensorBoard can read from. A different log directory should be used for different runs because otherwise, TensorBoard will read and display all stats and graphs at once. Such visualizations might be very cluttered. A simple solution for distinguishing different runs is to include a time stamp in the log directory.<br><br>\n",
    "In the code below, we implement all the necessary steps to visualize a learning metric (here: the mean squared error) and the model's graph within TensorBoard. Specifically, we\n",
    "1. create a directory with a time stamp,\n",
    "2. build nodes for (i) the summary and for (ii) saving that summary and the graph at the end of the construction phase,\n",
    "3. save the summary and the graph every few epochs, and\n",
    "4. close the \"file_writer\" at the end of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/9_TF/run-20190919132004/\n",
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "# create a directory with a time stamp\n",
    "from datetime import datetime                    # import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") # get current UTC time as a string with specified format\n",
    "root_logdir = \"tf_logs/9_TF\"                     # this directory will in the same folder as this .ipynb notebook\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)   # folder with timestamp (inside the folder \"tf_logs/9_TF\")\n",
    "print(logdir)                                    # print the total path w.r.t. the path of this .ipynb notebook\n",
    "# code is the same as for Mini-batch Gradient Descent above\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "# step 2\n",
    "# before initialization, build a node for the summary and a node for saving that summary and the graph\n",
    "mse_summary = tf.summary.scalar('MSE', mse)                         # \"mse\" will be saved via \"mse_summary\"\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # saving of the graph under \"logdir\"\n",
    "# code is the same as for Mini-batch Gradient Descent above\n",
    "init = tf.global_variables_initializer()\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "# step 3\n",
    "            # every 10 epochs, save the summary and the graph (for all batches) \n",
    "            if epoch % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch}) # summary of current batch\n",
    "                step = epoch * n_batches + batch_index                             # step associated to current batch\n",
    "                file_writer.add_summary(summary_str, step)                         # add step and summary to file\n",
    "            # code is the same as for Mini-batch Gradient Descent above\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "# step 4\n",
    "# close the file writer\n",
    "file_writer.close()\n",
    "# code is the same as for Mini-batch Gradient Descent above\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning / caution**<br>\n",
    "Avoid logging training stats at every single training step, as this would significantly slow down training.<br><br>\n",
    "To **visualize the graph and statistics in TensorBoard**, one should do the following.\n",
    "1. Open a new shell and navigate to the folder containing the folder in which the time-stamped logging directories are saved. Here, that folder would be \"Hands-On-ML\" (in which this notebook is also located), as it contains the folder \"tf_logs/9_TF\", which in turn contains all the folders with different time stamps (as specified by the above string \"logdir\").\n",
    "2. After navigating to that folder, run \"tensorboard --logdir tf_logs/9_TF/\" in the shell. This is assuming that the folder containing the time-stamped logging directories is indeed \"tf_logs/9_TF\" (as is the case, here). Otherwise, that part of the shell command must be adapted to the actual name of the folder containing the logs.\n",
    "3. Open a new browser window and navigate to \"http://localhost:6006/\".\n",
    "\n",
    "Good introductions to the functionality of TensorBoard can be found in the TensorBoard link and in the YouTube link above. The full plot range of scalar quantities will be displayed if the box \"Ignore outliers in chart scaling\" is unchecked.<br>\n",
    "The graph becomes accessible by clicking on the \"GRAPHS\" tab inside TensorBoard. After removing the nodes \"theta\", \"gradients\", \"GradientDescent\", and \"random_uniform\" from the main graph by right-clicking on it, the graph looks very much like the one in Figure 9-4 of the book (but still a bit different, possibly because of another TensorFlow version).\n",
    "<br><br>\n",
    "**Suggestion or Tip**<br>\n",
    "If you want to take a peek at the graph directly within Jupyter, you can use the \"show_graph()\" function available in the notebook for this chapter. It was originally written by A. Mordvintsev in his great deepdream tutorial notebook (http://goo.gl/EtCWUc). Another option is to install E. Jang's TensorFlow debugger tool (https://github.com/ericjang/tdb) which includes a Jupyter extension for graph visualization (and more).\n",
    "## Name Scopes\n",
    "page 245<br>\n",
    "When dealing with more complex models (e.g., with neural nets), the graph may contain many, many more nodes, thus producing a lot of clutter in TensorBoard's visual graph representation. To avoid such clutter, multiple nodes can be summarized under a common name via *name scopes*. These name scopes can be expanded and collapsed in TensorBoard so that the graph becomes more structured. Below, we use the above code to demonstrate the use of name scopes by summarizing \"error\" and \"mse\" in a name scope called \"loss\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/9_TF/run-20190919132655/\n",
      "[[ 2.0714476 ]\n",
      " [ 0.8462012 ]\n",
      " [ 0.11558535]\n",
      " [-0.26835832]\n",
      " [ 0.32982782]\n",
      " [ 0.00608358]\n",
      " [ 0.07052915]\n",
      " [-0.87988573]\n",
      " [-0.8634251 ]]\n",
      "loss/sub\n",
      "loss/mse\n"
     ]
    }
   ],
   "source": [
    "# the following code is the same as above (but skipping the import of datetime, which has been imported already)\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs/9_TF\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "print(logdir)\n",
    "reset_graph()\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "# define \"error\" and \"mse\" inside a name scope called \"loss\"\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "# the following code is the same as above\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if epoch % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()\n",
    "file_writer.close()\n",
    "print(best_theta)\n",
    "# print the name of the ops \"error\" and \"mse\" (they should now be prefixed by \"loss/\")\n",
    "print(error.op.name)\n",
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above output shows, the operations \"error\" and \"mse\" are referred to as \"loss/sub\" (for \"substitution\", as \"error\" is not an explicitly defined TensorFlow variable or the like) and \"loss/mse\". And in TensorBoard's representation of the graph, the node \"loss\" appears and can be expanded to reveal \"error\" and \"mse\". The graph looks very similar to Figure 9-5 in the book (but not exactly the same, possible because of a different TensorFlow version).\n",
    "## Modularity\n",
    "page 246<br>\n",
    "We will demonstrate TensorFlow's modularity with an example of *ReLU*s. A *Rectified Lineare Unit*, or *ReLU* for short, is a function\n",
    "$$h_{\\vec{W},b}={\\rm max}(\\vec{X}\\cdot\\vec{w}+b,0)\\,.$$\n",
    "ReLUs are not necessary to demonstrate modularity but it is good to introduce them at last, as they play an important role in machine learning. The following block is not intended to produce something useful but rather to show that copy-pasting code will easily lead to failures in the code. Also, such code is hard to maintain. A common bon mot among coders is \"Two or more: use a for.\", where \"for\" means a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\") # building two (or more) ReLUs by copy-pasting code can easily lead to ...\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\") # ... failures like the faulty z1 (instead of z2) in this line\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of using the repetitive code above, we now define a function that builds ReLUs and then run this function as often as we want. In order to display the graph in TensorBoard, we save its structure via \"tf.summary.FileWriter()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "tf_logs/9_TF/relu1/\n"
     ]
    }
   ],
   "source": [
    "# very important: reset the graph\n",
    "reset_graph()\n",
    "# define a function that builds a ReLU\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                      # get the shape of X\n",
    "    w =tf.Variable(tf.random_normal(w_shape), name=\"weights\") # weight matrix with the same shape as X and random ...\n",
    "                                                              # components (normally distributed)\n",
    "    b = tf.Variable(0.0, name=\"bias\")                         # scalar bias term\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"linear\")             # prediction (use \"linear\" instead of \"z\" as name)\n",
    "    return tf.maximum(z, 0., name=\"relu\")                     # return the prediction if positive and 0 otherwise\n",
    "# specify the number of features (n) and the feature matrix X\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "# build 5 relus and output their sum via the \"print()\" function\n",
    "relus =[relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "print(output)\n",
    "# we use \"tf_logs/9_TF/relu1\" instead of \"logs/relu1\" as the log directory (see Github code linked above)\n",
    "logdir = \"tf_logs/9_TF/relu1/\"\n",
    "print(logdir)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code does not save any scalars etc. that TensorBoard could display but only the graph structure. Apparently, the browser tab in which TensorBoard is running needs to be closed and reopened to \"update\" the available graphs. But it is not necessary to shut down and restart TensorBoard via the command line.<br><br>\n",
    "Once the graph is displayed (at best with all nodes inside the main graph), the structure becomes apparent. But it is a little hard to read because all nodes required for a ReLU appear 5 times. Only the first instance of a named node (e.g. \"bias\") is displayed under its default name while all further instances bear an index \"\\_i\" denoting the i-th repetition (e.g. \"bias_1\"), see also Figure 9-6 in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "tf_logs/9_TF/relu2/\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):                     # define the \"relu\" name scope and indent the according lines\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w =tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n",
    "        return tf.maximum(z, 0., name=\"relu\")\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus =[relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "print(output)\n",
    "logdir = \"tf_logs/9_TF/relu2/\"                      # save the graph in the folder \"tf_logs/9_TF/relu2/\"\n",
    "print(logdir)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above code demonstrates, we can group all nodes of a ReLU together by using name scopes. This leads to a much simpler graph structure, see also Figure 9-7 in the book.\n",
    "## Sharing Variables\n",
    "page 248<br>\n",
    "Iteratively adjusting many parameters is key to machine learning. Therefore, it is important to pass updated parameters (possibly many of them) around to functions (possibly also many of them) that depend on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'relu/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_1/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_2/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_3/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_4/max:0' shape=(?, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# reset the graph\n",
    "reset_graph()\n",
    "# define a function for ReLUs that receive \"threshold\" as a parameter\n",
    "def relu(X, threshold):\n",
    "    # put all the following in the name scope \"relu\"\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        # the following is the same as above\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        # now use the \"threshold\" that was passed as a parameter\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "# define the threshold, X, build ReLUs, and add up their results\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "print(relus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses five ReLUs, all of which receive the same parameter \"threshold\". One could update \"threshold\" and the code will perfectly fine because \"threshold\" is passed explicitly as an argument to the \"relu\" function. However, this approach reaches its limits for functions that depend on many, many parameters. There are several alternatives:\n",
    "- Create and maintain (update) a complete dictionary of all parameters and pass that dictionary to all the functions that depend on a parameter.\n",
    "- Create a new Python class for each module. (I do not really understand this but according to the footnote on page 249, this should be the cleanest solution.)\n",
    "- Use a solution (explained further below) that is apparently very popular with TensorFlow.\n",
    "- Set the shared variable as an attribute of the ReLU upon its first call.\n",
    "\n",
    "Let's try the last option first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'relu/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_1/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_2/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_3/max:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'relu_4/max:0' shape=(?, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# everything is the same as above apart from those parts with extra comments\n",
    "reset_graph()\n",
    "def relu(X):                                                    # \"threshold\" is not passed as a parameter\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):                      # returns True or False\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\") # initialize the attribute \"threshold\" if necessary \n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]                             # \"threshold\" is not passed as a parameter\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "print(relus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated in the above list, there is a popular solution for TensorFlow. It uses the \"get_variable()\" function to create the variable if it does not exist yet and to reuse it if it does exist already. This is similar to the above code but now, the desired behaviour (creating or reusing the variable) is controlled by an attribute of \"variable_scope()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "tf_logs/9_TF/relu3/\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):     # use tf.variable_scope() instead of tf.name_scope() and note ...\n",
    "                                                    # ... that existing variables will be reused in following runs\n",
    "        threshold = tf.get_variable(\"threshold\")    # use \"tf.get_variable()\" to get \"threshold\"\n",
    "        w_shape = (int(X.get_shape()[1]), 1)        # shape of weight matrix \"w\"\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\") # random initialization of \"w\"\n",
    "        b = tf.Variable(0.0, name=\"bias\")                          # bias term \"b\"\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                   # prediction \"z\"\n",
    "        return tf.maximum(z, threshold, name=\"max\") # ReLU output\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\") # declare a placeholder node \"X\"\n",
    "with tf.variable_scope(\"relu\"):                                    # establish the variable scope \"relu\" which is ...\n",
    "                                                                   # ... also used by \"relu(X)\"\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),             # in this variable scope, declare \"threshold\" ...\n",
    "                                initializer=tf.constant_initializer(0.0)) # ... initialize it to 0 on the first ...\n",
    "                                                                   # ... run and reuse it in following runs (for ...\n",
    "                                                                   # ... relu(X), reuse=True in this variable scope)\n",
    "relus = [relu(X) for relu_index in range(5)]        # list of 5 ReLUs\n",
    "output = tf.add_n(relus, name=\"output\")             # sum of 5 ReLUs\n",
    "print(output)\n",
    "logdir = \"tf_logs/9_TF/relu3/\"                      # save the graph in the folder \"tf_logs/9_TF/relu3/\"\n",
    "print(logdir)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the \"reuse\" attribute of \"tf.variable_scope()\" had not been set to \"True\" in the third line above, the code would trigger an exception upon repeated call of the \"relu(X)\" function since \"threshold\" has been initialized already in the first run. Try it out using \"reuse=False\" in the third line!<br><br>\n",
    "** Warning / caution**<br>\n",
    "Once \"reuse\" is set to \"True\", it cannot be set back to \"False\" within the block. Moreover, if you define other variable scopes inside this one, they will automatically inherit \"reuse=True\". Lastly, only variables created by \"get_variable()\" can be reused this way.<br><br>\n",
    "**General note**<br>\n",
    "Variables created using \"get_variable()\" are always named using the name of their \"variable_scope\" as a prefix (e.g., \"relu/threshold\"), but for all other nodes (including variables created with \"tf.Variable()\") the variable scope acts like a new name scope. In particular, if a name scope with an identical name was already created, then a suffix is added to make the name unique. Fore example, all nodes created in the preceding code (except the \"threshold\" variable) have a name prefixed with \"relus_1/\" to \"relu_5/\", as shown in Figure 9-8.<br><br>\n",
    "Indeed, the above code produces a graph very similiar to the one shown in Figure 9-8. However, it is a bit unfortunate that the \"threshold\" variable is defined outside the function \"relu(X)\". This can be fixed by telling the relevant variable_scope to reuse \"threshold\" after its first use, as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "tf_logs/9_TF/relu4/\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):                        # set the variable scope\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), # initialize \"threshold\"\n",
    "                                    initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    first_relu = relu(X)                                   # use \"relu(X)\" once, thus initializing \"threshold\" ...\n",
    "    scope.reuse_variables()                                # ... and then reuse it ...\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]     # ... in following calls of \"relu(X)\"\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "print(output)\n",
    "logdir = \"tf_logs/9_TF/relu4/\"                             # save the graph in the folder \"tf_logs/9_TF/relu4/\"\n",
    "print(logdir)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the above code (which comes from the Github link above) and the following code (which corresponds to the last code block displayed in Chapter 9 of the book) lead to graphs very similar to Figure 9-9 in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output:0\", shape=(?, 1), dtype=float32)\n",
      "tf_logs/9_TF/relu5/\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):                               # set the variable scope\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(),        # initialize \"threshold\"\n",
    "                                    initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [] # this list of ReLUs will be filled during the excution of the following loop\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\",reuse=(relu_index>0)) as scope: # reuse=False for the first ReLU but ...\n",
    "        relus.append(relu(X))                                     # reuse=True for following ReLUs\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "print(output)\n",
    "logdir = \"tf_logs/9_TF/relu5/\"                                    # save the graph in the folder \"tf_logs/9_TF/relu5/\"\n",
    "print(logdir)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "page 251\n",
    "### 1.-11.\n",
    "Solutions are shown in Appendix A of the book and in the separate notebook *ExercisesWithoutCode*.\n",
    "### 12.\n",
    "Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. Train it and evaluate it on the moons dataset (introduced in Chapter 5). Try adding all the bells and whistles:\n",
    "- Define the graph within a \"logistic_regression()\" function that can be reused easily.\n",
    "- Save checkpoints using a \"Saver\" at regular intervals during training, and save the final model at the end of training.\n",
    "- Restore the last checkpoint upon startup if training was interrupted.\n",
    "- Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "- Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve.\n",
    "\n",
    "[The following code is heavily based on the solution shown in the Github link above.]\n",
    "First, let's make moons and display the data. The classes are 0 and 1 (or -1 and +1, or \"negative\" and \"positive\") and the features are simply x- and y-coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make moons\n",
    "from sklearn.datasets import make_moons                      # import moons\n",
    "m = 1000                                                     # number of data points\n",
    "X_moons, y_moons = make_moons(m, noise=0.1, random_state=42) # make noisy dataset\n",
    "# make nice pictures with matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# display the generated moons dataset (two classes)\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.plot(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1], 'go', label=\"Positive\") # positive class (green)\n",
    "plt.plot(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1], 'r^', label=\"Negative\") # negative class (red)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in Chapter 4, Logistic Regression first employs a linear model, $\\theta^{T}\\cdot x$, where $x$ is a feature vector including a bias term. Then, the result of this linear model, $t$, is fed to the logistic function\n",
    "$$\\sigma(t)=\\frac{1}{1+e^{-t}}\\,.$$\n",
    "So in particular, the feature vector first needs to have a bias term. Let's add it!<br>\n",
    "Additionally, we shall reshape the target data into a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.05146968  0.44419863]\n",
      " [ 1.          1.03201691 -0.41974116]\n",
      " [ 1.          0.86789186 -0.25482711]\n",
      " [ 1.          0.288851   -0.44866862]\n",
      " [ 1.         -0.83343911  0.53505665]]\n",
      "(1000, 3)\n",
      "(1000,)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons] # add a column with 1s to the feature matrix\n",
    "print(X_moons_with_bias[:5])                        # show the first 5 rows of the feature matrix\n",
    "print(X_moons_with_bias.shape)                      # shape of feature data\n",
    "print(y_moons.shape)                                # shape of target data\n",
    "y_moons_column_vector = y_moons.reshape(-1, 1)      # reshaping it\n",
    "print(y_moons_column_vector.shape)                  # reshaped target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the data into training and test sets, and also build a function that returns mini-batches of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.93189866  0.13158788]\n",
      " [ 1.          1.07172763  0.13482039]\n",
      " [ 1.         -1.01148674 -0.04686381]\n",
      " [ 1.          0.02201868  0.19079139]\n",
      " [ 1.         -0.98941204  0.02473116]]\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# set size of test set\n",
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "# training set\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "# test set\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]\n",
    "# build a function that returns a mini-batch (size: \"batch_size) of training data\n",
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train),batch_size) # random indices between 0 and \"test_size\" (see above)\n",
    "    X_batch = X_train[rnd_indices]                              # features\n",
    "    y_batch = y_train[rnd_indices]                              # classes\n",
    "    return X_batch, y_batch                                     # return features and classes\n",
    "# try the function out for a mini-batch of size 5\n",
    "X_batch, y_batch = random_batch(X_train, y_train, 5)\n",
    "print(X_batch)\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to build a graph for Mini-batch Gradient Descent on Logistic Regression. Here, TensorFlow has its own implementation of the sigmoid (Equation 4-14 in the book). We shall use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"truediv:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "reset_graph()  # resetting the graph\n",
    "n_inputs = 2   # number of inputs (x- and y-cooridnates, the bias term will be added \"by hand\" via \"+1\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1),name=\"X\") # placeholder for data (note \"n_input + 1\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")           # placeholder for classes (target values)\n",
    "theta = tf.Variable(tf.random_uniform([n_inputs + 1, 1], -1.0, 1.0, seed=42), name=\"theta\") # weight vector\n",
    "logits = tf.matmul(X, theta, name=\"logits\")                         # result of the linear model\n",
    "y_proba = 1 / (1 + tf.exp(-logits))                                 # sigmoid prediction of the Logistic Regression\n",
    "print(y_proba) # print this\n",
    "y_proba = tf.sigmoid(logits)                                        # TensorFlow's custom sigmoid (overwrites y_proba)\n",
    "print(y_proba) # print this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a cost function to be minimized. As discussed in Chapter 4 (pages 135/136 of the book), the \"log loss\" function,\n",
    "$$-log(p)\\quad\\text{if $y=1$},\\quad-log(1-p)\\quad\\text{if $y=0$}\\,,$$\n",
    "is a good cost function for Logistic Regression. If we implement it manually, it makes sense to add a tiny term to $p$ so $log(0)$ will be avoided for $p=0$. Yet, TensorFlow has a custom implmentation for the log loss, too. So we will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Neg_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"log_loss/value:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-7                                                                    # to avoid log(0) below\n",
    "loss = -tf.reduce_mean(y*tf.log(y_proba+epsilon)+(1-y)*tf.log(1-y_proba+epsilon)) # manual implementation of log loss\n",
    "print(loss)                                                                       # print this\n",
    "loss = tf.losses.log_loss(y, y_proba)                                             # tf also uses +1e-7 by default\n",
    "print(loss)                                                                       # print this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish the TensorFlow model! It is in large parts the same as the main text above (see \"Feeding Data to the Training Algorithm\" / page 239 in the book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.79260236\n",
      "Epoch: 100 \tLoss: 0.34346345\n",
      "Epoch: 200 \tLoss: 0.30754045\n",
      "Epoch: 300 \tLoss: 0.29288897\n",
      "Epoch: 400 \tLoss: 0.28533578\n",
      "Epoch: 500 \tLoss: 0.2804781\n",
      "Epoch: 600 \tLoss: 0.2780829\n",
      "Epoch: 700 \tLoss: 0.2761544\n",
      "Epoch: 800 \tLoss: 0.27551997\n",
      "Epoch: 900 \tLoss: 0.27491233\n",
      "[[0.54895616]\n",
      " [0.70724374]\n",
      " [0.51900256]\n",
      " [0.9911136 ]\n",
      " [0.5085905 ]]\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01                                                       # specify the learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) # use tf's gradient descent optimizer\n",
    "training_op = optimizer.minimize(loss)                                     # training operation\n",
    "init = tf.global_variables_initializer()                                   # initialize the variables\n",
    "# plan how data will be fed to the algorithm\n",
    "n_epochs = 1000                                                            # number of epochs\n",
    "batch_size = 50                                                            # batch size\n",
    "n_batches = int(np.ceil(m / batch_size))                                   # number of batches\n",
    "# let's run the thing in tensorflow\n",
    "with tf.Session() as sess:                                                 # run a tensorflow session\n",
    "    sess.run(init)                                                         # run initializer\n",
    "    for epoch in range(n_epochs):                                          # epochs\n",
    "        for batch_index in range(n_batches): # batches\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)  # build batch\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})      # run training step with current batch\n",
    "        loss_val = loss.eval({X: X_test, y: y_test})                       # evaluate loss on the test set\n",
    "        if epoch % 100 == 0:                                               # every 100 epochs ...\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)                    # ... print the loss on the test set \n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})           # prediction of the final model\n",
    "# print final results\n",
    "print(y_proba_val[:5])                                                     # model output\n",
    "y_pred = (y_proba_val >= 0.5)                                              # class predictions\n",
    "print(y_pred[:5])                                                          # print the first 5 class predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, check\n",
    "$${\\rm precision}=\\frac{\\rm TP}{{\\rm TP}+{\\rm FP}}$$\n",
    "i.e., the fraction of positively classified instances that do actually belong to the positive class and\n",
    "$${\\rm recall}=\\frac{\\rm TP}{{\\rm TP}+{\\rm FN}}$$\n",
    "i.e., the fraction of actual positive instances that are classified positively.<br><br>\n",
    "While increasing the threshold, *precision is expected to go up* as false positives should decrease. But occasionally, it can also decrease as true positives get (wrongfully) filtered out. In contrast, *recall will decrease monotonically* while increasing the threshold since its nominator (the \"positive class\") is fixed and the numerator can only decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627450980392157\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score # import precisison and recall scores from scikit-learn\n",
    "print(precision_score(y_test, y_pred))                    # print precision score on test set\n",
    "print(recall_score(y_test, y_pred))                       # print recall score on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the predictions on the test set in a plot. The code makes use of the tilde \"~\" opearator. Apparently, this operator simly inverts the bits. In particular, it turns \"True\" into \"False\" and vice versa. See the contribution of user \"wberry\" on https://stackoverflow.com/questions/8305199/the-tilde-operator-in-python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n",
      "(200,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_pred.shape)                                                              # shape of a matrix\n",
    "y_pred_idx = y_pred.reshape(-1)                                                  # make the matrix a vector\n",
    "print(y_pred_idx.shape)                                                          # shape of a vector\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")   # positives\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\") # negatives\n",
    "plt.legend()                                                                     # add a legend\n",
    "plt.show()                                                                       # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks pretty bad! However, we are using a linear model, i.e., a linear decision boundary. Taking this into account, the result is actually quite good. We continue with a linear model but from now on involve higher-order polynomials in the hope that this will lead to better results. We also add all the fancy stuff mentioned in the original exercise.<br><br>\n",
    "We start by adding higher polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -5.14696757e-02  4.44198631e-01  2.64912752e-03\n",
      "   1.97312424e-01 -1.36349734e-04  8.76459084e-02]\n",
      " [ 1.00000000e+00  1.03201691e+00 -4.19741157e-01  1.06505890e+00\n",
      "   1.76182639e-01  1.09915879e+00 -7.39511049e-02]\n",
      " [ 1.00000000e+00  8.67891864e-01 -2.54827114e-01  7.53236288e-01\n",
      "   6.49368582e-02  6.53727646e-01 -1.65476722e-02]\n",
      " [ 1.00000000e+00  2.88850997e-01 -4.48668621e-01  8.34348982e-02\n",
      "   2.01303531e-01  2.41002535e-02 -9.03185778e-02]\n",
      " [ 1.00000000e+00 -8.33439108e-01  5.35056649e-01  6.94620746e-01\n",
      "   2.86285618e-01 -5.78924095e-01  1.53179024e-01]]\n"
     ]
    }
   ],
   "source": [
    "X_train_enhanced = np.c_[X_train,                  # original data (including the bias term)\n",
    "                         np.square(X_train[:, 1]), # squares ...\n",
    "                         np.square(X_train[:, 2]), # ... without mixing\n",
    "                         X_train[:, 1] ** 3,       # cubics ...\n",
    "                         X_train[:, 2] ** 3]       # ... without mixing\n",
    "X_test_enhanced = np.c_[X_test,                    # and the same for the features of the test set\n",
    "                        np.square(X_test[:, 1]),\n",
    "                        np.square(X_test[:, 2]),\n",
    "                        X_test[:, 1] ** 3,\n",
    "                        X_test[:, 2] ** 3]\n",
    "print(X_train_enhanced[:5])                        # print the first 5 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get serious! After resetting the graph, we define a function that receives a mini-batch and possibly more arguments (e.g., an initializer), performs a training step, and returns relevant data. We also build a function that returns a time-stamped log-directory. Then we consider 6 (+1) features, get a directory for saving our model, define placeholders for feeding the data to the algorithm, implement training steps, and finally save the model.<br>Now, the graph exists. Yet until now, it has not been run, yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/9_TF/logreg-exercise-20190919133634/\n"
     ]
    }
   ],
   "source": [
    "# reset the graph\n",
    "reset_graph()\n",
    "# define a function that performs a training step, makes a checkpoint, and returns relevant output.\n",
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):                                         # name scope\n",
    "        with tf.name_scope(\"model\"):                                                   # name scope\n",
    "            if initializer is None:                                                    # choose an initializer\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")                             # weights\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")                                # prediction\n",
    "            y_proba = tf.sigmoid(logits)                                               # mapping to interval [0,1]\n",
    "        with tf.name_scope(\"train\"):                                                   # name scope\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")                        # loss function\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) # gradient descent optimizer\n",
    "            training_op = optimizer.minimize(loss)                                     # training step\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)                         # summary for records\n",
    "        with tf.name_scope(\"init\"):                                                    # name scope\n",
    "            init = tf.global_variables_initializer()                                   # initialize variables\n",
    "        with tf.name_scope(\"save\"):                                                    # name scope\n",
    "            saver = tf.train.Saver()                                                   # saving\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver                       # returned data\n",
    "# define a function that returns a path for checkpoint files\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs/9_TF\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"exercise-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "# with quadratic and cubic (no interference terms) features, we have 4+2=6 inputs (+1 for the bias term, see X below)\n",
    "n_inputs = 2 + 4\n",
    "# directory for saving the model\n",
    "logdir = log_dir(\"logreg\")\n",
    "print(logdir)\n",
    "# define placeholders for feeding data to the algorithm\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "# perform a training step\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "# build a file_writer via tf.summary.FileWriter\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.629985\n",
      "Epoch: 500 \tLoss: 0.16122364\n",
      "Epoch: 1000 \tLoss: 0.1190321\n",
      "Epoch: 1500 \tLoss: 0.097329214\n",
      "Epoch: 2000 \tLoss: 0.08369793\n",
      "Epoch: 2500 \tLoss: 0.074375816\n",
      "Epoch: 3000 \tLoss: 0.06750215\n",
      "Epoch: 3500 \tLoss: 0.062206898\n",
      "Epoch: 4000 \tLoss: 0.058026794\n",
      "Epoch: 4500 \tLoss: 0.054562975\n",
      "Epoch: 5000 \tLoss: 0.051708292\n",
      "Epoch: 5500 \tLoss: 0.049237743\n",
      "Epoch: 6000 \tLoss: 0.047167286\n",
      "Epoch: 6500 \tLoss: 0.04537664\n",
      "Epoch: 7000 \tLoss: 0.043818746\n",
      "Epoch: 7500 \tLoss: 0.04237422\n",
      "Epoch: 8000 \tLoss: 0.041089162\n",
      "Epoch: 8500 \tLoss: 0.039970912\n",
      "Epoch: 9000 \tLoss: 0.038920246\n",
      "Epoch: 9500 \tLoss: 0.038010743\n",
      "Epoch: 10000 \tLoss: 0.037155695\n"
     ]
    }
   ],
   "source": [
    "# remaining import\n",
    "import os\n",
    "# planning the mini-batches\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "# paths for saving stuff\n",
    "checkpoint_path = \"./tf_logs/9_TF/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./tf_logs/9_TF/my_logreg_model\"\n",
    "# run the graph\n",
    "with tf.Session() as sess:\n",
    "    # resume training where it stopped previously ...\n",
    "    if os.path.isfile(checkpoint_epoch_path): # if a checkpoint exists, restore that model and load its epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    # ... or start over if no chekpoint is available\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "    # loop through expchs\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        # loop through batches and save current model\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        # every 500 epochs, print the current epoch and update the check point\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "    # save final model and delete checkpoints            \n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the higher order features, the predictions are much, much better than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9797979797979798\n",
      "0.9797979797979798\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = (y_proba_val >= 0.5)          # binary classification\n",
    "print(precision_score(y_test, y_pred)) # precision on test set\n",
    "print(recall_score(y_test, y_pred))    # recall on test set\n",
    "y_pred_idx = y_pred.reshape(-1)        # bring predictions in the right shape\n",
    "# plot the predictions\n",
    "plt.plot(X_test[y_pred_idx, 1], X_test[y_pred_idx, 2], 'go', label=\"Positive\")\n",
    "plt.plot(X_test[~y_pred_idx, 1], X_test[~y_pred_idx, 2], 'r^', label=\"Negative\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also play around with some hyperparameters, which is the last item on our to-do list. The \"reciprocal\" function from \"scipy.stats\" is usefule when exploring an unknown parameter range. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919134554/\n",
      "  batch size: 54\n",
      "  learning_rate: 0.004430375245218265\n",
      "  training: .....................\n",
      "  precision: 0.9797979797979798\n",
      "  recall: 0.9797979797979798\n",
      "Iteration 1\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919135503/\n",
      "  batch size: 22\n",
      "  learning_rate: 0.0017826497151386947\n",
      "  training: .....................\n",
      "  precision: 0.9797979797979798\n",
      "  recall: 0.9797979797979798\n",
      "Iteration 2\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919141355/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.00203228544324115\n",
      "  training: .....................\n",
      "  precision: 0.9696969696969697\n",
      "  recall: 0.9696969696969697\n",
      "Iteration 3\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919141925/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.004491523825137997\n",
      "  training: .....................\n",
      "  precision: 0.9797979797979798\n",
      "  recall: 0.9797979797979798\n",
      "Iteration 4\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919142629/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.07963234721775589\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 5\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919143317/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.0004634250583294876\n",
      "  training: .....................\n",
      "  precision: 0.912621359223301\n",
      "  recall: 0.9494949494949495\n",
      "Iteration 6\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919143905/\n",
      "  batch size: 74\n",
      "  learning_rate: 0.047706818419354494\n",
      "  training: .....................\n",
      "  precision: 0.98\n",
      "  recall: 0.98989898989899\n",
      "Iteration 7\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919144553/\n",
      "  batch size: 58\n",
      "  learning_rate: 0.0001694044709524274\n",
      "  training: .....................\n",
      "  precision: 0.9\n",
      "  recall: 0.9090909090909091\n",
      "Iteration 8\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919145408/\n",
      "  batch size: 61\n",
      "  learning_rate: 0.04171461199412461\n",
      "  training: .....................\n",
      "  precision: 0.9801980198019802\n",
      "  recall: 1.0\n",
      "Iteration 9\n",
      "  logdir: tf_logs/9_TF/logreg-exercise-20190919150420/\n",
      "  batch size: 92\n",
      "  learning_rate: 0.00010742922968438615\n",
      "  training: .....................\n",
      "  precision: 0.8823529411764706\n",
      "  recall: 0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "# use the reciprocal function for efficient random exploration of a vast parameter range\n",
    "# link: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html\n",
    "from scipy.stats import reciprocal\n",
    "# determine the number of training iterations and loop through them\n",
    "n_search_iterations = 10\n",
    "for search_iteration in range(n_search_iterations):\n",
    "    # determine characteristics of the current training iterations (some of them are random)\n",
    "    batch_size = np.random.randint(1, 100)\n",
    "    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)\n",
    "    n_inputs = 2 + 4\n",
    "    logdir = log_dir(\"logreg\")\n",
    "    # show characteristics of the current training iteration\n",
    "    print(\"Iteration\", search_iteration)\n",
    "    print(\"  logdir:\", logdir)\n",
    "    print(\"  batch size:\", batch_size)\n",
    "    print(\"  learning_rate:\", learning_rate)\n",
    "    print(\"  training: \", end=\"\")\n",
    "    # reset the graph and build a new one (including a file writer)\n",
    "    reset_graph()\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(\n",
    "        X, y, learning_rate=learning_rate)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    # number of epochs and batches\n",
    "    n_epochs = 10001\n",
    "    n_batches = int(np.ceil(m / batch_size))\n",
    "    # file path for the final model (note the dependence on \"search_iteration\")\n",
    "    final_model_path = \"./tf_logs/9_TF/my_logreg_model_%d\" % search_iteration\n",
    "    # run a tf session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # loop through epochs\n",
    "        for epoch in range(n_epochs):\n",
    "            # loop through batches\n",
    "            for batch_index in range(n_batches):\n",
    "                # build a mini-batch and feed it to the algorithm so it can make a training step\n",
    "                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            # get a summary and save it (\"loss_val\" is not used below)\n",
    "            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "            # print a dot every 500 epochs without changing the line\n",
    "            if epoch % 500 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "        # save the final model\n",
    "        saver.save(sess, final_model_path)\n",
    "        # make predictions\n",
    "        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})\n",
    "        y_pred = (y_proba_val >= 0.5)\n",
    "        # go to a new line and then print precision and recall scors\n",
    "        print()\n",
    "        print(\"  precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"  recall:\", recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output may help with finding hyperparamters that return good results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
