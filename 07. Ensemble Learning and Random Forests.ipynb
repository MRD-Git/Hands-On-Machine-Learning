{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests\n",
    "page 181<br>\n",
    "See\n",
    "- https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb,\n",
    "- https://www.kaggle.com/general/18793 (strategy A), and\n",
    "- https://docs.python.org/3/library/functions.html#enumerate.\n",
    "\n",
    "Aggregating answers from many randomly chosen people often yields a better insight on a question than asking one or a few experts. This is called the *wisdom of the crowd*. The machine learning analog to this is *Ensemble Learning* which gathers predictions from several predictors instead of just using one predictor. An algorithm that automatically employs ensemble learning is called an *ensemble Method*.<br>\n",
    "*Random Forests* aggregate predictions from many different decision trees that differ by each other because of random conditions imposed on them. For example, the selection of hyperparameters and/or of training instances may be selected randomly.\n",
    "## Voting Classifiers\n",
    "page 181<br>\n",
    "A *hard voting* classifier predicts that class that is predicted by the majority of classifiers in the predictor ensemble. Importantly, this method often performs better than the best classifier in the ensemble. A voting classifier can work very well even if all the classifiers in the ensemble are only marginally better than random guessing. This is due to the *law of large numbers*: flipping a slightly biased coin (51% heads vs. 49% tails) only three times and then hard voting for the majority face will not yield a very reliable prediction. But hard voting after flipping the coin 1000 times will yield a 75% accuracy.<br><br>\n",
    "**Suggestion or tip**<br>\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of erors, improving the ensemble's accuracy.<br><br>\n",
    "Below, we use a hard voting classifier on different classifiers, all of them from Scikit-Learn to separate a moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='warn', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decision_function_shape='ovr',\n",
       "                                  degree=3, gamma='auto_deprecated',\n",
       "                                  kernel='rbf', max_iter=-1, probability=False,\n",
       "                                  random_state=None, shrinking=True, tol=0.001,\n",
       "                                  verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make moons data and split it into training and testing sets (from Github link above)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# importing four different classiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()     # logistic regression classifier\n",
    "rnd_clf = RandomForestClassifier() # random forest classifier\n",
    "svm_clf = SVC()                    # support vector machine classifier          # below is the hard voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)   # train the hard voting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the ensemble only consists of three classifiers so the *law of large numbers* does not really apply. And still the ensemble predictions made by the hard voting classifier yield the best accuracy on some occasions. While the support vector machine and the logistic regression always return the same accuracy, the random forest classifier involves randomness and thus has a varying accuracy over different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.888\n",
      "VotingClassifier 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hard voting* simply classifies according to the majority vote without taking into account how confident these votes are. A measure for confidence of a vote could be the probability that a classifier gives for its vote if such probability is computed by the classifier. In Scikit-Learn, one thus needs a \"predict_proba()\" method. Classifying according to the aggregated confidences / probabilities is called *soft voting*. To implement this in a similar fashion as the hard voting above, we simply have the support vector machine classifier calculate probabilities and change the \"voting\" hyperparameter of the voting classifier from \"hard\" to \"soft\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.912\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(probability=True)  # instruct SVC to calculate probabilities (taken from Github link above)\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train) # train the soft voting classifier (see last hyperparameter in the above line)\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "page 185<br>\n",
    "Instead of training different models, one can also get different predictions by training the same model on different data. *Bagging* and *Pasting* are both techniques to obtain diverse (and smaller: size $m_1$) training sets from the full (and larger: size $m_2\\geq m_1$) training set. This is done in a random fashion. When each training instance may occur only once in each sampled training set (size $m_1$), this is called *pasting*. When it may occur several times in the same sampled training set (size $m_1$), it is called *bagging* which is short for *bootstrap aggregating* (any instance will be strapped in, no matter what it is). The aggregation function is usually *statistical mode*, which is the same as hard voting, for classification and the average for regression.<br>\n",
    "Due to the smaller training sets ($m_1$), each predictor has a higher bias (and probably also a higher variance) than if it was trained on the full training set ($m_2$). But the aggregation reduces both bias (assumption made on the data and thus on the model) and variance (model is sensitive to the addition of a new training instance). As the small training sets exist in parallel, the classifiers that are trained on only one of these small training sets each, can also be trained in parallel (they do not depend on each other). This can make training very fast if many predictors can be trained in parallel. The same applies for predictions. This possible speed-up makes bagging and pasting very popular methods.\n",
    "### Bagging and Pasting in Scikit-Learn\n",
    "page 186<br>\n",
    "*Bagging* in implemented in Scikit-Learn with \"BaggingClassifier\" for classification and \"BaggingRegressor\" for regression. *Pasting* uses exactly these algorithms yet with the bootstrapping (bootstrap aggregating = bagging) hyperparameter set to \"bootstrap=False\". An ensemble using one and the same algorithm trained on several small training sets (established with Bagging or Pasting) has usually a similar bias but a smaller variance than one single such algorithm trained on the entire training set, see Figure 7-5 on page 187.<br>\n",
    "Usually, *Bagging* performs a bit better than *Pasting* because allowing instances to occur several times in a small training set produces a more diverse ensemble of small training sets. But if enough compute is available one may also try both: sometimes *Pasting* produces better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# using \"n_jobs=-1\" in the following line uses all available cores (see last few sentences before \"Bagging and ...\n",
    "# ... Pasting in Scikit-Learn\" above); also note \"bootstrap=True\" for bagging\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General note**<br>\n",
    "The \"BaggingClassifier\" automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a \"predict_proba()\" method), which is the case with Decision Trees classifiers.\n",
    "### Out-of-Bag Evaluation\n",
    "page 187<br>\n",
    "For *Bagging* with $m_1=m_2$, on average only about 63% of the instances in the full training set (size $m_2$) occur in the sampled training set (size $m_1$). This is a logical consequence of bootstrap aggregation, i.e., putting an instance in the sampled training set even if it occurs there already (once ore more times). That means that on average there are about 37% *out-of-bag* (oob) instances that do not occur in the sampled training set and can thus be used for testing. These oob instances are different for all predictors in the ensemble.<br>\n",
    "Setting \"oob_score=True\" for a \"BaggingClassifier\" with \"bootstrap=True\" (default) in Scikit-Learn produces an automatic oob evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9013333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use all available cores for training and compute the score performed on the out-of-bag instances for each classifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "# train\n",
    "bag_clf.fit(X_train, y_train)\n",
    "# show the out-of-bag score (this must be an average of \"oob_score\" over all \"n_estimators\" or something like that)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, ons should note that the entire ensemble has (most likely) seen all instances so our *BaggingClassifier* might still be prone to overfitting. Let's check that with the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n",
      "[[0.37714286 0.62285714]\n",
      " [0.37433155 0.62566845]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.09714286 0.90285714]\n",
      " [0.31016043 0.68983957]\n",
      " [0.01086957 0.98913043]\n",
      " [0.98507463 0.01492537]\n",
      " [0.99428571 0.00571429]\n",
      " [0.75287356 0.24712644]\n",
      " [0.         1.        ]\n",
      " [0.73214286 0.26785714]\n",
      " [0.87283237 0.12716763]\n",
      " [0.96470588 0.03529412]\n",
      " [0.04864865 0.95135135]\n",
      " [0.         1.        ]\n",
      " [0.96491228 0.03508772]\n",
      " [0.92571429 0.07428571]\n",
      " [0.99470899 0.00529101]\n",
      " [0.02105263 0.97894737]\n",
      " [0.32338308 0.67661692]\n",
      " [0.91758242 0.08241758]\n",
      " [1.         0.        ]\n",
      " [0.99411765 0.00588235]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.61077844 0.38922156]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.17837838 0.82162162]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.38251366 0.61748634]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.23756906 0.76243094]\n",
      " [0.29239766 0.70760234]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.02173913 0.97826087]\n",
      " [1.         0.        ]\n",
      " [0.00534759 0.99465241]\n",
      " [0.99408284 0.00591716]\n",
      " [0.89673913 0.10326087]\n",
      " [0.99415205 0.00584795]\n",
      " [0.96610169 0.03389831]\n",
      " [0.         1.        ]\n",
      " [0.06111111 0.93888889]\n",
      " [1.         0.        ]\n",
      " [0.00568182 0.99431818]\n",
      " [0.         1.        ]\n",
      " [0.00546448 0.99453552]\n",
      " [1.         0.        ]\n",
      " [0.84023669 0.15976331]\n",
      " [0.36470588 0.63529412]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.7486911  0.2513089 ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.87790698 0.12209302]\n",
      " [1.         0.        ]\n",
      " [0.57377049 0.42622951]\n",
      " [0.14438503 0.85561497]\n",
      " [0.66326531 0.33673469]\n",
      " [0.91477273 0.08522727]\n",
      " [0.         1.        ]\n",
      " [0.16666667 0.83333333]\n",
      " [0.925      0.075     ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.99438202 0.00561798]\n",
      " [0.         1.        ]\n",
      " [0.02525253 0.97474747]\n",
      " [0.01075269 0.98924731]\n",
      " [0.36516854 0.63483146]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.8700565  0.1299435 ]\n",
      " [0.00518135 0.99481865]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.21857923 0.78142077]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.94811321 0.05188679]\n",
      " [0.71910112 0.28089888]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.21022727 0.78977273]\n",
      " [0.63350785 0.36649215]\n",
      " [0.         1.        ]\n",
      " [0.05759162 0.94240838]\n",
      " [0.47894737 0.52105263]\n",
      " [1.         0.        ]\n",
      " [0.01485149 0.98514851]\n",
      " [1.         0.        ]\n",
      " [0.33333333 0.66666667]\n",
      " [0.43979058 0.56020942]\n",
      " [1.         0.        ]\n",
      " [0.0106383  0.9893617 ]\n",
      " [0.99450549 0.00549451]\n",
      " [0.30107527 0.69892473]\n",
      " [0.92178771 0.07821229]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.74705882 0.25294118]\n",
      " [1.         0.        ]\n",
      " [0.02083333 0.97916667]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.98342541 0.01657459]\n",
      " [0.99479167 0.00520833]\n",
      " [0.         1.        ]\n",
      " [0.95402299 0.04597701]\n",
      " [0.99456522 0.00543478]\n",
      " [0.01630435 0.98369565]\n",
      " [0.19786096 0.80213904]\n",
      " [0.96954315 0.03045685]\n",
      " [0.29189189 0.70810811]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.00492611 0.99507389]\n",
      " [0.6684492  0.3315508 ]\n",
      " [0.39130435 0.60869565]\n",
      " [0.40588235 0.59411765]\n",
      " [0.84615385 0.15384615]\n",
      " [0.91666667 0.08333333]\n",
      " [0.05       0.95      ]\n",
      " [0.81218274 0.18781726]\n",
      " [0.00529101 0.99470899]\n",
      " [0.         1.        ]\n",
      " [0.01639344 0.98360656]\n",
      " [0.99444444 0.00555556]\n",
      " [0.99468085 0.00531915]\n",
      " [1.         0.        ]\n",
      " [0.00502513 0.99497487]\n",
      " [0.         1.        ]\n",
      " [0.01111111 0.98888889]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.95402299 0.04597701]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.37575758 0.62424242]\n",
      " [0.24581006 0.75418994]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.30939227 0.69060773]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.99033816 0.00966184]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.00531915 0.99468085]\n",
      " [0.63243243 0.36756757]\n",
      " [0.91089109 0.08910891]\n",
      " [0.         1.        ]\n",
      " [0.97916667 0.02083333]\n",
      " [1.         0.        ]\n",
      " [0.99462366 0.00537634]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.04736842 0.95263158]\n",
      " [1.         0.        ]\n",
      " [0.04232804 0.95767196]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.06       0.94      ]\n",
      " [1.         0.        ]\n",
      " [0.96531792 0.03468208]\n",
      " [0.7539267  0.2460733 ]\n",
      " [0.5990099  0.4009901 ]\n",
      " [0.         1.        ]\n",
      " [0.14361702 0.85638298]\n",
      " [1.         0.        ]\n",
      " [0.92783505 0.07216495]\n",
      " [0.97297297 0.02702703]\n",
      " [1.         0.        ]\n",
      " [0.00653595 0.99346405]\n",
      " [0.         1.        ]\n",
      " [0.41358025 0.58641975]\n",
      " [0.83076923 0.16923077]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.00526316 0.99473684]\n",
      " [0.00574713 0.99425287]\n",
      " [0.9673913  0.0326087 ]\n",
      " [0.         1.        ]\n",
      " [0.25853659 0.74146341]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.98245614 0.01754386]\n",
      " [0.86243386 0.13756614]\n",
      " [0.98895028 0.01104972]\n",
      " [0.         1.        ]\n",
      " [0.07734807 0.92265193]\n",
      " [0.98795181 0.01204819]\n",
      " [0.02061856 0.97938144]\n",
      " [0.         1.        ]\n",
      " [0.03012048 0.96987952]\n",
      " [1.         0.        ]\n",
      " [0.78089888 0.21910112]\n",
      " [0.         1.        ]\n",
      " [0.89772727 0.10227273]\n",
      " [1.         0.        ]\n",
      " [0.13989637 0.86010363]\n",
      " [0.23036649 0.76963351]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.20118343 0.79881657]\n",
      " [0.96551724 0.03448276]\n",
      " [0.00478469 0.99521531]\n",
      " [1.         0.        ]\n",
      " [0.99459459 0.00540541]\n",
      " [0.         1.        ]\n",
      " [0.52020202 0.47979798]\n",
      " [1.         0.        ]\n",
      " [0.00518135 0.99481865]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.11560694 0.88439306]\n",
      " [0.14       0.86      ]\n",
      " [0.98882682 0.01117318]\n",
      " [0.01036269 0.98963731]\n",
      " [1.         0.        ]\n",
      " [0.43093923 0.56906077]\n",
      " [0.08994709 0.91005291]\n",
      " [0.52760736 0.47239264]\n",
      " [0.61290323 0.38709677]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.64739884 0.35260116]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.2606383  0.7393617 ]\n",
      " [0.73480663 0.26519337]\n",
      " [0.07734807 0.92265193]\n",
      " [1.         0.        ]\n",
      " [0.7752809  0.2247191 ]\n",
      " [0.         1.        ]\n",
      " [0.00558659 0.99441341]\n",
      " [0.13131313 0.86868687]\n",
      " [0.02150538 0.97849462]\n",
      " [0.         1.        ]\n",
      " [0.99421965 0.00578035]\n",
      " [0.9025641  0.0974359 ]\n",
      " [0.13142857 0.86857143]\n",
      " [0.9804878  0.0195122 ]\n",
      " [0.01010101 0.98989899]\n",
      " [0.5819209  0.4180791 ]\n",
      " [0.06010929 0.93989071]\n",
      " [0.99462366 0.00537634]\n",
      " [0.85572139 0.14427861]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.94711538 0.05288462]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.2962963  0.7037037 ]\n",
      " [0.98850575 0.01149425]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.83695652 0.16304348]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.79234973 0.20765027]\n",
      " [0.94974874 0.05025126]\n",
      " [1.         0.        ]\n",
      " [0.69005848 0.30994152]\n",
      " [0.56281407 0.43718593]\n",
      " [0.         1.        ]\n",
      " [0.90229885 0.09770115]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.89502762 0.10497238]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.73743017 0.26256983]\n",
      " [0.09782609 0.90217391]\n",
      " [0.50837989 0.49162011]\n",
      " [0.24431818 0.75568182]\n",
      " [0.         1.        ]\n",
      " [0.85024155 0.14975845]\n",
      " [0.83815029 0.16184971]\n",
      " [0.00492611 0.99507389]\n",
      " [1.         0.        ]\n",
      " [0.99444444 0.00555556]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.04166667 0.95833333]\n",
      " [0.98224852 0.01775148]\n",
      " [0.97126437 0.02873563]\n",
      " [1.         0.        ]\n",
      " [0.47457627 0.52542373]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.99444444 0.00555556]\n",
      " [0.02366864 0.97633136]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.96335079 0.03664921]\n",
      " [0.         1.        ]\n",
      " [0.08       0.92      ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.9895288  0.0104712 ]\n",
      " [0.01485149 0.98514851]\n",
      " [1.         0.        ]\n",
      " [0.11235955 0.88764045]\n",
      " [0.         1.        ]\n",
      " [0.00515464 0.99484536]\n",
      " [0.         1.        ]\n",
      " [0.33707865 0.66292135]\n",
      " [0.07567568 0.92432432]\n",
      " [0.18857143 0.81142857]\n",
      " [1.         0.        ]\n",
      " [0.99470899 0.00529101]\n",
      " [0.22751323 0.77248677]\n",
      " [0.99450549 0.00549451]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.94764398 0.05235602]\n",
      " [0.3        0.7       ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.97777778 0.02222222]\n",
      " [0.00574713 0.99425287]\n",
      " [0.0125     0.9875    ]\n",
      " [0.97619048 0.02380952]\n",
      " [1.         0.        ]\n",
      " [0.02525253 0.97474747]\n",
      " [0.640625   0.359375  ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)      # make predictions on the test set\n",
    "print(accuracy_score(y_test, y_pred)) # check the accuracy of these predictions\n",
    "print(bag_clf.oob_decision_function_) # show the predicted class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even over several runs - all of which produce different results because of randomness - there is not really a sign of overfitting. The \"oob_score\" further above might be an average of the scores of the \"n_estimators\" individual scores. Due to the *law of large numbers*, it is not surprising that the ensemble performs better than the average individual classifier.<br>\n",
    "The \"oob_decision_function\" variable allows access to the class probabilities (the calculation of a *probability* is due to the fact that the base estimator has a \"predict_proba()\" method). These are inferred from aggregated oob-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "page 188<br>\n",
    "Instead of only sampling a selection of instances, it is also possible to only sample a selection of features. For the Scikit-Learn's \"BaggingClassifier\" class this is controlled by the hyperparameters \"max_features\" and \"bootstrap_features\", which are the feature analogues of \"max_samples\" and \"bootstrap\". Sampling both instances and features is called the *Random Patches* method. Sampling features (\"bootstrap_features=True\" and/or \"max_features\"<1.0) while keeping all training instances (\"bootstrap=False\" and \"max_samples=1.0\") is called the *Random Subspaces* method. These methods can be used to get (even) more diversity in the predictor ensemble.\n",
    "## Random Forests\n",
    "page 189<br>\n",
    "Instead of having a \"BaggingClassifier\" employ the \"DecisionTreeClassifier\" algorithm, one can directly use Scikit-Learn's \"RandomForestClassifier\" (or the \"RandomForestRegressor\" instead of the \"DecisionTreeRegressor\") which is optimized towards Decision Trees. The \"BaggingClassifier\" is still useful if one wishes to aggregate decisions from different kinds of algorithms (other than only Decistion Trees). Let's try the \"RandomForestClassifier\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By and large, the \"RandomForestClassifier\" has the same hyperparameters as the combination of the \"BaggingClassifier\" (to control the ensemble) and the \"DecisionTreeClassifier\" (to control the individual trees). See page 189 and footnote 11 there for details. The following \"BaggingClassifier\" is roughly equivalent to the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees\n",
    "page 190<br>\n",
    "Due to the *law of large numbers*, ensembles and thus Random Forests rely on the independency of the aggregated predictions. Randomness leads to diversity and thus to independency. *Extremely Randomized Trees* or *Extra-Trees* use a random feature subset at each node, just like a regular tree in a Random Forest would do by default. (Note that this is different for a single Decision Tree that would search for the most relevant feature.) But *Extra-Trees* go further than the regular tree in the Random Forest by choosing a random threshold. For the ensemble, this leads once again to higher bias and lower variance. And it makes the growing of Extra-Trees much faster than a regular Random Forest. In Scikit-Learn, Extra Trees are implemented with the \"ExtraTreeRegressor\" and \"ExtraTreeClassifier\" classes.<br><br>\n",
    "**Tip or suggestion**<br>\n",
    "It is hard to tell in advance whether a \"RandomForestClassifier\" will perform better or worse than an \"ExtraTreesClassifier\". Generally, the only way to know is to try both and compare them using cross-validation (and tuning the hpyerparameters using grid search).\n",
    "### Feature Importance\n",
    "page 190<br>\n",
    "For Random Forests, Scikit-Learn can measure how much a feature reduces the impurity (or entropy). Averaging this reduction of impurity (or entropy) over all trees gives the correctly weighted importance of that feature. Doing this for all features and normalizing the outcomes so they sum up to 1 shows the relative importance of a feature. Scikit-Learn does this automatically. The feature importances are accessible through the \"feature_importances_\" variable. Below, we output the feature importances of the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09398500671749066\n",
      "sepal width (cm) 0.023923069847677697\n",
      "petal length (cm) 0.4566600775910901\n",
      "petal width (cm) 0.42543184584374155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Random Forests are very handy to find out what features are actually important. This can be used for feature selection.\n",
    "## Boosting\n",
    "page 191<br>\n",
    "*Hypothesis Boosting* or just *Boosting* for short refers to sequentially ordered predictors where each tries to correct its predecessor. This allows the combination of many weak learners into one strong learner. However, parallelization of sequential predictors is not possible since the predecessor relies on the predictions of its successor. There are many boosting methods available but *AdaBoost* and *Gradient Boosting* are by far the most popular ones.\n",
    "### AdaBosst\n",
    "page 192<br>\n",
    "*Adaptive Boosting* or *AdaBoost* for short uses a sequence of predictors where each successor increases the weight of those instances that have been misclassified by its predecessor. The amount by which the weight of these instances is increased can be manipulated with a *learning rate* hyperparameter. Once the entire sequence of predictors is trained, the ensemble makes predictions based on all predictors, where each predictor's prediction is weighted by its accuracy.<br><br>\n",
    "**Warning / caution**<br>\n",
    "There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting.<br><br>\n",
    "The weight of all instances is initalized as $w^{(i)}=1/m$, with $m$ being the number of instances. The weighted error rate of the $j$-th predictor is defined as\n",
    "$$r_j=\\frac{\\sum_{i=1,y_j^{(i)}\\neq y^{(i)}}^mw^{(i)}}{\\sum_{i=1}^mw^{(i)}}\\,,$$\n",
    "where $y_j^{(i)}$ is the $j$-th predictor's predictions on instance $i$. This predictors weight $\\alpha_j$ is then determined via\n",
    "$$\\alpha_j=\\eta\\log\\frac{1-r_j}{r_j}\\,,$$\n",
    "where $\\eta$ is the *learning rate*. Note that $\\log\\frac{1-r_j}{r_j}=\\log(1-r_j)-\\log(r_j)$ goes to $-\\infty$ for $r_j\\to1$ (everything wrong => go the other direction) and to $+\\infty$ for $r_j\\to0$ (everything right => go this direction, very far). For the next predictor, the instance weights are updated as follows:\n",
    "$$w^{(i)}=\\left\\{\\begin{array}f\\text{$w^{(i)}\\,\\,\\,\\,\\,\\quad\\qquad$ if $y^{(i)}_j=y^{(i)}$,}\\\\\\text{$w^{(i)}\\exp(\\alpha_j)\\quad$ if $y^{(i)}_j\\neq y^{(i)}$.}\\end{array}\\right.$$\n",
    "Then all instance weights are normalized by dividing by $\\sum_{i=1}^mw^{(i)}$.<br>\n",
    "This iteration stops when the specified upper bound of total predictors has been reached or when a perfect predictor has been found ($r_{\\text{final}}=0$). AdaBoost predicts the class that gets the highest weighted prediction of all $N$ predictors:\n",
    "$$y(x)=\\text{argmax}_k\\sum_{j=1,\\,y_j(x)=k}^N\\alpha_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code produces 200 *Decision Stumps* (depth=1 => one decision node and two leaf nodes). In addition to the \"AdaBoostClassifier\", there is also an \"AdaBoostRegressor\". Scikit-Learn actually uses a multiclass version of AdaBoost called *SAMME* (Stagewise Additive Modeling using a Multiclass Exponential loss function). With just two classes, *SAMME* is equivalent to AdaBoost apart from the fact that it uses class probabilities instead of class predictions, i.e., soft voting instead of hard voting.<br><br>\n",
    "**Tip or suggestion**<br>\n",
    "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.\n",
    "### Gradient Boosting\n",
    "page 195<br>\n",
    "Instead of increasing the weight of misclassified instances, *Gradient Boosting* reduces errors by training a successor algorithm on the residual errors made by the preceding algorithm. Here, we implement this with Decision Trees that are trained on a noisy quadratic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75026781]\n"
     ]
    }
   ],
   "source": [
    "# generate noisy quadratic data\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "# hardcode gradient boosting (in 3 stages)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)            # 1st set of predictions\n",
    "y2 = y - tree_reg1.predict(X)  # use the remaining error as features for the second predictor\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)           # 2nd set of predictions\n",
    "y3 = y2 - tree_reg2.predict(X) # again, use the remaining errors as new features\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)           # 3rd set of predictions\n",
    "# make prediction\n",
    "X_new = np.array([[0.8]])      # new instance\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)) # the total prediction is the sum ...\n",
    "# ... of the first prediction plus the first and second correction\n",
    "print(y_pred)                  # show resulting total prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use code from Github to display the progress of the regression as gradient boosting advances, i.e., as subsequent trees try to correct previous deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x1100 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# residual imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# define plotting function\n",
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)\n",
    "# plot figures\n",
    "plt.figure(figsize=(11,11))\n",
    "# first prediction\n",
    "plt.subplot(321)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\",\n",
    "                 data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
    "# first ensemble = first prediction\n",
    "plt.subplot(322)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\",\n",
    "                 data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Ensemble predictions\", fontsize=16)\n",
    "# first correction\n",
    "plt.subplot(323)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals\")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "# second ensemble = first prediction + first correction\n",
    "plt.subplot(324)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "# second correction\n",
    "plt.subplot(325)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "# second ensemble = second prediction + second correction\n",
    "plt.subplot(326)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8],\n",
    "                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit-Learn, one can simply use the \"GradientBoostingRegressor\" class instead of the above code. It has hyperparameters that tune the growth of the trees and other hyperparameters that control the gradient boosting. The following code is very similar to the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=1.0, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=3,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the learning rate hyperparameter determines how strongly subsequent trees are taken into account. Setting it to a low value has the effect that more trees are required to fit the data well. But the predictions usually generalize better. This regularization technique is called *shrinkage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same code as above but with 10 times more estimators and 10 times smaller learning rate\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=30, learning_rate=0.1)\n",
    "gbrt_slow.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the above trees (with code from Github). Note that using too many trees can lead to overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121) # first subplot\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
    "plt.subplot(122) # second subplot\n",
    "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may attempt (but not guarantee) to find the optimum number of trees as follows: (i) make so many trees (with Gradient Boosting) that you think they will be surely enough (i.e., they will most likely not underfit); (ii) on the validation set, calculate your performance measure of choice after each added tree; (iii) when overfitting sets in, this measure should go up; (iv) use that number of trees that has led to the best performance; (v) retrain an algorithm with that number of trees.<br>\n",
    "Note that this is some kind of early stopping: the validation error goes up when overfitting sets in. Now, let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=61,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)         # split dataset in training and validation sets\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) # 120 predictors should be enough\n",
    "gbrt.fit(X_train, y_train)                                      # train the model\n",
    "errors = [mean_squared_error(y_val,y_pred) for y_pred in gbrt.staged_predict(X_val)] # THIS IS KEY: ...\n",
    "# ... with \"staged_predict\" one can calculate the mean squared error after each added predictor\n",
    "bst_n_estimators = np.argmin(errors)                            # in this row is the smallest validation error\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)    # retrain with this number of ...\n",
    "gbrt_best.fit(X_train, y_train)                                 # ... trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With code from Github, we plot the validation error as a function of the number of trees and we show the regression results obtained with the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031284945152031435\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003154387980516349\n"
     ]
    }
   ],
   "source": [
    "min_error = np.min(errors)  # above, we only inferred in which row (=number of trees) the erros is smallest; this ...\n",
    "print(min_error)            # ... here is the value of this smallest error\n",
    "plt.figure(figsize=(11, 4)) # make a plot\n",
    "# validation error\n",
    "plt.subplot(121)\n",
    "plt.plot(errors, \"b.-\")\n",
    "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
    "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
    "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
    "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
    "plt.axis([0, 120, 0, 0.01])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.title(\"Validation error\", fontsize=14)\n",
    "# best model\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
    "plt.show()\n",
    "# mean squared error of best model\n",
    "y_pred_best_val = gbrt_best.predict(X_val)\n",
    "mse_val_best=mean_squared_error(y_val, y_pred_best_val)\n",
    "print(mse_val_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to implement early stopping such that the algorithm stops immediately when the validation error goes up (instead of finishing all the way and then look back at the validation error minimum). This can be achieved with \"warm_start=True\", which has Scikit-Learn keep existing trees. The following code stops training once the validation error did not reach a new minimum for five iterations in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031284945152031435\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True) # keep existing trees with warm_start=True\n",
    "min_val_error = float(\"inf\")                      # placeholder for minimum validation error\n",
    "error_going_up = 0                                # count consecutive times of error not reaching a new minimum\n",
    "for n_estimators in range(1, 120):                # loop from 1 through 120\n",
    "    gbrt.n_estimators = n_estimators              # establish a new estimator (between 1 and 120)\n",
    "    gbrt.fit(X_train, y_train)                    # train the new estimator\n",
    "    y_pred = gbrt.predict(X_val)                  # make predictions on the validation set\n",
    "    val_error = mean_squared_error(y_val, y_pred) # calculate the mean squared error on the validation set\n",
    "    if val_error < min_val_error:                 # check if it is a new minimum error ...\n",
    "        min_val_error = val_error                 # ... and if so: update the minimum error ...\n",
    "        error_going_up = 0                        # ... and reset \"error_going_up\" to 0\n",
    "    else:                                         # otherwise ...\n",
    "        error_going_up += 1                       # increase \"error_going_up\" by 1 ...\n",
    "        if error_going_up == 5:                   # ... and if a count of 5 has been reached ...\n",
    "            print(min_val_error)                  # ... print the minimum validation error ...\n",
    "            print(n_estimators)                   # ... as well as the total number of trained estimators ...\n",
    "            break                                 # ... and finish the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"GradientBoostingRegressor\" also supports *Stochastic Gradient Boosting*, whereby each tree is trained only on a fraction of all available training instances. This fraction can be set with the hyperparameter \"subsample\", e.g. *subsample=0.25* if each predictor shall be trained on only one randomly chosen quarter of all training instances. Again, this increases the bias but lowers the variance and makes the algorithm faster.<br><br>\n",
    "**General note**<br>\n",
    "It is possible to use Gradient Boosting with other cost functions. This is controlled by the \"loss hyperparameter\" 8see Scikit-Learn's documentation for more details).\n",
    "## Stacking\n",
    "page 200<br>\n",
    "We have learned that the aggregated predictions of the ensemble can be transformed into a final vote by hard or soft voting. Why not have a predictor make the final decision? *Stacked generalization* or *stacking* for short does exaclty that. For a given instance, the individual predictions of all the predictors act as features fed to a *blender* (final predictor) that makes the final decision.<br>\n",
    "In order to train the blender, one first splits the training set into two subsets and trains the ensemble (without the blender) on the first subset. When this is done, the ensemble makes predictions on the second subset, yielding as many features (predictions) as there are predictors in the ensemble. These predictions are *clean* as the ensemble has not seen these instances during training. Then, the blender is trained on these clean predictions. Instead of using a hold-out set for training, one may also employ out-of-fold predictions, see footnote 19 on page 200 and the second link above. Using out-of-fold predictions is called *stacking* and using a hold out set is called *blending* by some people.<br>\n",
    "In the second layer, one may actually train different blenders, e.g., an SVM, a Random Forest, and so on. The predictions of these blenders are then combined in a third layer. To this end, one needs to split the training data into three subsets: one to train the base predictors, the second to have the base predictors make predictions that the middle-layer blenders are trained on, and the third set for training of the top-level blender which takes predictions form the middle layer as input.<br>\n",
    "Scikit-Learn does not have a stacking algorithm. However, implementing stacking will be one of the exercises of this chapter.\n",
    "## Exercises\n",
    "page 202\n",
    "### 1.-7.\n",
    "Solutions are shown in Appendix A of the book and in the separate notebook *ExercisesWithoutCode*.\n",
    "### 8.\n",
    "Load the MNIST data (introduced in Chapter 3), and split it into a a training set, a validation set, and a test set (e.g., use 40000 instances for training, 10000 for validation, and 10000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the indicidual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:85: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:85: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                     n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                     warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
      "          verbose=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9467, 0.9512, 0.8547, 0.9608]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish training, testing, and validation sets for the MNIST dataset\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data, mnist.target, test_size=10000,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=10000, random_state=42)\n",
    "# establish 4 different classifiers\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "random_forest_clf = RandomForestClassifier(random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(random_state=42)\n",
    "svm_clf = LinearSVC(random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)\n",
    "# train the classifiers\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)\n",
    "# show the scores\n",
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9467, 0.9512, 0.8547, 0.9608]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the classifiers into a hard-voting ensemble classifier\n",
    "named_estimators = [\n",
    "    (\"random_forest_clf\", random_forest_clf),\n",
    "    (\"extra_trees_clf\", extra_trees_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "]\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "# compare the ensemble score witht the individual scores\n",
    "print(voting_clf.score(X_val, y_val))\n",
    "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble is better than each of its constituents. However, the support vector machine is performing really bad. Maybe the ensemble is better without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('random_forest_clf',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                         max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                         min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                         min_samples_leaf=1, min_samples_split=2,\n",
       "                         min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                         n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                         warm_start=False)),\n",
       " ('extra_trees_clf',\n",
       "  ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "                       oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)),\n",
       " ('svm_clf', None),\n",
       " ('mlp_clf',\n",
       "  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "                beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "                hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "                learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "                n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "                random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
       "                validation_fraction=0.1, verbose=False, warm_start=False))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following line removes the support vector machine from the ensemble, however ...\n",
    "voting_clf.set_params(svm_clf=None)\n",
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                        n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                        warm_start=False),\n",
       " ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "                      oob_score=False, random_state=42, verbose=0,\n",
       "                      warm_start=False),\n",
       " LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "           verbose=0),\n",
       " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "               n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "               random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
       "               validation_fraction=0.1, verbose=False, warm_start=False)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... that command does not remove it from the trained ensemble\n",
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to really remove the SVM from the (trained) ensemble, we can delete it explicitly as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9659"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del voting_clf.estimators_[2]  # delete the third estimator (SVM) from the trained ensemble\n",
    "voting_clf.score(X_val, y_val) # run the voting classifier again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the hard voting classifier performs a bit better withou the SVM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now try soft voting\n",
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soft voting classifier performs even a bit better than the hard voting classifier. Let's try is on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9434, 0.9444, 0.9629]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(voting_clf.score(X_test, y_test))                                  # ensemble score with soft voting on test set\n",
    "[estimator.score(X_test,y_test) for estimator in voting_clf.estimators_] # individual scores on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifiers you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., 2.],\n",
       "       [7., 7., 7., 7.],\n",
       "       [4., 4., 4., 4.],\n",
       "       ...,\n",
       "       [4., 4., 4., 4.],\n",
       "       [9., 9., 9., 9.],\n",
       "       [4., 4., 4., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect all the predictions on the validation set of all 4 (still trained, see above) estimators\n",
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32) # data container\n",
    "for index, estimator in enumerate(estimators):                                # loop through columns / estimators ...\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)                    # ... see third link at top of page\n",
    "X_val_predictions                                                             # show the input for the blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "                       n_jobs=None, oob_score=True, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "0.9638\n"
     ]
    }
   ],
   "source": [
    "# train a random forest classifier on blending these predictions and check its score\n",
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "print(rnd_forest_blender.fit(X_val_predictions, y_val))\n",
    "print(rnd_forest_blender.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9605"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish test set instances (predicions of classifiers) and check the trained blender's predicions on them\n",
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)\n",
    "y_pred = rnd_forest_blender.predict(X_test_predictions)\n",
    "# accuracy of blending ensemble on the test set\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
