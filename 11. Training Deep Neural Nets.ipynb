{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets\n",
    "page 275<br>\n",
    "See\n",
    "- https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "- http://goo.gl/1rhAef (Xavier Glorot and Yoshua Bengio publication)\n",
    "- http://goo.gl/VHP3pB (K. He *et al.* publication),\n",
    "- http://goo.gl/gA4GSP (Sergey Ioffe and Christian Szegedy),\n",
    "- https://arxiv.org/pdf/1706.02515.pdf (Günter Klambauer, Thomas Unterthiner, and Andreas Mayr publication),\n",
    "- https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo (training, validation, and test sets),\n",
    "- http://goo.gl/dRDAaf (gradient clipping),\n",
    "- and for details.\n",
    "\n",
    "When training (very) deep neural networks, a number of problems are likely to arise:\n",
    "- The tricky **vanishing gradiens** and **exploding gradients** problems affects deep neural networks and makes it difficult to train lower lying layer efficiently.\n",
    "- For a very large network, training will usually be very slow.\n",
    "- A model with millions of parameters is likely to overfit.\n",
    "\n",
    "All these problems – and possible solutoins – will be addressed in this chapter.\n",
    "## Vanishing/Exploding Gradients Problems\n",
    "page 275<br>\n",
    "As discussed in chapter 10, backpropagation calculates all the gradients in passing the graph twice: first in the forward direction to compute each node's output and then in the backward direction to calculate all the gradients (using the node values from the forward pass). Unfortunately, the gradienst tend to become smaller and smaller during the backward pass so that the gradients are very small in the lower layers. This makes it difficult to train the lower layers. This is the **vanishing gradient problem**. For the related **exploding gradient problem**, the gradients become larger and larger for lower and lower layers. thus making the algorithm diverge due to too large update steps. The latter is mainly observed in recurrent neural networks (see chapter 14).<br>\n",
    "As Xavier Glorot and Yoshua Bengio found in 2010 (see publication link above), the vanishing gradient problem can be caused by an unfortunate combination of weight initialization (radnomly with Gaussian distribution of mean 0 and standard deviation 1) and activation function (sigmoid). They found that in the forward pass, the variance increases from layer to layer, thus leading to large [small] gradients in the top [lower] layers.<br>\n",
    "For illustration, the logistic function (sigmoid,see chapter 4) saturates for large (positive or negative) values. So even when the (large) input changes a lot, the output changes almost not at all.\n",
    "### Xavier and He Initialization\n",
    "page 277<br>\n",
    "Glorot and Bengio argued that the signal needs to flow properly both forward (for predictions during inference) and backward (for weight updates during training). To achieve that, the variance of the outputs should be equal to the variance of the inputs. Additionally, the gradients (derivatives of the signal) also should all have equal variance, too. While it is not possible to guarantee equal variance throughout the network for both (i) signal and (ii) gradients, optimization is still possible. For a logistic activation function, **Xavier-** or **Glorot initialization** (named after the first [last] name of the first author of the Glorot & Bengio paper) uses (see Equation 11-1 in the book)<br>\n",
    "$$\\text{a normal distribution with $\\mu=0$ and }\\sigma=\\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}}\\\\\n",
    "\\text{or a uniform distribution between $-r$ and $+r$, with} r=\\sqrt{\\frac{6}{n_{\\rm inputs}+n_{\\rm outputs}}}\\,,$$\n",
    "where $n_{\\rm inputs\\,[outputs]}$ is the number of input [output] connections (also called **fan-in** and **fan-out**, respectively). When the number of inputs is roughly equal to the number of outputs (order of magintude), one may simply use $\\sigma=1/\\sqrt{n_{\\rm inputs}}$ or $r=\\sqrt{3}/\\sqrt{n_{\\rm inputs}}$.<br>\n",
    "Similar best practices for initialization have been found for other activation functions, see the paper by He *et al.* linked above. An overview is shown in the table below (Table 11-1 in the book).\n",
    "\n",
    "|Activation function|Uniform distribution [-r,r]|Normal distribution$\\text{$\\qquad$}$|\n",
    "|-|-|-|\n",
    "|Logistic|$r=\\sqrt{\\frac{6}{n_{\\rm inputs}+n_{\\rm outputs}}}$|$\\sigma=\\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}}$|\n",
    "|Hyperbolic tangent|$r=4\\sqrt{\\frac{6}{n_{\\rm inputs}+n_{\\rm outputs}}}$|$\\sigma=4\\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}}$|\n",
    "|ReLU (and variants)|$r=\\sqrt{2}\\sqrt{\\frac{6}{n_{\\rm inputs}+n_{\\rm outputs}}}$|$\\sigma=\\sqrt{2}\\sqrt{\\frac{2}{n_{\\rm inputs}+n_{\\rm outputs}}}$|\n",
    "\n",
    "By default `tf.layers.dense()` uses Xavier initialization with a uniform distribution. Changing to He initialization is as easy as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/Relu:0' shape=(?, 4) dtype=float32>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "# function to reset the graph (always a good idea)\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "# changing the initializing method from Xavier to He\n",
    "n_inputs = 1 + np.random.randint(10)\n",
    "n_hidden1 = 1 + np.random.randint(10)\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # placeholder for features\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")\n",
    "hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General note**<br>\n",
    "He initialization considers only the fan-in [input connections], not the average between fan-in and fan-out [output connections] like in Xavier initialization. This is also the default for the `variance_scaling_initializer()` function, but you can change this by setting the argument `mode=\"FAN_AVG\"`.\n",
    "### Nonsaturating Activation Funcitons\n",
    "page 279<br>\n",
    "As Glorot and Bengio showed, the activation function is important. The sigmoid had been assumed to be a very good choice because nature employs activations that resemble the sigmoid very much. But it turns out that ReLUs work much better. And even the ReLU has its shortcomings: (i) it's derivative at 0 is not defined and (ii) it's derivative vanishes for negative values. The latter property leads to the phenomenon of dying ReLUs. In particular for large learning rates, it happens that a ReLU starts outputting 0. Due to the vanishing gradient, it is unlikely to continue learning – it is dead. In some cases, 50% of all ReLUs are dead after some training time (**dying ReLUs**). This has led to several versions of ReLU. The **leaky ReLU** outputs $\\max(\\alpha z,\\,z)$ with $0<\\alpha<1$. So for $z>0$, it is the ReLU and for $z<0$ it is similar to ReLU but it leaks out a finite gradient. A small [huge] leak corresponds to $\\alpha=0.01[0.2]$. The leaky ReLU always outperforms the ReLU, see https://goo.gl/B1xhKn (they also found that $\\alpha=0.2$ works better than $\\alpha=0.01$). The **randomized leaky ReLU** (**RReLU**) uses a different $\\alpha$ (within a given range) during training and fixed average during testing. RRelu performs well and also seems to act like a regularizer: it reduces the risk of overfitting. The **parametric leaky ReLU** (**PReLU**) uses $\\alpha$ such that is learned during training. This outperforms ReLU on large datasets but leads to overfitting for small datasets. Finally, Djork-Arné Clevert *et al.* found that their proposed **exponential linear unit** (**ELU**, see Equation 11-2 in the book),<br><br>\n",
    "$$\\text{ELU}_{\\alpha}(x)=\\left\\{\\begin{array}0\\alpha({\\rm exp}(z)-1)&{\\rm if}\\quad z<0\\\\z&{\\rm if}\\quad z\\geq0\\end{array}\\right.$$\n",
    "\n",
    "outperforms all the ReLU variants in their experiment (http://goo.gl/USdl2P7). It has a few major differences compared to ReLU:\n",
    "- It takes on negative values. So the average output is closer to 0, which helps reducing vanishing gradients problem. The limit for $z\\to-\\infty$ is $-\\alpha$: a parameter that can be set or learned.\n",
    "- No dying ELUs because the gradient never vanishes.\n",
    "- Faster gradient descent because the gradient is smooth and hence jumps not as much back and forth.\n",
    "\n",
    "However, it is also more demanding to compute (due to the exponential for $z<0$). During training, this time loss is overcompensated by the faster convergence (less steps). Still, ELUs do lead to slower inference.<br><br>\n",
    "As for TensorFlow, a function for ELU is readily provided but leaky ReLUs need to be defined by the user, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"hidden1/Elu:0\", shape=(?, 4), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/Maximum:0' shape=(?, 4) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # placeholder for features\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")\n",
    "print(hidden1)\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # placeholder for features\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under certain conditions, the **Scaled Exponential Linear Unit** (**SELU**) can outperform the ELU activation function. The SELU was introduced in 2017 by Günter Klambauer, Thomas Unterthiner, and Andreas Mayr (see link above).\n",
    "### Batch Normalization\n",
    "page 282<br>\n",
    "By the choice of the activation function (e.g. ELU) and the random initialization (e.g. He initialization) one can usually avoid the vanishing / exploding gradients problem at the very beginning of training. But it might come back during training! The cause is still that the magnitude of the inputs and outputs may change drastically from the top to the bottom layers. Sergey Ioffe and Christian Szegedy addressed this problem in their 2015 publication (see link above).<br>\n",
    "Their technique, **Batch Normalization (BN)**, applies operations before and after the activation function: After the layer has computed weighted sums, these results are zero-centered (subtract the mean) and normalized (divide by the standard deviation) before being sent to the activation function. Then, the activation function's result is multiplied by a scalar factro $\\gamma$ and added to scalar summand $\\beta$, both of which are learnable parameters. So this lets the algorithm learn the optimal scale and mean of the inputs for each layer. The mean and standard deviation of the first step are calculated from the current mini-batch, hence the name. Equation 11-3 in the book shows the details:\n",
    "- $\\mu_B=\\frac{1}{m_B}\\sum_{i=1}^{m_B}\\bar{x}^{(i)}$,\n",
    "- $\\sigma_B^2=\\frac{1}{m_B}\\sum_{i=1}^{m_B}\\left(\\bar{x}^{(i)}-\\mu_B\\right)^2$,\n",
    "- $x'^{(i)}=\\frac{x^{(i)}-\\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}$,\n",
    "- $z^{(i)}=\\gamma x'^{(i)}+\\beta$,\n",
    "\n",
    "where $\\mu_B$ is the mean over the mini-batch B, $\\sigma_B$ is the according standard deviation, $m_B$ is the number of instances in the mini-batch, $x'^{(i)}$ is the zero-centered and normalized input, $\\gamma$ is the scaling parameter of the layer, $\\beta$ is the shifting parameter (offset) of the layer, $\\epsilon$ is a tiny number to avoid division by zero (typically $10^{-5}$, also called *smoothing term*), and $z^{(i)}$ is the output of the BN operation: it is a scaled and shifted version of the inputs.<br>\n",
    "During inference for a new instance, there is no mini-batch to take the mean and standard deviation from. So the values of the entire training set are used instead. They can be computed efficiently during training via a moving average. So in total, each batch normalization operation produces 4 learned parameters during training: $\\beta$ (offset of output), $\\gamma$ (scale of output), $\\mu$ (mean / offset of input), and $\\sigma$ (standard deviation / scale of input).<br>\n",
    "Ioffe and Szegedy demonstrated that batch normalization improved all networks they experimented with. The vanishing gradients problem was so strongly decreased that they could employ saturating activation functions like tanh and sigmoid. Weight initialization was also less relevant. Much larger learning rates were possible. They note \"Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. [...] Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.\" Finally, like a gift that keeps giving, Batch Normalization regularizes the algorithm.<br>\n",
    "Nevertheless, BN still adds a little complexity. While the extra time for additional computations during training is overcompensated by larger convergence, it is not compensated during inference. So if predictions need to be very fast, using ELU activation together with He initialization might be more performant.<br><br>\n",
    "**General note**<br>\n",
    "You may find that training is rather slow at first while Gradient Descent is searching for the optimal scales and offsets for each layer, but it accelerates once it has found resonably good values.<br><br>\n",
    "**Implementing Batch Normalization with TensorFlow**<br>\n",
    "TensorFlow provides \"tf.nn.batch_normalization()\", which takes care of centering and scaling the input but does not take care of the running averages of $\\mu$ and $\\sigma$ for inference and also not for the scale $\\gamma$ and offset $\\beta$. They can be computed via own code but it is much more convenient to use `tf.layer.batch_normaliztaion()` instead. It takes care of everything. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"training:0\", shape=(), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'batch_normalization_3/batchnorm/add_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = 28 * 28 # input (probably MNIST)\n",
    "n_hidden1 = 300    # neurons in hidden layer 1\n",
    "n_hidden2 = 100    # neurons in hidden layer 2\n",
    "n_outputs = 10     # neurons in output layer\n",
    "reset_graph()      # always a good idea\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")         # input placeholder\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\") # training node\n",
    "print(training)    # see remark in markdown below\n",
    "# first layer with batch normalization and elu activation function\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")                  # hidden layer 1\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training,          # bn1\n",
    "                                    momentum=0.9)\n",
    "# \"bn1\" learns the (moving) mean and the standard deviation of the input as well as the ...\n",
    "# ... offset and the  scale of the output; by passing \"bn1 to the activation function, ...\n",
    "# ... \\gamma (scale) and beta (offset) are automatically taken into account\n",
    "bn1_act = tf.nn.elu(bn1) # output of first hidden layer: \\beta and \\gamma are handled ...\n",
    "                         # ... automatically by bn1 (tf.layers.batch_normalization())\n",
    "# second layer with batch normalization and elu activation function (in principle as above)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we use the `training` placeholder is to tell `tf.layer.batch_normalization()` whether it should use the current mini-batch's mean and standard deviation (for training) or the values from the entire training set (for inference). The fully connected layers `tf.layers.dense()` are not given an activation function because activation only takes place after the batch normalization operation.<br>\n",
    "The `momentum` parameter is used to compute the moving average via an *exponential decay*:<br><br>\n",
    "$$v'\\leftarrow v'\\times{\\rm momentum}+v(1- {\\rm momentum})\\,,$$\n",
    "<br>\n",
    "where $v$ corresponds to the current mini-batch and $v'$ to the previous value of the moving average. Typically, $v$ is larger the larger the training set is (or the smaller the mini-batches are; previous mini-batches shall not be forgotten even if the training set is huge), e.g., $v=0.9,\\,0.99$ or 0.999.<br>\n",
    "The repetitive code above can be simplified using Python's `partial()` function, as shown below.<br><br>\n",
    "Note: Python's partial function allows the user to fix variables of a function. For example, it can set $y=2$ for the function f(x,y), thus making it a function g(x)=f(x,2) with simpler syntax (see https://www.learnpython.org/en/Partial_functions for a tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'batch_normalization_3/batchnorm/add_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")         # these two lines are taken ...\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training') # ... from the github link above\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,  # use python's partial function to fix ...\n",
    "                              training=training,momentum=0.9) # ... the training and momentum arguments\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)                            # input (\"hidden1\") still needs to be specified\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)                            # input (\"hidden2\") still needs to be specified\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)                # input (\"logits_before_bn\") still needs to be specified\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small example it may not look like a big improvement. But when a larger network needs to be defined, this can be very handy!<br>\n",
    "The rest of the construction phase remains unchanged (define a cost function, create an optimizer, let it minimize the cost function, define an evaluation score, e.g. accuracy, create a saver, and so on). Execution is also very similar. Only the `training` placeholder needs to be set to `True` [`False`] during training [inference] and the running averages $\\mu$ (centering a new instance) and $\\sigma$ (scaling a new instance) need to be calculated. Setting the `training` placeholder to `True` during training is done *manually*. TensorFlow keeps the to-be-updated running averages in its `UPDATE_OPS` collection. We just need to retrieve this collection and tell TensorFlow to actully update them at each training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9042\n",
      "5 Validation accuracy: 0.9572\n",
      "10 Validation accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "### book, github, and own ideas\n",
    "reset_graph()                                                            # always a good idea\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")         # placeholder features\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")                     # placeholder for labels\n",
    "# function for batch normalization\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "batch_norm_momentum = 0.9\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=batch_norm_momentum)\n",
    "# function for fully conneced layers that use He initialization\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "my_dense_layer = partial(tf.layers.dense, kernel_initializer=he_init)\n",
    "# actual network with two hidden layers; both use python's partial function to quickly build batch-normalized layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # 2 hidden layers and an output layer, all of them fully connected and with batch normalization\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "# cost function node\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "# learning schedule\n",
    "learning_rate = 0.01\n",
    "n_epochs = 11\n",
    "batch_size = 200\n",
    "# training node\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# evaluation node\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "#   accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "# initializer and saver nodes\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# make training, testing, and validation sets for the MNIST dataset (note that the validation set is intended to ...\n",
    "# ... avoid overfitting, e.g., via early stopping; and the test set is to evaluate the performance of the final model\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "# make randomized batches of the specified size\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "### book\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)                # get the quantities that shall be ...\n",
    "with tf.Session() as sess:                                                   # ... updated (mean and standard dev.)\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # book code deprecated - use github code\n",
    "            sess.run([training_op, extra_update_ops],                        # amongst many things, the mean and ...\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})     # ... the standard deviation get updated\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        # own if-statement to reduce output (n_epochs has been increased)\n",
    "        if epoch % 5 is 0:\n",
    "            print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./tf_logs/11_Training/Batch_Norm/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "page 286<br>\n",
    "The exploding gradients problem occurs mainly in recurrent neural networks. One technique to avoid this issue is to simply clip the gradients such that they do not exceed certain values, e.g., they may be clipped to the interval $[-c, +c]$, where $c>0$. This is called **Gradient Clipping** (see link above). Nowadays, people prefer Batch Normalization but is still useful to know about Gradien Clipping.<br>\n",
    "TensorFlow optimizer's `minimize()` function calculates and applies the gradients automatically. For gradient clipping, this must be disentangled and controlled. An example is shown below. It uses the optimizer's `compute_gradients()` method, a function that clips the gradients, and the optimizer's `apply_gradients()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.288\n",
      "1 Validation accuracy: 0.7938\n",
      "2 Validation accuracy: 0.8796\n",
      "3 Validation accuracy: 0.9058\n",
      "4 Validation accuracy: 0.9164\n",
      "5 Validation accuracy: 0.9218\n",
      "6 Validation accuracy: 0.9292\n",
      "7 Validation accuracy: 0.9358\n",
      "8 Validation accuracy: 0.938\n",
      "9 Validation accuracy: 0.9416\n",
      "10 Validation accuracy: 0.9456\n"
     ]
    }
   ],
   "source": [
    "### github\n",
    "reset_graph() # always a good idea\n",
    "# mnist and dnn architecture\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# deep neural network with 5 hidden layers to produce large gradients\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "# cost function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "### book and own code\n",
    "# training with clipped gradients\n",
    "with tf.name_scope(\"train\"):\n",
    "    threshold = 1.0                                              # clip gradient to interval [-1, +1]\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) # define an optimizer\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)           # get the gradents\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars] # loop ...\n",
    "# ... through the gradients and clip them with tensorflow's \"clip_by_value\" method\n",
    "training_op = optimizer.apply_gradients(capped_gvs)          # apply the clipped gradients\n",
    "### github\n",
    "# evaluation node\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "# initializer and saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# schedule (learning rate already defined a few lines above)\n",
    "n_epochs = 11\n",
    "batch_size = 200\n",
    "# run the session\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./tf_logs/11_Training/Clipping/my_clipped_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "page 287<br>\n",
    "Training model with many parameters costs a lot of compute. So it is generally a good idea to reuse a model that has been very well trained on a similar task. Specifically, one can reuse the lower layer and only retrain the top layers. This is called **transfer learning**. A modification of the top layers is also possible. The idea behind is that lower level recognize low-level features. Those will usually be shared by very similar tasks. Only the higher level features really depend on the very specific task and those require retraining. This approach will not only be much quicker but it will also require much less data, since the lower layers do not require training anymore.<br><br>\n",
    "**General note**<br>\n",
    "If the input pictures of your new task don't have the same size as the ones used in the original task, you will have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will only work well if the inputs have similar low-level features.\n",
    "### Reusing a TensorFlow Model\n",
    "page 287<br>\n",
    "If the original model was trained (and saved) with TensorFlow, restoration of the entire model should be straightforward:\n",
    "- Use `import_meta_graph()` to import the operations into the default graph.\n",
    "- This will return an object (a \"saver\", yes) that can be used to import the model's state.\n",
    "- To obtain the operations and tensors needed for training, one can use the graph's `get_operation_by_name()` and `get_tensor_by_name()` methods. The name of a tensor is the name of the operation that outputs it followed by `:0` (or `:i` if it is the i+1-th output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important: 'filename'.ckpt.META!\n",
      "\n",
      "Tensor(\"X:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"y:0\", dtype=int32)\n",
      "Tensor(\"eval/accuracy:0\", shape=(), dtype=float32)\n",
      "Tensor(\"loss/loss:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "print(\"Important: 'filename'.ckpt.META!\\n\")\n",
    "# import the model created for batch normalization\n",
    "saver = tf.train.import_meta_graph(\"./tf_logs/11_Training/Batch_Norm/my_model_final.ckpt.meta\")\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "print(X)\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "print(y)\n",
    "Mean = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "print(Mean)\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the pretrained model is not well documented, showing all its operations via the graph's `get_operations()` method is one option ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "training/input\n",
      "training\n",
      "hidden1/kernel/Initializer/truncated_normal/shape\n",
      "hidden1/kernel/Initializer/truncated_normal/mean\n",
      "hidden1/kernel/Initializer/truncated_normal/stddev\n",
      "hidden1/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden1/kernel/Initializer/truncated_normal/mul\n",
      "hidden1/kernel/Initializer/truncated_normal\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "batch_normalization/gamma/Initializer/ones\n",
      "batch_normalization/gamma\n",
      "batch_normalization/gamma/Assign\n",
      "batch_normalization/gamma/read\n",
      "batch_normalization/beta/Initializer/zeros\n",
      "batch_normalization/beta\n",
      "batch_normalization/beta/Assign\n",
      "batch_normalization/beta/read\n",
      "batch_normalization/moving_mean/Initializer/zeros\n",
      "batch_normalization/moving_mean\n",
      "batch_normalization/moving_mean/Assign\n",
      "batch_normalization/moving_mean/read\n",
      "batch_normalization/moving_variance/Initializer/ones\n",
      "batch_normalization/moving_variance\n",
      "batch_normalization/moving_variance/Assign\n",
      "batch_normalization/moving_variance/read\n",
      "dnn/batch_normalization/moments/mean/reduction_indices\n",
      "dnn/batch_normalization/moments/mean\n",
      "dnn/batch_normalization/moments/StopGradient\n",
      "dnn/batch_normalization/moments/SquaredDifference\n",
      "dnn/batch_normalization/moments/variance/reduction_indices\n",
      "dnn/batch_normalization/moments/variance\n",
      "dnn/batch_normalization/moments/Squeeze\n",
      "dnn/batch_normalization/moments/Squeeze_1\n",
      "dnn/batch_normalization/cond/Switch\n",
      "dnn/batch_normalization/cond/switch_t\n",
      "dnn/batch_normalization/cond/switch_f\n",
      "dnn/batch_normalization/cond/pred_id\n",
      "dnn/batch_normalization/cond/Switch_1\n",
      "dnn/batch_normalization/cond/Switch_2\n",
      "dnn/batch_normalization/cond/Merge\n",
      "dnn/batch_normalization/cond_1/Switch\n",
      "dnn/batch_normalization/cond_1/switch_t\n",
      "dnn/batch_normalization/cond_1/switch_f\n",
      "dnn/batch_normalization/cond_1/pred_id\n",
      "dnn/batch_normalization/cond_1/Switch_1\n",
      "dnn/batch_normalization/cond_1/Switch_2\n",
      "dnn/batch_normalization/cond_1/Merge\n",
      "dnn/batch_normalization/cond_2/Switch\n",
      "dnn/batch_normalization/cond_2/switch_t\n",
      "dnn/batch_normalization/cond_2/switch_f\n",
      "dnn/batch_normalization/cond_2/pred_id\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg/decay\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg/sub\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg/sub/Switch\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg/mul\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg\n",
      "dnn/batch_normalization/cond_2/AssignMovingAvg/Switch\n",
      "dnn/batch_normalization/cond_2/Switch_1\n",
      "dnn/batch_normalization/cond_2/Merge\n",
      "dnn/batch_normalization/cond_3/Switch\n",
      "dnn/batch_normalization/cond_3/switch_t\n",
      "dnn/batch_normalization/cond_3/switch_f\n",
      "dnn/batch_normalization/cond_3/pred_id\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg/decay\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg/sub\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg/sub/Switch\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg/sub/Switch_1\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg/mul\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg\n",
      "dnn/batch_normalization/cond_3/AssignMovingAvg/Switch\n",
      "dnn/batch_normalization/cond_3/Switch_1\n",
      "dnn/batch_normalization/cond_3/Merge\n",
      "dnn/batch_normalization/batchnorm/add/y\n",
      "dnn/batch_normalization/batchnorm/add\n",
      "dnn/batch_normalization/batchnorm/Rsqrt\n",
      "dnn/batch_normalization/batchnorm/mul\n",
      "dnn/batch_normalization/batchnorm/mul_1\n",
      "dnn/batch_normalization/batchnorm/mul_2\n",
      "dnn/batch_normalization/batchnorm/sub\n",
      "dnn/batch_normalization/batchnorm/add_1\n",
      "dnn/Elu\n",
      "hidden2/kernel/Initializer/truncated_normal/shape\n",
      "hidden2/kernel/Initializer/truncated_normal/mean\n",
      "hidden2/kernel/Initializer/truncated_normal/stddev\n",
      "hidden2/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "hidden2/kernel/Initializer/truncated_normal/mul\n",
      "hidden2/kernel/Initializer/truncated_normal\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "batch_normalization_1/gamma/Initializer/ones\n",
      "batch_normalization_1/gamma\n",
      "batch_normalization_1/gamma/Assign\n",
      "batch_normalization_1/gamma/read\n",
      "batch_normalization_1/beta/Initializer/zeros\n",
      "batch_normalization_1/beta\n",
      "batch_normalization_1/beta/Assign\n",
      "batch_normalization_1/beta/read\n",
      "batch_normalization_1/moving_mean/Initializer/zeros\n",
      "batch_normalization_1/moving_mean\n",
      "batch_normalization_1/moving_mean/Assign\n",
      "batch_normalization_1/moving_mean/read\n",
      "batch_normalization_1/moving_variance/Initializer/ones\n",
      "batch_normalization_1/moving_variance\n",
      "batch_normalization_1/moving_variance/Assign\n",
      "batch_normalization_1/moving_variance/read\n",
      "dnn/batch_normalization_2/moments/mean/reduction_indices\n",
      "dnn/batch_normalization_2/moments/mean\n",
      "dnn/batch_normalization_2/moments/StopGradient\n",
      "dnn/batch_normalization_2/moments/SquaredDifference\n",
      "dnn/batch_normalization_2/moments/variance/reduction_indices\n",
      "dnn/batch_normalization_2/moments/variance\n",
      "dnn/batch_normalization_2/moments/Squeeze\n",
      "dnn/batch_normalization_2/moments/Squeeze_1\n",
      "dnn/batch_normalization_2/cond/Switch\n",
      "dnn/batch_normalization_2/cond/switch_t\n",
      "dnn/batch_normalization_2/cond/switch_f\n",
      "dnn/batch_normalization_2/cond/pred_id\n",
      "dnn/batch_normalization_2/cond/Switch_1\n",
      "dnn/batch_normalization_2/cond/Switch_2\n",
      "dnn/batch_normalization_2/cond/Merge\n",
      "dnn/batch_normalization_2/cond_1/Switch\n",
      "dnn/batch_normalization_2/cond_1/switch_t\n",
      "dnn/batch_normalization_2/cond_1/switch_f\n",
      "dnn/batch_normalization_2/cond_1/pred_id\n",
      "dnn/batch_normalization_2/cond_1/Switch_1\n",
      "dnn/batch_normalization_2/cond_1/Switch_2\n",
      "dnn/batch_normalization_2/cond_1/Merge\n",
      "dnn/batch_normalization_2/cond_2/Switch\n",
      "dnn/batch_normalization_2/cond_2/switch_t\n",
      "dnn/batch_normalization_2/cond_2/switch_f\n",
      "dnn/batch_normalization_2/cond_2/pred_id\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg/decay\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg/sub\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg/sub/Switch\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg/sub/Switch_1\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg/mul\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg\n",
      "dnn/batch_normalization_2/cond_2/AssignMovingAvg/Switch\n",
      "dnn/batch_normalization_2/cond_2/Switch_1\n",
      "dnn/batch_normalization_2/cond_2/Merge\n",
      "dnn/batch_normalization_2/cond_3/Switch\n",
      "dnn/batch_normalization_2/cond_3/switch_t\n",
      "dnn/batch_normalization_2/cond_3/switch_f\n",
      "dnn/batch_normalization_2/cond_3/pred_id\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg/decay\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg/sub\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg/sub/Switch\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg/sub/Switch_1\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg/mul\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg\n",
      "dnn/batch_normalization_2/cond_3/AssignMovingAvg/Switch\n",
      "dnn/batch_normalization_2/cond_3/Switch_1\n",
      "dnn/batch_normalization_2/cond_3/Merge\n",
      "dnn/batch_normalization_2/batchnorm/add/y\n",
      "dnn/batch_normalization_2/batchnorm/add\n",
      "dnn/batch_normalization_2/batchnorm/Rsqrt\n",
      "dnn/batch_normalization_2/batchnorm/mul\n",
      "dnn/batch_normalization_2/batchnorm/mul_1\n",
      "dnn/batch_normalization_2/batchnorm/mul_2\n",
      "dnn/batch_normalization_2/batchnorm/sub\n",
      "dnn/batch_normalization_2/batchnorm/add_1\n",
      "dnn/Elu_1\n",
      "outputs/kernel/Initializer/truncated_normal/shape\n",
      "outputs/kernel/Initializer/truncated_normal/mean\n",
      "outputs/kernel/Initializer/truncated_normal/stddev\n",
      "outputs/kernel/Initializer/truncated_normal/TruncatedNormal\n",
      "outputs/kernel/Initializer/truncated_normal/mul\n",
      "outputs/kernel/Initializer/truncated_normal\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "batch_normalization_2/gamma/Initializer/ones\n",
      "batch_normalization_2/gamma\n",
      "batch_normalization_2/gamma/Assign\n",
      "batch_normalization_2/gamma/read\n",
      "batch_normalization_2/beta/Initializer/zeros\n",
      "batch_normalization_2/beta\n",
      "batch_normalization_2/beta/Assign\n",
      "batch_normalization_2/beta/read\n",
      "batch_normalization_2/moving_mean/Initializer/zeros\n",
      "batch_normalization_2/moving_mean\n",
      "batch_normalization_2/moving_mean/Assign\n",
      "batch_normalization_2/moving_mean/read\n",
      "batch_normalization_2/moving_variance/Initializer/ones\n",
      "batch_normalization_2/moving_variance\n",
      "batch_normalization_2/moving_variance/Assign\n",
      "batch_normalization_2/moving_variance/read\n",
      "dnn/batch_normalization_3/moments/mean/reduction_indices\n",
      "dnn/batch_normalization_3/moments/mean\n",
      "dnn/batch_normalization_3/moments/StopGradient\n",
      "dnn/batch_normalization_3/moments/SquaredDifference\n",
      "dnn/batch_normalization_3/moments/variance/reduction_indices\n",
      "dnn/batch_normalization_3/moments/variance\n",
      "dnn/batch_normalization_3/moments/Squeeze\n",
      "dnn/batch_normalization_3/moments/Squeeze_1\n",
      "dnn/batch_normalization_3/cond/Switch\n",
      "dnn/batch_normalization_3/cond/switch_t\n",
      "dnn/batch_normalization_3/cond/switch_f\n",
      "dnn/batch_normalization_3/cond/pred_id\n",
      "dnn/batch_normalization_3/cond/Switch_1\n",
      "dnn/batch_normalization_3/cond/Switch_2\n",
      "dnn/batch_normalization_3/cond/Merge\n",
      "dnn/batch_normalization_3/cond_1/Switch\n",
      "dnn/batch_normalization_3/cond_1/switch_t\n",
      "dnn/batch_normalization_3/cond_1/switch_f\n",
      "dnn/batch_normalization_3/cond_1/pred_id\n",
      "dnn/batch_normalization_3/cond_1/Switch_1\n",
      "dnn/batch_normalization_3/cond_1/Switch_2\n",
      "dnn/batch_normalization_3/cond_1/Merge\n",
      "dnn/batch_normalization_3/cond_2/Switch\n",
      "dnn/batch_normalization_3/cond_2/switch_t\n",
      "dnn/batch_normalization_3/cond_2/switch_f\n",
      "dnn/batch_normalization_3/cond_2/pred_id\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg/decay\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg/sub\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg/sub/Switch\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg/sub/Switch_1\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg/mul\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg\n",
      "dnn/batch_normalization_3/cond_2/AssignMovingAvg/Switch\n",
      "dnn/batch_normalization_3/cond_2/Switch_1\n",
      "dnn/batch_normalization_3/cond_2/Merge\n",
      "dnn/batch_normalization_3/cond_3/Switch\n",
      "dnn/batch_normalization_3/cond_3/switch_t\n",
      "dnn/batch_normalization_3/cond_3/switch_f\n",
      "dnn/batch_normalization_3/cond_3/pred_id\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg/decay\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg/sub\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg/sub/Switch\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg/sub/Switch_1\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg/mul\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg\n",
      "dnn/batch_normalization_3/cond_3/AssignMovingAvg/Switch\n",
      "dnn/batch_normalization_3/cond_3/Switch_1\n",
      "dnn/batch_normalization_3/cond_3/Merge\n",
      "dnn/batch_normalization_3/batchnorm/add/y\n",
      "dnn/batch_normalization_3/batchnorm/add\n",
      "dnn/batch_normalization_3/batchnorm/Rsqrt\n",
      "dnn/batch_normalization_3/batchnorm/mul\n",
      "dnn/batch_normalization_3/batchnorm/mul_1\n",
      "dnn/batch_normalization_3/batchnorm/mul_2\n",
      "dnn/batch_normalization_3/batchnorm/sub\n",
      "dnn/batch_normalization_3/batchnorm/add_1\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "train/gradients/Shape\n",
      "train/gradients/grad_ys_0\n",
      "train/gradients/Fill\n",
      "train/gradients/loss/loss_grad/Reshape/shape\n",
      "train/gradients/loss/loss_grad/Reshape\n",
      "train/gradients/loss/loss_grad/Shape\n",
      "train/gradients/loss/loss_grad/Tile\n",
      "train/gradients/loss/loss_grad/Shape_1\n",
      "train/gradients/loss/loss_grad/Shape_2\n",
      "train/gradients/loss/loss_grad/Const\n",
      "train/gradients/loss/loss_grad/Prod\n",
      "train/gradients/loss/loss_grad/Const_1\n",
      "train/gradients/loss/loss_grad/Prod_1\n",
      "train/gradients/loss/loss_grad/Maximum/y\n",
      "train/gradients/loss/loss_grad/Maximum\n",
      "train/gradients/loss/loss_grad/floordiv\n",
      "train/gradients/loss/loss_grad/Cast\n",
      "train/gradients/loss/loss_grad/truediv\n",
      "train/gradients/zeros_like\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "train/gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/mul\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/mul_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Neg\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/sub_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_2_grad/Mul\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_2_grad/Mul_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_2_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_2_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_2_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_3/cond/Merge_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_3/cond/Merge_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/cond/Merge_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/cond/Merge_grad/tuple/control_dependency_1\n",
      "train/gradients/AddN\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_grad/Mul\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_grad/Mul_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/mul_grad/tuple/control_dependency_1\n",
      "train/gradients/Switch\n",
      "train/gradients/Shape_1\n",
      "train/gradients/zeros/Const\n",
      "train/gradients/zeros\n",
      "train/gradients/dnn/batch_normalization_3/cond/Switch_1_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/Rsqrt_grad/RsqrtGrad\n",
      "train/gradients/dnn/batch_normalization_3/moments/Squeeze_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/moments/Squeeze_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/batchnorm/add_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_3/cond_1/Merge_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_3/cond_1/Merge_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/cond_1/Merge_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/cond_1/Merge_grad/tuple/control_dependency_1\n",
      "train/gradients/Switch_1\n",
      "train/gradients/Shape_2\n",
      "train/gradients/zeros_1/Const\n",
      "train/gradients/zeros_1\n",
      "train/gradients/dnn/batch_normalization_3/cond_1/Switch_1_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_3/moments/Squeeze_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/moments/Squeeze_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Size\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/add\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/mod\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/range/start\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/range/delta\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/range\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Fill/value\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Fill\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/DynamicStitch\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Maximum/y\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Maximum\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/floordiv\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Tile\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Shape_2\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Shape_3\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Const\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Prod\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Const_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Prod_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Maximum_1/y\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Maximum_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/floordiv_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/Cast\n",
      "train/gradients/dnn/batch_normalization_3/moments/variance_grad/truediv\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/scalar\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/mul\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/sub\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/mul_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/Neg\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_3/moments/SquaredDifference_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Size\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/add\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/mod\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/range/start\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/range/delta\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/range\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Fill/value\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Fill\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/DynamicStitch\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Maximum/y\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Maximum\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/floordiv\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Tile\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Shape_2\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Shape_3\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Const\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Prod\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Const_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Prod_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Maximum_1/y\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Maximum_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/floordiv_1\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/Cast\n",
      "train/gradients/dnn/batch_normalization_3/moments/mean_grad/truediv\n",
      "train/gradients/AddN_1\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "train/gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/Elu_1_grad/EluGrad\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/mul\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/mul_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Neg\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/sub_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_2_grad/Mul\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_2_grad/Mul_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_2_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_2_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_2_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_2/cond/Merge_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_2/cond/Merge_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/cond/Merge_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/cond/Merge_grad/tuple/control_dependency_1\n",
      "train/gradients/AddN_2\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_grad/Mul\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_grad/Mul_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/mul_grad/tuple/control_dependency_1\n",
      "train/gradients/Switch_2\n",
      "train/gradients/Shape_3\n",
      "train/gradients/zeros_2/Const\n",
      "train/gradients/zeros_2\n",
      "train/gradients/dnn/batch_normalization_2/cond/Switch_1_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/Rsqrt_grad/RsqrtGrad\n",
      "train/gradients/dnn/batch_normalization_2/moments/Squeeze_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/moments/Squeeze_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/batchnorm/add_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_2/cond_1/Merge_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_2/cond_1/Merge_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/cond_1/Merge_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/cond_1/Merge_grad/tuple/control_dependency_1\n",
      "train/gradients/Switch_3\n",
      "train/gradients/Shape_4\n",
      "train/gradients/zeros_3/Const\n",
      "train/gradients/zeros_3\n",
      "train/gradients/dnn/batch_normalization_2/cond_1/Switch_1_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization_2/moments/Squeeze_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/moments/Squeeze_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Size\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/add\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/mod\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/range/start\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/range/delta\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/range\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Fill/value\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Fill\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/DynamicStitch\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Maximum/y\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Maximum\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/floordiv\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Tile\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Shape_2\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Shape_3\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Const\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Prod\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Const_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Prod_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Maximum_1/y\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Maximum_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/floordiv_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/Cast\n",
      "train/gradients/dnn/batch_normalization_2/moments/variance_grad/truediv\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/scalar\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/mul\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/sub\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/mul_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Sum\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/Neg\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization_2/moments/SquaredDifference_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Shape\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Size\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/add\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/mod\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/range/start\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/range/delta\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/range\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Fill/value\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Fill\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/DynamicStitch\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Maximum/y\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Maximum\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/floordiv\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Tile\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Shape_2\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Shape_3\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Const\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Prod\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Const_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Prod_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Maximum_1/y\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Maximum_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/floordiv_1\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/Cast\n",
      "train/gradients/dnn/batch_normalization_2/moments/mean_grad/truediv\n",
      "train/gradients/AddN_3\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/Elu_grad/EluGrad\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/Sum\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/mul\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/Sum\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/mul_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_1_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Sum\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Neg\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/batchnorm/sub_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_2_grad/Mul\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_2_grad/Mul_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_2_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_2_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_2_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization/cond/Merge_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization/cond/Merge_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/cond/Merge_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/cond/Merge_grad/tuple/control_dependency_1\n",
      "train/gradients/AddN_4\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_grad/Mul\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_grad/Mul_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/batchnorm/mul_grad/tuple/control_dependency_1\n",
      "train/gradients/Switch_4\n",
      "train/gradients/Shape_5\n",
      "train/gradients/zeros_4/Const\n",
      "train/gradients/zeros_4\n",
      "train/gradients/dnn/batch_normalization/cond/Switch_1_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization/batchnorm/Rsqrt_grad/RsqrtGrad\n",
      "train/gradients/dnn/batch_normalization/moments/Squeeze_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/moments/Squeeze_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/Sum\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/batchnorm/add_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization/cond_1/Merge_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization/cond_1/Merge_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/cond_1/Merge_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/cond_1/Merge_grad/tuple/control_dependency_1\n",
      "train/gradients/Switch_5\n",
      "train/gradients/Shape_6\n",
      "train/gradients/zeros_5/Const\n",
      "train/gradients/zeros_5\n",
      "train/gradients/dnn/batch_normalization/cond_1/Switch_1_grad/cond_grad\n",
      "train/gradients/dnn/batch_normalization/moments/Squeeze_1_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/moments/Squeeze_1_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Size\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/add\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/mod\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/range/start\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/range/delta\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/range\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Fill/value\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Fill\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/DynamicStitch\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Maximum/y\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Maximum\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/floordiv\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Tile\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Shape_2\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Shape_3\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Const\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Prod\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Const_1\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Prod_1\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Maximum_1/y\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Maximum_1\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/floordiv_1\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/Cast\n",
      "train/gradients/dnn/batch_normalization/moments/variance_grad/truediv\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/BroadcastGradientArgs\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/scalar\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/mul\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/sub\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/mul_1\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Sum\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Sum_1\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Reshape_1\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/Neg\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/tuple/group_deps\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/tuple/control_dependency\n",
      "train/gradients/dnn/batch_normalization/moments/SquaredDifference_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Shape\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Size\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/add\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/mod\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Shape_1\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/range/start\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/range/delta\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/range\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Fill/value\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Fill\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/DynamicStitch\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Maximum/y\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Maximum\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/floordiv\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Reshape\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Tile\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Shape_2\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Shape_3\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Const\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Prod\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Const_1\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Prod_1\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Maximum_1/y\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Maximum_1\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/floordiv_1\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/Cast\n",
      "train/gradients/dnn/batch_normalization/moments/mean_grad/truediv\n",
      "train/gradients/AddN_5\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "train/gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "train/gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "train/GradientDescent/learning_rate\n",
      "train/GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_batch_normalization/gamma/ApplyGradientDescent\n",
      "train/GradientDescent/update_batch_normalization/beta/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_batch_normalization_1/gamma/ApplyGradientDescent\n",
      "train/GradientDescent/update_batch_normalization_1/beta/ApplyGradientDescent\n",
      "train/GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "train/GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "train/GradientDescent/update_batch_normalization_2/gamma/ApplyGradientDescent\n",
      "train/GradientDescent/update_batch_normalization_2/beta/ApplyGradientDescent\n",
      "train/GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/RestoreV2_12/tensor_names\n",
      "save/RestoreV2_12/shape_and_slices\n",
      "save/RestoreV2_12\n",
      "save/Assign_12\n",
      "save/RestoreV2_13/tensor_names\n",
      "save/RestoreV2_13/shape_and_slices\n",
      "save/RestoreV2_13\n",
      "save/Assign_13\n",
      "save/RestoreV2_14/tensor_names\n",
      "save/RestoreV2_14/shape_and_slices\n",
      "save/RestoreV2_14\n",
      "save/Assign_14\n",
      "save/RestoreV2_15/tensor_names\n",
      "save/RestoreV2_15/shape_and_slices\n",
      "save/RestoreV2_15\n",
      "save/Assign_15\n",
      "save/RestoreV2_16/tensor_names\n",
      "save/RestoreV2_16/shape_and_slices\n",
      "save/RestoreV2_16\n",
      "save/Assign_16\n",
      "save/RestoreV2_17/tensor_names\n",
      "save/RestoreV2_17/shape_and_slices\n",
      "save/RestoreV2_17\n",
      "save/Assign_17\n",
      "save/restore_all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "862"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_count = 0\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)\n",
    "    op_count += 1\n",
    "op_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and visualizing the graph in TensorBoard is another. For the latter, run a short session that uses a FileWriter to write the graph (due to the \"saver\" above, this should now be the default graph) to an `events` file. Then start TensorBoard and visualize the newly written file. Here, navigate to the folder containing the folder \"tf_logs\" and run \"tensorboard --logdir tf_logs\".<br>\n",
    "**Note:** To show that indeed a *new* `events` file for graph visualization is been created, that file is written to different folder (\"Batch_Norm\") below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. You can now visualize the final graph for Batch Normalization in TensorBoard.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.summary.FileWriter(\"./tf_logs/11_Training/Batch_Norm\", tf.get_default_graph())\n",
    "print(\"Done. You can now visualize the final graph for Batch Normalization in TensorBoard.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the creator of a model, one shall make it easy for potential reusers to understand the graph by giving operations intuitive names and documenting them. Additionally, one can make dedicated collectoins for the most important operations, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if necessary at some point, investigate how a \"collection\" can be saved and restored\n",
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops1\", op)\n",
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the original graph restored (not in the previous cell but in the one above it), it is possible to retrain the model with Batch Normalization. However, the input placeholders `X` and `y` as well as the `accuracy` and the `training_op` need to be restored. The placeholders are needed for feeding new input, the training operation is required to continue training, and the accuracy is needed to assess the training progress. This is intuitive. However, restoring these variables and operations would be **not so straightforward** if we did not have the original code further above. (Re-)Creating the graph for inspection and listing all the operations - both have been demonstrated above - might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Batch_Norm/my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9704\n",
      "1 Validation accuracy: 0.973\n",
      "2 Validation accuracy: 0.9738\n",
      "3 Validation accuracy: 0.9742\n",
      "4 Validation accuracy: 0.9752\n",
      "5 Validation accuracy: 0.9728\n",
      "6 Validation accuracy: 0.976\n",
      "7 Validation accuracy: 0.9764\n",
      "8 Validation accuracy: 0.9766\n",
      "9 Validation accuracy: 0.9788\n",
      "10 Validation accuracy: 0.9776\n"
     ]
    }
   ],
   "source": [
    "# combination of book, own code, and the github link above\n",
    "reset_graph()\n",
    "saver = tf.train.import_meta_graph(\"./tf_logs/11_Training/Batch_Norm/my_model_final.ckpt.meta\")\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"train/GradientDescent\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tf_logs/11_Training/Batch_Norm/my_model_final.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    save_path = saver.save(sess, \"./tf_logs/11_Training/Batch_Norm/my_retrained_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original python code is available, one can also build the model from scratch instead of using `import_meta_graph()`. Even if `import_meta_graph()` is used to import the model, one may still neglect the upper layers, as shown in Figure 11-4 of the book. Additional or replacement layers can still be added on top of the remaining pretrained layers (see the github link above for details). However, two savers will be required in that case. Otherwise, TensorFlow will complain that the graphs do not match.<br><br>\n",
    "**Tip or Suggestion**<br>\n",
    "The more similar the tasks are, the more layers you want to reuse (starging with the lower layers). For very similar tasks, you can try keeping all the hidden layers and just replace the output layer.\n",
    "### Reusing Models from Other Frameworks\n",
    "page 289<br>\n",
    "If the model to be reused was trained on another framework, the conversion can be quite an ordeal. The parameters need to be loaded and then assigned to TensorFlow in a quite manual way. As an example, the following code shows the assignment of weights and biases, assuming that they have already been loaded from another framework. The tricky part is to have tensorflow initialize the model parameters with those weights and biases. This is done by feeding the weights to the appropriate assignment nodes via a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61.  83. 105.]]\n"
     ]
    }
   ],
   "source": [
    "# everything from github, see link above\n",
    "reset_graph()                             # always a good idea\n",
    "n_inputs = 2                              # number of inputs\n",
    "n_hidden1 = 3                             # number of neurons in the first (and here the only) hidden layer\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # load the biases from the other framework\n",
    "# feature placeholder\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "# first (and here only) hidden layer\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] here, the rest of the network shall be built\n",
    "# get a handle on TensorFlow's assignment nodes for the weights and biases of the layer \"hidden1\"\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "# initial weights and biases\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "# global initializer\n",
    "init = tf.global_variables_initializer()\n",
    "# run a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    # initialize global variables but assign the imported weights and biases of the first hidden layer; this is ...\n",
    "    # ... achieved by feeding the imported weights to TensorFlow's assignment nodes (see above) through a dicionary\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # let's produce a small result: based on the 2 inputs, the 3 neurons proudce 3 outputs (using 2x3 imported ...\n",
    "    # ... weights and 3 biases)\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Lower Layers\n",
    "page 290<br>\n",
    "When a network that had been trained on a similar task has been imported (in some way) in order to be adapted to the actual task, it makes sense to keep the lower layers as they are, i.e., to **freeze the lower layers**. Then, training of the upper layer will be much quicker. The following code shows how to achieve that with the trained and saved model for *Gradient Clipping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla DNN defined and ready for training on MNIST.\n"
     ]
    }
   ],
   "source": [
    "### everything github\n",
    "reset_graph()       # always a good idea\n",
    "# architecure\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300     # reused\n",
    "n_hidden2 = 50      # reused\n",
    "n_hidden3 = 50      # reused\n",
    "n_hidden4 = 20      # new! (was 50 before)\n",
    "n_outputs = 10      # new! (input is now the 4th layer with 20 outuputs but has been the 5th layer with 50 outputs)\n",
    "# input\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# dnn (vanilla)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")            # reused ...\n",
    "    hidden1_stop = tf.stop_gradient(hidden1)                                                  # ... frozen\n",
    "    hidden2 = tf.layers.dense(hidden1_stop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused ...\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)                                                  # ... frozen\n",
    "    # the third layer is reused but produces output in a different format and thus required training (=> not frozen)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused but not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")      # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                              # new!\n",
    "# loss (vanilla - no gradient clipping)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "# evaluation (vanilla)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "# training (vanilla)\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "print(\"Vanilla DNN defined and ready for training on MNIST.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables that shall be reused are restored using a collection (see further above) and a `tf.train.saver` instance on which the `restore` method will be called during runtime. **Cleverly**, the collection is build using a **regular expression**, thus increasing the probability that variables related to hidden layer 1-3 are colllected (and then restored). See\n",
    "- https://docs.python.org/3/library/re.html#regular-expression-syntax and\n",
    "- the Jupyter tutorial notebook \"LearningPythonTutorial.ipynb\" for details.\n",
    "\n",
    "Since no clipping but only freezing is done here, we save the \".ckpt\" file (for the session) and an \"events\" file (for the graph) in the new folder \"freezing_but_clipping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Clipping/my_clipped_model_final.ckpt\n",
      "0 Validation accuracy: 0.8938\n",
      "1 Validation accuracy: 0.9214\n",
      "2 Validation accuracy: 0.9312\n",
      "3 Validation accuracy: 0.9362\n",
      "4 Validation accuracy: 0.9366\n",
      "5 Validation accuracy: 0.9356\n",
      "6 Validation accuracy: 0.9398\n",
      "7 Validation accuracy: 0.9388\n",
      "8 Validation accuracy: 0.9398\n",
      "9 Validation accuracy: 0.94\n",
      "10 Validation accuracy: 0.9418\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars)                                         # to restore layers 1-3\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # restore variables from the collection above\n",
    "    restore_saver.restore(sess, \"./tf_logs/11_Training/Clipping/my_clipped_model_final.ckpt\")\n",
    "    # resume training\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    tf.summary.FileWriter(\"./tf_logs/11_Training/freezing_but_clipping\", tf.get_default_graph())\n",
    "    save_path = saver.save(sess, \"./tf_logs/11_Training/freezing_but_clipping/frozen.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a bit confusing, the graph clearly indicates that only the 4-th hidden layer and the output layer are being trained. This is achieved with the command `tf.stop.gradient(...)`, see further above.\n",
    "### Caching the Frozen Layers\n",
    "page 291<br>\n",
    "The frozen lower layers will always produce the same output for the same input because they are not being trained. So it makes sense to compute the output of the frozen layers for every instance only once, make batches out of these outputs, and feed these batches to the upper layers that are being trained. This will save a lot of time. Yet, it might require a lot of RAM because the output of the last frozen layer might be much larger than the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Clipping/my_clipped_model_final.ckpt\n",
      "0 Validation accuracy: 0.8976\n",
      "1 Validation accuracy: 0.9222\n",
      "2 Validation accuracy: 0.9316\n",
      "3 Validation accuracy: 0.9356\n",
      "4 Validation accuracy: 0.937\n",
      "5 Validation accuracy: 0.9374\n",
      "6 Validation accuracy: 0.9384\n",
      "7 Validation accuracy: 0.9386\n",
      "8 Validation accuracy: 0.939\n",
      "9 Validation accuracy: 0.941\n",
      "10 Validation accuracy: 0.9412\n"
     ]
    }
   ],
   "source": [
    "# wild mix of book and github\n",
    "n_batches = len(X_train) // batch_size\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # import the same model as above for \"Freezing the Lower Layers\"\n",
    "    restore_saver.restore(sess, \"./tf_logs/11_Training/Clipping/my_clipped_model_final.ckpt\")\n",
    "    # cache the output (both for training and for validation sets) of the lower layers that are not being trained\n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid})\n",
    "    # now loop through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # make batches out of the lower layers' output\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        # use these batches for training the non-cached layers\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "        # calculate and print the accuracy on the validation set\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking, Dropping, or Replacing the Upper Layers\n",
    "page 292<br>\n",
    "Most likely, the original output layer that has been trained on some task does not perform optimally on the new task. Often, the number of outputs does not even fit the new task. But also the layers just below the output layer usually require adjustment: they have adapted to high-level features that are relevant for the original task but they might not perform well on the new task. In contrast, low-level features are often common even among quite different tasks. So the lower layers can often be kept as in the original task but the upper layers - in particular the output layer - usually need adjustment.<br>\n",
    "But what layers shall be kept? First, freeze all layers but the last, training only the output layer. Then unfreeze the next layer, check the performance and continue unfreezing / retraining more and more layers. The more training data is available, the more layer can be unfrozen.<br>\n",
    "If performance does not improve enough and only little training data is available, try dropping the top hidden layer(s) (drop iteratively one top layer after another) altogether, freeze all lower ones, and train only the output layer. When a lot of training data is available, replacing the top layers or even adding more layers is also worth trying.\n",
    "### Model Zoos\n",
    "page 292<br>\n",
    "In order to reuse an existing model for a similar task, one first needs to have that model. Fortunately, there are many sources **model zoos** (a model zoo is a collection of models):\n",
    "- Your own collection. Establish and maintain it!\n",
    "- TensorFlow's own model zoo https://github.com/tensorflow/models.\n",
    "- Caffe's model zoo https://goo.gl/XI02X3, which can be converted to TensorFlow with Saumitro Dasgupta's converter https://github.com/ethereon/caffe-tensorflow.\n",
    "- OpenAI on github, includling Reinforcement Learning  https://github.com/germain-hug/Deep-RL-Keras.\n",
    "- Deep Reinforcement Learning with Keras https://modelzoo.co/model/deep-reinforcement-learning-for-keras.\n",
    "\n",
    "### Unsupervised Pretraining\n",
    "page 293<br>\n",
    "If a dataset has only a small fraction of labelled instances and no model for a similar task is available (thus excluding freezing all but the top layer of such similar but nonexistent model), then one can use feature detectors such as *Restricted Boltzmann Machines* or *Autoenconders* to pretrain the model layer by layer, starting from the bottom. Each hidden layer is trained on the output of the previous pretrained layer. All previous layers remain frozen during pretraining of the next layer. When all layers have been pretrained, one can fine tune the output and possibly some of the top hidden layers with the labelled data while keeping the lower layer frozen (see also Figure 11-5 in the book).<br>\n",
    "This is a long and tedious process but it was very common to train networks this way until batch normalization led to the breakthrough of backpropagation.\n",
    "### Pretraining on an Auxiliary Task\n",
    "page 294<br>\n",
    "Another option is to create a labelled dataset - with labels and dataset being different but qualitatively similar to the the actual task - and train a network on this dataset. Then, transfer learning can be used as described above.<br>\n",
    "Say you want to classifiy photos of your friends according to the names of your friends in the photo. Then you could download a lot of photos from, say, actors and train a network to tell whether two photos show the same person (after labelling it manually). This network must have learned a lot about detecting faces so its lower-level hidden layers should be useful for your actual task.\n",
    "## Faster Optimizers\n",
    "page 295<br>\n",
    "Training large DNNs can be painstakingly slow. So far, we have learned 4 techniques to make training faster:\n",
    "- Good choice of parameter initialization (Xavier or He),\n",
    "- good choice of activation function (ELU, for example),\n",
    "- batch normalization (noramlizing the output of a layer wrt. mean and standare deviation before sending it to the activation function, then learning the scale and offset after the activation function), and\n",
    "- reusing pretrained models (usually freezing the lower layers).\n",
    "\n",
    "Using a well suited optimizer is another technique that can speed up training tremendously. The most popular ones,\n",
    "- Momentum optimization,\n",
    "- Nesterov Accelerated Gradient,\n",
    "- AdaGrad,\n",
    "- RMSProp,\n",
    "- and Adam optimization\n",
    "\n",
    "are presented below.\n",
    "### Momentum Optimization\n",
    "page 295<br>\n",
    "Just like a ball picks up momentum as it rolls down a slope with a constant (and finite) gradient, the Momentum Optimizer also picks up speed: it adds the local gradient - multiplied by the learning rate $\\eta$ - to the **momentum vector** $m$ and then updates the weights and biases ($\\theta$) by simply subtracting $m$, see Equation 11-4 in the book:\n",
    "$$m\\leftarrow\\beta m+\\eta\\nabla_{\\theta}J(\\theta)\\,,\\quad\\theta\\leftarrow\\theta-m\\,.$$\n",
    "<br>\n",
    "The hyperparameter $\\beta$ is simply referred to as **momentum** and must be set between 0 (no momentum, high friction) and 1 (full momentum, no friction). For terminal momentum, the current and next $m$ have the same value and \"$\\leftarrow$\" can be replaced by \"$=$\" in the (first) above equation and on may resolve the terminal momentum\n",
    "$$m_{\\rm terminal}=\\frac{\\eta}{1-\\beta}\\nabla_{\\theta}J(\\theta)\\,,$$\n",
    "<br>\n",
    "which - for $\\beta=0.9$ - is 10 times faster than default gradient descent. This allows Momentum Optimization to leave plateaus much faster. It is also more likely to *roll* past local optima. Implementation in TensorFlow is simple, just use the following `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might argue that the extra hyperparameter $\\beta$ is a drawback since it must be determined. But in most cases $\\beta=0.9$ works out of the box and much faster than default GradientDescent.<br><br>\n",
    "**General note**<br>\n",
    "Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons why it is good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.\n",
    "### Nesterov Accelerated Gradient\n",
    "page 297<br>\n",
    "In contrast to vanilla Momentum Optimization, Nesterov Momentum optimization or **NAG** for **Nesterov Accelerated Gradient** (proposed by Yurii Nesterov in 1983, see https://goo.gl/V011vD) measures the gradient not at the local position $\\theta$ but slightly down the road at $\\theta+\\beta m$, where $m$ is again the momentum:\n",
    "- Momentum Optimization updates the momentum via $\\qquad m\\leftarrow\\beta m+\\eta\\nabla_{\\theta}(\\theta)\\qquad$ and\n",
    "- Nesterov Accelerated Gradient via $\\qquad\\hspace{+3.06cm} m\\leftarrow\\beta m+\\eta\\nabla_{\\theta}(\\theta+\\beta m)\\quad$.\n",
    "\n",
    "This makes sense because usually the gradient does not change abruptly and so Nesterov's gradient is closer to the average gradient along the step. Due to this, NAG also helps reduce oscillations. NAG will almost always be faster than vanilla Momentum Optimization. To use it, one just needs to set `use_nesterov=True`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "page 298<br>\n",
    "Imagine a cost function that form an elongated bowl. Vanilla gradient descent would go down the steepest slope and then spend a lot of time crawling along the shallow valley towards the minimum. It would be much better to aim more directly to the minimum from the beginning on. The **AdaGrad** - short for \"Adaptive Subgradient Methods for Online Learning and Stochatic Optimization\" (http://goo.gl/4Tyd4j) - algorithm aims to solve this by scaling the gradient components. The vector $s$ accumulates the squares of the gradients and is then used to rescale the learning rate $\\eta$ dependent on the component of the gradient (see Equation 11-6 in the book):\n",
    "$$s\\leftarrow s+\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)\\,,\\quad\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta)\\oslash\\sqrt{s+\\epsilon}\\,.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbol $\\otimes$ ($\\oslash$) denotes element-wise multiplication (division). The smoothing term $\\epsilon$ is usually chosen to be $10^{-10}$ and avoids division by zero.<br>\n",
    "AdaGrad is often useful for simple quadratic problems but tends to stop too early when training artificial neural networks. So while TensorFlow does provide an `AdagradOptimizer`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", it might not be the optimal choice for training DNNs.\n",
    "### RMSProp\n",
    "page 300<br>\n",
    "This algorithm improves AdaGrad by slowly decaying the influence of previous contributions to the vector $s$ via the decay rate $\\beta$ (Equation 11-7 in the book):<br><br>\n",
    "$$s\\leftarrow\\beta s+(1-\\beta)\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)\\,,\\quad\\theta\\leftarrow\\theta-\\eta\\nabla_{\\theta}J(\\theta)\\oslash\\sqrt{s+\\epsilon}\\,.$$\n",
    "<br>\n",
    "A decay rate of $\\beta=0.9$ usually works very well. So although formally, $\\beta$ is a new hyperparameter, there is often no need to tune it. Apart from very simple problems, RMSProp usually performs much better than AdaGrad and converges faster than Momentum Optimization and Nesterov Accelerated Gradient. Many considered it the best default optimizer until Adam Optimization came around. There is no formal paper for RMSProp, so people often cite \"slide 29 in lecture 6\" of Geoffrey Hinton's Coursera class (see http://goo.gl/RsQeis and https://goo.gl/XUbIyJ). TensorFlow's implementation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", also uses \"momentum\", probably referring to Momentum Optimization. This can be avoided by setting `momentum=0.0`, see https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer for details.\n",
    "### Adam Optimization\n",
    "page 300<br>\n",
    "*Adam* (https://goo.gl/Un8Axa) stands for *adaptive moment estimation* and combines Momentum Optimization and RMSProp by keeping track of exponentially decaying averages of (i) past gradients (for momentum) and (ii) past squares of gradient components (for component-wise scaling of the learning rate). For each iteration $T$, both decaying averages are rescaled before being used. That rescaling is useful because both averages are initialized to 0 and thus would be inconveniently small for early iterations. Equation 11-8 in the book summarizes the algorithm:\n",
    "1. $m\\leftarrow\\beta_1m+(1-\\beta_1)\\nabla_{\\theta}J(\\theta)$,\n",
    "2. $s\\leftarrow\\beta_2s+(1-\\beta_2)\\nabla_{\\theta}J(\\theta)\\otimes\\nabla_{\\theta}J(\\theta)$,\n",
    "3. $m\\leftarrow\\frac{m}{1-\\beta_1^T}$,\n",
    "4. $s\\leftarrow\\frac{s}{1-\\beta_2^T}$,\n",
    "5. $\\theta\\leftarrow\\theta-\\eta m\\oslash\\sqrt{s+\\epsilon}$.\n",
    "\n",
    "Typical hyperparameter choices are $\\beta_1=0.9$, $\\beta_2=0.999$, and $\\epsilon=10^{-8}$. These are also the default values of the TensorFlow class, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam, AdaGrad, and RMSProp are adaptive optimizers and thus need less manual tuning of the learning rate $\\eta$. A good default value is $\\eta=0.001$. With these values working usually well, Adam Optimization is even simpler to use than vanilla Gradient Descent.<br><br>\n",
    "**Warning / caution**<br>\n",
    "This book initially recommended using Adam optimization, because it was generally considered faster and better than other methods. However, a 2017 paper (https://goo.gl/NAkWIa) by Ashia C. Wilson *et al.* showed that adaptive optimization methods (i.e., AdaGrad, RMSProp, and Adam optimization) can lead to solutions that generalize poorly on some datasets. So you may want to **stick to Momentum optimization or Nesterov Accelerated Gradient** for now, until researchers have a better understanding of this issue.<br><br>\n",
    "**Training Sparse Models**<br>\n",
    "All the optimization algorithms just presented produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead.<br>\n",
    "One trivial way to achieve this is to train the model as usual, then get rid of the tiny weights (set them to 0).<br>\n",
    "Another otpion is to apply strong $l_1$ regularization during training, as it pushes the optimizer to zero out as many weights as it can (as discussed in Chapter 4 about Lasso Regression).<br>\n",
    "However, in some cases these techniques may remain insufficient. One last option is to apply *Dual Averaging*, often called *Follow The Regularied Leader* (FTRL), a techniuqe proposed by Yurii Nesterov (https://goo.gl/xSQD4C). When used with $l_1$ regularization, this technique often leads to very sparse models. TensorFlow implements a variant of FTRL called *FTRL-Proximal* (https://goo.gl/bxme2B) in the `FTRLOptimizer` class.<br><br>\n",
    "All optimizers that have been introduced here only use the first-order partial derivatives (*Jacobians*). In addition, there are interesting optimizers that also use the **second-order partial derivatives (*Hessians*)**. However, these optimizers are very hard to apply to DNNs because with $n$ parameters, there are $n$ Jacobians and $n^2$ Hessians. With typically $n>10^4$ (or even $n>10^6$), the Hessians often **do not even fit in memory**. And if the do, it still takes a long time to compute them.\n",
    "### Learning Rate Scheduling\n",
    "page 303<br>\n",
    "The correct learning rate is important: a too large learning rate will have the algorithm jump around the minimum, possibly even going further and further away from it and a too small learning rate will surely get very close to the minimum but will usually take a very, very long time to get there. *Learning schedules* (briefly introduced in Chapter 4 can speed up the process of getting very close to the minimum by gradually adjusting the learning rate. Here is a list of the most common learning schedules:\n",
    "- *Predetermined piecwiese constant learning rate*<br>\n",
    "For example, set the learning rate to $\\eta_0=0.1$ at first, the to $\\eta_1=0.001$ after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them.\n",
    "- *Performance scheudling*<br>\n",
    "Measure the validation error every $N$ steps (just like for early stopping) and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping.\n",
    "- *Exponential scheduling*<br>\n",
    "Set the learning rate to a function of the iteration number $t:\\,\\eta(t)=\\eta_010^{-t/r}$. This works great, but it requires tuning $\\eta_0$ and $r$. The learning rate will drop by a factor of 10 every $r$ steps.\n",
    "- *Power scheduling*<br>\n",
    "Set the learning rate to $\\eta(t)=\\eta_0(1+t/r)^{-c}$. The hyperparameter $c$ is typically set to 1. This is similar to exponential scheduling, but the learning rate drops much more slowly.\n",
    "\n",
    "In their 2013 paper (http://goo.gl/Hu6Zyq), Andreq Senior *et al.* compared different learning schedules on a speech recognition task and found that both performance scheduling and exponential scheduling performed well. They slightly favor exponential decay because it is simple to implement, easy to use, and it converged slightly faster. A TensorFlow implementation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:\t\t 0 \tvalidation accuracy:\t 0.9618\n",
      "global step:\t 1100 \tlearning_rate:\t\t 0.077624716\n",
      "\n",
      "epoch:\t\t 1 \tvalidation accuracy:\t 0.9754\n",
      "global step:\t 2200 \tlearning_rate:\t\t 0.060255956\n",
      "\n",
      "epoch:\t\t 2 \tvalidation accuracy:\t 0.975\n",
      "global step:\t 3300 \tlearning_rate:\t\t 0.04677352\n",
      "\n",
      "epoch:\t\t 3 \tvalidation accuracy:\t 0.9828\n",
      "global step:\t 4400 \tlearning_rate:\t\t 0.036307808\n",
      "\n",
      "epoch:\t\t 4 \tvalidation accuracy:\t 0.9812\n",
      "global step:\t 5500 \tlearning_rate:\t\t 0.02818383\n"
     ]
    }
   ],
   "source": [
    "### github\n",
    "# some shallow dnn for MNIST\n",
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "### book\n",
    "# training node with learning rate schedule (name_scope not shown in book)\n",
    "with tf.name_scope(\"train\"):\n",
    "    initial_learning_rate = 0.1 # start with this learning rate\n",
    "    decay_steps = 10000         # stop decaying the learning rate after 10000 steps\n",
    "    decay_rate = 1/10           # decrease the learning rate by factor of 1/e every 10 steps\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")               # count the steps, starting from 0\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,               # let tf update learning rate ...\n",
    "                                               global_step,decay_steps, decay_rate) # ... according to parameters\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)             # pass learning rate to optimizer\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)                 # use optimizer for training op\n",
    "### github\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        ### own formatting and additional outputs \"global_step\" and \"learning_rate\"\n",
    "        print(\"\\nepoch:\\t\\t\", epoch, \"\\tvalidation accuracy:\\t\", accuracy_val)\n",
    "        print(\"global step:\\t\", global_step.eval(), \"\\tlearning_rate:\\t\\t\", learning_rate.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad, RMSProp and Adam optimization rescale the learning rate already so they do not require learning scheduling. But for other optimizers, in particular for Momentum Optimization and Nesterov Accelerated Gradient, Exponential Scheduling can considerably speed up convergence.\n",
    "## Avoiding Overfitting Through Regularization\n",
    "page 305<br>\n",
    "\"With four parameters I can fit an elephant and with five I can make ihim wiggle his trunk.\" –– John von Neumann, *cited by Enrico Fermi in Nature 427*<br><br>\n",
    "DNNs can have millions of parameters. This allows them to learn complex function but also makes them prone to overfitting on the training set. Regularization helps avoid overfitting. And this section is about regularizing with TensorFlow using *early stopping*, *$l_1$* and *$l_2$ regularization*, *dropout*, *max-norm regularization*, and *data augmentation*.\n",
    "### Early Stopping\n",
    "page 305<br>\n",
    "First introduced in Chapter 4, **early stopping** is a great way to avoid overfitting: simply stop training (=fitting) the model once the performance on the validation set drops. To this end, evaluate the performance on the validation set during training and stop training when the performance has dropped and did not recover after, say 5 (feel free to adjust), epochs. During training, always save the best model so it can be recovered after stopping early.<br>\n",
    "Early stopping works wonderfully but usually one can still increase performance by combining it with another regularization technique.\n",
    "### $l_1$ and $l_2$ Regularization\n",
    "page 305<br>\n",
    "Just as in Chapter 4, **$l_1$ and $l_2$ regularization** can be implemented by adding a term to the loss function that is the bigger the larger the $l_1$- and $l_2$-norms, respectively, of the DNN's weights (but usually not its biases) are. There are at least two ways to do this with TensorFlow. The first, rather manual is shown below.<br>\n",
    "[In Chapter 4, look for \"Ridge Regression\" ($l_2$, page127), \"Lasso Regression\" ($l_1$, page 130, and \"Elastic Net\" (mix of $l_1$ and $l_2$, page 132).]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n"
     ]
    }
   ],
   "source": [
    "### book and gitub\n",
    "reset_graph() # always a good idea\n",
    "# architecture\n",
    "n_inputs = 28 * 28 # MNIST\n",
    "n_hidden1 = 300    # layer 1\n",
    "n_outputs = 10     # output (number of classes)\n",
    "# inputs\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") # feature batch placeholder\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")             # label batch placeholder\n",
    "# building\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "# retrieve the weights (not the biases)\n",
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "# regularization hyperparameter\n",
    "scale = 0.001\n",
    "# construction of the combined (!) loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) # cross entropy\n",
    "    # the output of the above has the same shape as \"y\" => one output for every instance of the batch ...\n",
    "    # ... so we need to average the output (over all instances in the batch) to obtain a single scalar;\n",
    "    # for details, see: https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")          # mean loss of cross entropy\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) # scalar sum of regularization in both layers\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")          # combined loss\n",
    "### just github\n",
    "# evaluation\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)                  # is the label y among the top k (here, k=1) logits?\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), # accuracy\n",
    "                              name=\"accuracy\")\n",
    "# learning schedule\n",
    "learning_rate = 0.01\n",
    "n_epochs = 6       # adapted from default 20 for brevity\n",
    "batch_size = 200\n",
    "# training node\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "# initialization and saving\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# let the session begin\n",
    "with tf.Session() as sess:\n",
    "    # initialize all variables\n",
    "    init.run()\n",
    "    # loop over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # shuffle the data, make batches, and feed them for training\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # in each epoch, after training on all batches, check and print the accuracy on the validation set\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "    # saving is skipped, here (although shown on github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"manual\" way is a bit invconvenient if there are many layers. In that case, TensorFlow's own implementation might be useful: many functions that create trainable variables accept a `*_regularizer` argument (e.g., `kernel_regularizer`). Passing a function the regularizer argument of choice will have it calculate the according loss. In the end, **all losses can be collected automatically**  and added to the cost function. There are many regularizers, including `l1_regularizer()`, `l2_regularizer()`, and `l1_l2_regularizer()`.<br>\n",
    "The code below resembles the one above so only the changed parts are commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8118\n",
      "1 Validation accuracy: 0.8624\n",
      "2 Validation accuracy: 0.8782\n",
      "3 Validation accuracy: 0.8856\n",
      "4 Validation accuracy: 0.8932\n",
      "5 Validation accuracy: 0.8966\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# regularization hyperparameter now appears here\n",
    "scale = 0.001\n",
    "# use partial function to simplify code (but not necessary)\n",
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu,\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "# build same dnn but in a different way (use above partial function, which uses regularization)\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    logits = my_dense_layer(hidden1, n_outputs, activation=None, name=\"outputs\")\n",
    "# no need to retrieve the weights one by one via \"tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\"\n",
    "# construct the combined loss in a slightly different way (much easier for many layers)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) # collect all regularization losses this easy!\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")             # combined loss\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "learning_rate = 0.01\n",
    "n_epochs = 6\n",
    "batch_size = 200\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "# \"saver\" shown on github but deleted here and in session\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning / caution**<br>\n",
    "Don't forget to add the regularization losses to your overall loss, or else they will simply be ignored.\n",
    "### Dropout\n",
    "page 307<br>\n",
    "This is a very powerful and popular regularization technique that was introduced by G. E. Hinton in 2012 (https://goo.gl/PMjVnG) and further detailed in a paper (http://goo.gl/DNKZo1) by Nitish Srivastava *et al.*. Even on very performant networks it tends to give an accuracy boost of 1-2%. That is a lot if the network already had an accuracy of 95%. **Dropout** works by switching off / suppressing the output of every neuron in the network (including the input neurons but excluding the output neurons) with a **probability $p$ during training**. Dropout is not applied during inference. So during training, each neuron sees on average input from only $1-p$ times the neurons it sees during inference. To avoid that total input scale to a neuron is during training is the same as during inference one can\n",
    "- divide each neuron's output by $1-p$ during training (implemented by TensorFlow with `tf.layers.dropout()`) or\n",
    "- multiply a each neuron's input by $1-p$ during inference.\n",
    "\n",
    "The quantity $1-p$ is also referred to as the **keep rate** since it is the number of neurons that are on average kept (= not dropped) during training.<br>\n",
    "Say there are $N$ neurons in a network in each has to possible states during training: dropped or kept. That makes $2^N$ different neural networks, which is usually a humongous number since neural networks have often far more than 1000 parameters. In many cases the network will never be exactly the same for two training instances. And after training, the resulting network is actually a mixture (or average) of the networks that occurred during training.<br>\n",
    "Dropout forces the network to be more resilient in that it does not rely strongly on individual neurons. Would a company also work better when each employee showed up only with probability $p=0.5$ on a given day? Maybe yes. The competences would be more distributed over the employees (neurons). The failure of one individual (due to a bad day - or due to bad input) could be better compensated by other employees (neurons). It would take longer for the company to run smooth but then it might do very well. Indeed, neural networks that use dropout usually take longer to train but then perform better. We shall try it out below!<br><br>\n",
    "**Warning / caution**<br>\n",
    "You want to use the `tf.layers.dropout()` function, not `tf.nn.dropout()`. The first one turns off (no-op) when not training, which is what you want, while the second one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9254\n",
      "1 Validation accuracy: 0.9452\n",
      "2 Validation accuracy: 0.9492\n",
      "3 Validation accuracy: 0.9566\n",
      "4 Validation accuracy: 0.9618\n",
      "5 Validation accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "### own\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "### github\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "### book\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\") # specify when training and when infering\n",
    "dropout_rate = 0.5                                                       # \"dropout rate\" = 1 - \"keep rate\"\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)           # dropout for input neurons\n",
    "with tf.name_scope(\"dnn\"):                                               # build the dnn\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # layer 1 default\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)                # layer 1 dropout\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # layer 2 default\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)                # layer 2 dropout\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")    # no dropout for output layer\n",
    "### github\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "# \"saver\" shown on github but deleted here and in session\n",
    "n_epochs = 6 # adapted from default 20 for brevity\n",
    "batch_size = 50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})         # training \"True\" ...\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})                      # ... \"False\" (default)\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout behaves differently during trainig (neurons are dropped out) and inference (no neurons are dropped out). So we need to tell the network when we are training and when not. In the code above, this is achieved with the placeholder `training`. We have done something like this also for batch normalization, where we needed to tell the network whether to calculate the mean and standard deviation of the input to the activation function over the current batch (for training) or take the running averages of these values calculated during training (for inference).<br><br>\n",
    "**General note**<br>\n",
    "*Dropconnect* is a variant of dropout where individual connections are dropped randomly rather than whole nerurons. In general dropout performs better.\n",
    "### Max-Norm Regularization\n",
    "page 309<br>\n",
    "For each neuron, **max-norm regularization** constrains the weight vector $w$ for the incoming connections such that $|w|_2\\leq r$, where the scalar $r>0$ is the max-norm hyperparameter. Decreasing $r$ increases the regularization. Regularization is implemented by rescaling the weights in the following way (if $|w|_2>r$):\n",
    "$$w\\leftarrow w\\frac{r}{|w|_2}\\,.$$\n",
    "Usually, max-norm regularization is applied **to all layers' but the output layer's** neurons' kernels (=weights). Max-norm regularization is quite popular for neural networks and can help alleviate the vanishing / exploding gradients problem (if Batch Normalization is not used). There is no off-the-shelf implementation for TensorFlow but there are at least two ways to do it, again one is more *manual* and the other more *automatic* (and thus more convenient when many layers are used). In any case, we need to get a handle on the weights, normalize them, and feed them back. We start with the more manual implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "0 Validation accuracy: 0.9568\n",
      "1 Validation accuracy: 0.9702\n",
      "2 Validation accuracy: 0.9718\n",
      "3 Validation accuracy: 0.9774\n",
      "4 Validation accuracy: 0.9772\n",
      "5 Validation accuracy: 0.9782\n"
     ]
    }
   ],
   "source": [
    "# by and large, the network is the same as before so only the max-norm regularization will be commented\n",
    "### github\n",
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "### book and own code (for second hidden layer; also on github)\n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")   # get the weights / kernel\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)   # use \"tf.clip_by_norm\" for clipping ...\n",
    "# ... them to the specified thrshold; https://www.tensorflow.org/api_docs/python/tf/clip_by_norm\n",
    "clip_weights = tf.assign(weights, clipped_weights)                        # feed the clipped weights back\n",
    "# do this same for all layers but the output layer\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")  # same as above ...\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1) # ... only with ...\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)                     # ... different names\n",
    "### github\n",
    "init = tf.global_variables_initializer()\n",
    "# \"saver\" shown on github but deleted here and in session\n",
    "n_epochs = 6 # adapted from default 20 for brevity\n",
    "batch_size = 50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            # book and own code (for second hidden layer; also on github)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()\n",
    "        # github\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work really well! But note that we have to build code for each and every layer to which we wish to apply max-norm regularization. Both for the construction and for the execution phase. This means a lot of work for very deep networks. The below code alleviates this problem by creating a `max_norm_regularizer()` function that can be assigned to a layer just like any other regularizer function, e.g., like `l1_regularizer()` (see demonstration further above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.max_norm_regularizer.<locals>.max_norm(weights)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### book\n",
    "# define the regularizer function\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None\n",
    "    return max_norm # this returns the function \"max_norm\", not the output of \"max_norm(weights)\"; in fact ...\n",
    "                    # ... \"max_norm(weights)\" does not return output (\"None\"), it just adds the clipped weights ...\n",
    "                    # ... \"clip_weights\" to the collection \"max_norm\"\n",
    "# specify the threshold and leave the default values unchanged\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "max_norm_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we feed this regularizer function simply to all layers but the output layer. But there is still more to be done! Max-norm regularization is not added to the cost function. It needs to be executed explicitly at runtime! So we retrieve the clipping collection `\"max_norm\"` and execute it **at runtime after the training operation but before calculating the accuracy** score on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9556\n",
      "1 Validation accuracy: 0.9706\n",
      "2 Validation accuracy: 0.9688\n",
      "3 Validation accuracy: 0.975\n",
      "4 Validation accuracy: 0.9774\n",
      "5 Validation accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "# by and large, the network is still the same so comments only where needed\n",
    "### github\n",
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\") # use \"max_norm_reg\" ...\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\") # ... as defined above\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "# \"saver\" shown on github but deleted here and in session\n",
    "n_epochs = 6                                                                   # adapted from default 20 for brevity\n",
    "batch_size = 50\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")                               # access all the weights for clipping\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)                                         # run the weight clipping\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we needed to add much less code for each *additional* layer. Only the `regularizer` function needs to be specified for each layer. This could even be simplified with Python's `partial()` function, as we did for batch-normalization further above.\n",
    "### Data Augmentation\n",
    "page 311<br>\n",
    "Artificially increasing the size of the training set by generating new instances out of the available data will reduce overfitting and thus counts as a regularization technique. Simply **adding noise does not work**: the applied modifications should be learnable. (Noise is not learnable since it is purely random.) At best, a human should not be able to tell which instances are original and which ones have been generated.<br>\n",
    "In the context of pictures one may for example\n",
    "- slightly translate the image,\n",
    "- slightly rotate the image,\n",
    "- mirror the image if the mirrored image still falls into this class (this does not work for the class of \"left hands\", for example),\n",
    "- apply a slightly different light condition,\n",
    "- or slightly change the contrast.\n",
    "\n",
    "By **combining** these techniques, one can **significantly** augment the dataset. This also means that it often makes sense to generate new instances on the fly: storing the augmented dataset would consume a lot of memory. TensorFlow offers commands for a lot of data augmentation operations: shifting, rotating, resizing, flipping, cropping, and changing the brightness, contrast, saturation, or hue. In fact, there are even many more on the tensorflow website, see https://www.tensorflow.org/api_docs/python/tf/image.<br><br>\n",
    "**General note**<br>\n",
    "Another powerful technique to train very deep neural networks is to add *skip connections* (a skip connection is when you add the input of a layer to the output of a higher layer). We will explore this idea in Chapter 13 when we talk about deep residual networks.\n",
    "### Practical Guidelines\n",
    "page 312<br>\n",
    "A lot of techniques for training DNNs have been introduced in this Chapter. The following cornerstones will usually be a good starting configuration:\n",
    "- He initialization,\n",
    "- ELU activation function,\n",
    "- Batch Normalization,\n",
    "- Dropout regularization,\n",
    "- Nesterov Accelerated Gradient optimizer, and\n",
    "- *no* learning rate schedule.\n",
    "\n",
    "If possible, try to reuse (the lower layers of) a model that was trained on a similar task. Often, the above default configuration may need adjustment.\n",
    "- Try a learning schedule (e.g. exponential decay) if adjusting the learning rate does not lead to fast convergence *and* good accuracy at the same time.\n",
    "- Use data augmentation if the training set is too small.\n",
    "- If a sparse model is needed, add $l_1$ regularization and possibly chop tiny weights to zero. If the model still needs to be more sparse, try out the *Following The Regularized Leader* (FTRL) optimizer. See above for more details.\n",
    "- For lightning fast models at runtime, drop Batch Normalization, maybe replace ELU by leaky ReLU activation, and try to make the model as sparse as possible.\n",
    "\n",
    "## Exercises\n",
    "page 313\n",
    "### 1.-7.\n",
    "Solutions are shown in Appendix A of the book and in the separate notebook *ExercisesWithoutCode*.\n",
    "### 8. Deep Learning.\n",
    "- Build a DNN with five hidden layers of 100 neurons each, He initializatio, and the ELU activation function.\n",
    "- Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "- Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "- Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "- Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
    "\n",
    "The entire solution is heavily based on the code from github. First, make a function that builds the desired DNN (with He initialization and ELU activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Y_proba:0\", shape=(?, 5), dtype=float32)\n",
      "first task accomplished\n"
     ]
    }
   ],
   "source": [
    "# we will need 2 DNNs so a function that takes care of the hidden layers is going to be handy\n",
    "he_init = tf.variance_scaling_initializer()                  # He initializer\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None, # function that builds a network and returns its output\n",
    "        activation=tf.nn.elu, initializer=he_init):          # by default, use ELU activation and He initialization\n",
    "    with tf.variable_scope(name, \"dnn\"):                     # build the network\n",
    "        for layer in range(n_hidden_layers):                 # loop through the layers, always calculating the output\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer, name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs                                        # return the last layers output\n",
    "# build the network using the above function and additional code (for input and output)\n",
    "n_inputs = 28 * 28\n",
    "n_outputs = 5\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "dnn_outputs = dnn(X)                                         # here, use the above function\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")              # softmax output with n_output=5 neurons\n",
    "print(Y_proba)\n",
    "print(\"first task accomplished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the dataset (very efficiently), use Adam optimization, and softmax output with five neurons for classification. Run and save this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([False,  True,  True, ..., False, False, False])]\n",
      "(28038, 784) (55000, 784)\n",
      "0\tValidation loss: 0.142523\tBest loss: 0.142523\tAccuracy: 96.87%\n",
      "1\tValidation loss: 1.206604\tBest loss: 0.142523\tAccuracy: 42.03%\n",
      "2\tValidation loss: 0.271909\tBest loss: 0.142523\tAccuracy: 95.58%\n",
      "3\tValidation loss: 0.269873\tBest loss: 0.142523\tAccuracy: 94.53%\n",
      "4\tValidation loss: 0.196974\tBest loss: 0.142523\tAccuracy: 95.97%\n",
      "5\tValidation loss: 0.167328\tBest loss: 0.142523\tAccuracy: 97.26%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_8/my_mnist_model_0_to_4.ckpt\n",
      "Final test accuracy: 97.35%\n"
     ]
    }
   ],
   "source": [
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "print([y_train < 5])                 # this here is very useful to ...\n",
    "print(X_train1.shape, X_train.shape) # ... select a dataset with digits 0 to 4\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]\n",
    "#\n",
    "learning_rate = 0.01\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) # \"logits\" uses above function ...\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")                                       # ... and \"x_entropy\" uses softmax\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)                                  # Adam optimization\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "#\n",
    "n_epochs = 16                        # changed from default 1000 for brevity\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 5      # changed from default 20 for brevity\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:                                                   # early stopping\n",
    "            save_path = saver.save(sess, \"./tf_logs/11_Training/Ex_8/my_mnist_model_0_to_4.ckpt\") # path adapted\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tf_logs/11_Training/Ex_8/my_mnist_model_0_to_4.ckpt\")                  # path adapted\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish the next three tasks, we use the very fancy code from github. Apparently, it builds a Scikit-Learn-like classifier – using TensorFlow! – so that Scikit-Learn's `RandomizedSearchCV` can be applied in the default manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.142523\tBest loss: 0.142523\tAccuracy: 96.87%\n",
      "1\tValidation loss: 1.206604\tBest loss: 0.142523\tAccuracy: 42.03%\n",
      "2\tValidation loss: 0.271909\tBest loss: 0.142523\tAccuracy: 95.58%\n",
      "3\tValidation loss: 0.269873\tBest loss: 0.142523\tAccuracy: 94.53%\n",
      "4\tValidation loss: 0.196974\tBest loss: 0.142523\tAccuracy: 95.97%\n",
      "5\tValidation loss: 0.167328\tBest loss: 0.142523\tAccuracy: 97.26%\n",
      "6\tValidation loss: 0.202854\tBest loss: 0.142523\tAccuracy: 94.68%\n",
      "7\tValidation loss: 0.291291\tBest loss: 0.142523\tAccuracy: 95.70%\n",
      "8\tValidation loss: 0.186152\tBest loss: 0.142523\tAccuracy: 97.73%\n",
      "9\tValidation loss: 0.948319\tBest loss: 0.142523\tAccuracy: 69.66%\n",
      "10\tValidation loss: 0.329804\tBest loss: 0.142523\tAccuracy: 93.78%\n",
      "11\tValidation loss: 0.162550\tBest loss: 0.142523\tAccuracy: 97.07%\n",
      "12\tValidation loss: 0.138445\tBest loss: 0.138445\tAccuracy: 96.79%\n",
      "13\tValidation loss: 0.216108\tBest loss: 0.138445\tAccuracy: 96.29%\n",
      "14\tValidation loss: 0.170976\tBest loss: 0.138445\tAccuracy: 95.50%\n",
      "15\tValidation loss: 0.183409\tBest loss: 0.138445\tAccuracy: 96.64%\n",
      "16\tValidation loss: 0.201099\tBest loss: 0.138445\tAccuracy: 97.42%\n",
      "17\tValidation loss: 0.188710\tBest loss: 0.138445\tAccuracy: 95.86%\n",
      "18\tValidation loss: 0.170321\tBest loss: 0.138445\tAccuracy: 97.65%\n",
      "19\tValidation loss: 0.146281\tBest loss: 0.138445\tAccuracy: 97.26%\n",
      "20\tValidation loss: 0.356284\tBest loss: 0.138445\tAccuracy: 97.58%\n",
      "21\tValidation loss: 0.229963\tBest loss: 0.138445\tAccuracy: 97.65%\n",
      "22\tValidation loss: 0.215502\tBest loss: 0.138445\tAccuracy: 95.97%\n",
      "23\tValidation loss: 0.316733\tBest loss: 0.138445\tAccuracy: 92.42%\n",
      "24\tValidation loss: 0.144643\tBest loss: 0.138445\tAccuracy: 97.38%\n",
      "25\tValidation loss: 0.661203\tBest loss: 0.138445\tAccuracy: 93.63%\n",
      "26\tValidation loss: 0.377005\tBest loss: 0.138445\tAccuracy: 91.44%\n",
      "27\tValidation loss: 0.516353\tBest loss: 0.138445\tAccuracy: 94.72%\n",
      "28\tValidation loss: 0.351703\tBest loss: 0.138445\tAccuracy: 95.58%\n",
      "29\tValidation loss: 0.315247\tBest loss: 0.138445\tAccuracy: 94.96%\n",
      "30\tValidation loss: 0.599598\tBest loss: 0.138445\tAccuracy: 78.26%\n",
      "31\tValidation loss: 0.355985\tBest loss: 0.138445\tAccuracy: 92.34%\n",
      "32\tValidation loss: 0.246641\tBest loss: 0.138445\tAccuracy: 96.91%\n",
      "33\tValidation loss: 0.500164\tBest loss: 0.138445\tAccuracy: 76.90%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function elu at 0x114248840>,\n",
       "              batch_norm_momentum=None, batch_size=20, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=100,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        # Initialize the DNNClassifier by simply storing all the hyperparameters.\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "    def _dnn(self, inputs):\n",
    "        # Build the hidden layers, with support for batch normalization and dropout.\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer, name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        # Build the same model as earlier!\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "        dnn_outputs = self._dnn(X)\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        # Make the important operations available easily through instance variables!\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "    def _get_model_params(self):\n",
    "        # Get all variable values (used for early stopping, faster than saving to disk)!\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "    def _restore_model_params(self, model_params):\n",
    "        # Set all variables to the given values (for early stopping, faster than loading from disk)!\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        # Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\n",
    "        self.close_session()\n",
    "        # Infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        # Translate the labels vector to a vector of sorted class indices, containing integers from 0 to n_outputs-1.\n",
    "        # For example, if y is equal to [8,8,9,5,7,6,6,6], then the sorted class labels (self.classes_) will be ...\n",
    "        # ... equal to [5, 6, 7, 8, 9], and the labels vector will be translated to [3, 3, 4, 0, 2, 1, 1, 1].\n",
    "        self.class_to_index_ = {label: index for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label] for label in y], dtype=np.int32)\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid, self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch, self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found!\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)\n",
    "# create an instance of the newly defined classifier class and train it (in a Scikit-Learn fashion)\n",
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initial training, check the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974508659272232"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `RandomizedSearchCV` to tune the hyperparameters and search for the best solution. Here, the hyperparamters are\n",
    "- the number of neurons,\n",
    "- the batch size,\n",
    "- the learning rate, and\n",
    "- the activation function.\n",
    "\n",
    "The **Leaky ReLU** activation is explicitly defined, here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.145078\tBest loss: 0.145078\tAccuracy: 96.64%\n",
      "1\tValidation loss: 0.173971\tBest loss: 0.145078\tAccuracy: 95.19%\n",
      "2\tValidation loss: 0.157920\tBest loss: 0.145078\tAccuracy: 96.40%\n",
      "3\tValidation loss: 0.148727\tBest loss: 0.145078\tAccuracy: 96.48%\n",
      "4\tValidation loss: 0.137161\tBest loss: 0.137161\tAccuracy: 96.72%\n",
      "5\tValidation loss: 0.118723\tBest loss: 0.118723\tAccuracy: 96.64%\n",
      "6\tValidation loss: 0.111457\tBest loss: 0.111457\tAccuracy: 96.56%\n",
      "7\tValidation loss: 0.125656\tBest loss: 0.111457\tAccuracy: 96.91%\n",
      "8\tValidation loss: 1.649478\tBest loss: 0.111457\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.573059\tBest loss: 0.111457\tAccuracy: 23.26%\n",
      "10\tValidation loss: 1.571778\tBest loss: 0.111457\tAccuracy: 23.26%\n",
      "11\tValidation loss: 1.580718\tBest loss: 0.111457\tAccuracy: 21.42%\n",
      "12\tValidation loss: 1.591989\tBest loss: 0.111457\tAccuracy: 21.42%\n",
      "13\tValidation loss: 1.586365\tBest loss: 0.111457\tAccuracy: 21.07%\n",
      "14\tValidation loss: 1.586736\tBest loss: 0.111457\tAccuracy: 21.07%\n",
      "15\tValidation loss: 1.573506\tBest loss: 0.111457\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.572849\tBest loss: 0.111457\tAccuracy: 21.62%\n",
      "17\tValidation loss: 1.576523\tBest loss: 0.111457\tAccuracy: 21.62%\n",
      "18\tValidation loss: 1.579752\tBest loss: 0.111457\tAccuracy: 21.07%\n",
      "19\tValidation loss: 1.587769\tBest loss: 0.111457\tAccuracy: 21.42%\n",
      "20\tValidation loss: 1.590676\tBest loss: 0.111457\tAccuracy: 21.42%\n",
      "21\tValidation loss: 1.592417\tBest loss: 0.111457\tAccuracy: 21.62%\n",
      "22\tValidation loss: 1.585878\tBest loss: 0.111457\tAccuracy: 21.07%\n",
      "23\tValidation loss: 1.577521\tBest loss: 0.111457\tAccuracy: 21.42%\n",
      "24\tValidation loss: 1.600499\tBest loss: 0.111457\tAccuracy: 22.01%\n",
      "25\tValidation loss: 1.585033\tBest loss: 0.111457\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.601310\tBest loss: 0.111457\tAccuracy: 21.07%\n",
      "27\tValidation loss: 1.596321\tBest loss: 0.111457\tAccuracy: 21.62%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x114248840>, total=   9.8s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    9.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.132567\tBest loss: 0.132567\tAccuracy: 96.64%\n",
      "1\tValidation loss: 0.140460\tBest loss: 0.132567\tAccuracy: 96.48%\n",
      "2\tValidation loss: 0.140922\tBest loss: 0.132567\tAccuracy: 96.48%\n",
      "3\tValidation loss: 0.303073\tBest loss: 0.132567\tAccuracy: 95.43%\n",
      "4\tValidation loss: 0.324907\tBest loss: 0.132567\tAccuracy: 93.86%\n",
      "5\tValidation loss: 0.271279\tBest loss: 0.132567\tAccuracy: 94.21%\n",
      "6\tValidation loss: 0.205956\tBest loss: 0.132567\tAccuracy: 94.80%\n",
      "7\tValidation loss: 0.169514\tBest loss: 0.132567\tAccuracy: 96.21%\n",
      "8\tValidation loss: 0.293873\tBest loss: 0.132567\tAccuracy: 93.55%\n",
      "9\tValidation loss: 0.191498\tBest loss: 0.132567\tAccuracy: 95.04%\n",
      "10\tValidation loss: 0.215853\tBest loss: 0.132567\tAccuracy: 94.53%\n",
      "11\tValidation loss: 0.251179\tBest loss: 0.132567\tAccuracy: 92.42%\n",
      "12\tValidation loss: 0.187263\tBest loss: 0.132567\tAccuracy: 95.31%\n",
      "13\tValidation loss: 0.187945\tBest loss: 0.132567\tAccuracy: 95.47%\n",
      "14\tValidation loss: 0.379301\tBest loss: 0.132567\tAccuracy: 86.94%\n",
      "15\tValidation loss: 0.336114\tBest loss: 0.132567\tAccuracy: 91.87%\n",
      "16\tValidation loss: 0.577157\tBest loss: 0.132567\tAccuracy: 73.46%\n",
      "17\tValidation loss: 0.614491\tBest loss: 0.132567\tAccuracy: 75.25%\n",
      "18\tValidation loss: 0.516692\tBest loss: 0.132567\tAccuracy: 75.06%\n",
      "19\tValidation loss: 0.504678\tBest loss: 0.132567\tAccuracy: 77.56%\n",
      "20\tValidation loss: 0.528930\tBest loss: 0.132567\tAccuracy: 74.63%\n",
      "21\tValidation loss: 0.492534\tBest loss: 0.132567\tAccuracy: 75.06%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x114248840>, total=   9.2s\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.172299\tBest loss: 0.172299\tAccuracy: 96.01%\n",
      "1\tValidation loss: 0.124226\tBest loss: 0.124226\tAccuracy: 96.72%\n",
      "2\tValidation loss: 0.147093\tBest loss: 0.124226\tAccuracy: 96.05%\n",
      "3\tValidation loss: 1.666462\tBest loss: 0.124226\tAccuracy: 22.05%\n",
      "4\tValidation loss: 1.609192\tBest loss: 0.124226\tAccuracy: 20.91%\n",
      "5\tValidation loss: 1.621601\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.618750\tBest loss: 0.124226\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.624670\tBest loss: 0.124226\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.616005\tBest loss: 0.124226\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.612978\tBest loss: 0.124226\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.613787\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.613473\tBest loss: 0.124226\tAccuracy: 20.91%\n",
      "12\tValidation loss: 1.632761\tBest loss: 0.124226\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.613757\tBest loss: 0.124226\tAccuracy: 20.91%\n",
      "14\tValidation loss: 1.613464\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.618415\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.637714\tBest loss: 0.124226\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.629466\tBest loss: 0.124226\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.615552\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.625259\tBest loss: 0.124226\tAccuracy: 19.08%\n",
      "20\tValidation loss: 1.608272\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.655565\tBest loss: 0.124226\tAccuracy: 19.27%\n",
      "22\tValidation loss: 1.627072\tBest loss: 0.124226\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=100, activation=<function elu at 0x114248840>, total=   9.9s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.123843\tBest loss: 0.123843\tAccuracy: 96.48%\n",
      "1\tValidation loss: 0.081996\tBest loss: 0.081996\tAccuracy: 97.58%\n",
      "2\tValidation loss: 0.080024\tBest loss: 0.080024\tAccuracy: 97.97%\n",
      "3\tValidation loss: 0.080587\tBest loss: 0.080024\tAccuracy: 97.81%\n",
      "4\tValidation loss: 0.090256\tBest loss: 0.080024\tAccuracy: 97.62%\n",
      "5\tValidation loss: 0.065006\tBest loss: 0.065006\tAccuracy: 98.12%\n",
      "6\tValidation loss: 0.066348\tBest loss: 0.065006\tAccuracy: 98.05%\n",
      "7\tValidation loss: 0.068641\tBest loss: 0.065006\tAccuracy: 98.28%\n",
      "8\tValidation loss: 0.088763\tBest loss: 0.065006\tAccuracy: 97.30%\n",
      "9\tValidation loss: 0.066853\tBest loss: 0.065006\tAccuracy: 98.28%\n",
      "10\tValidation loss: 0.071343\tBest loss: 0.065006\tAccuracy: 98.32%\n",
      "11\tValidation loss: 0.079152\tBest loss: 0.065006\tAccuracy: 98.08%\n",
      "12\tValidation loss: 0.078396\tBest loss: 0.065006\tAccuracy: 98.40%\n",
      "13\tValidation loss: 0.073002\tBest loss: 0.065006\tAccuracy: 98.24%\n",
      "14\tValidation loss: 0.077049\tBest loss: 0.065006\tAccuracy: 98.24%\n",
      "15\tValidation loss: 0.064506\tBest loss: 0.064506\tAccuracy: 98.55%\n",
      "16\tValidation loss: 0.073830\tBest loss: 0.064506\tAccuracy: 98.55%\n",
      "17\tValidation loss: 0.073431\tBest loss: 0.064506\tAccuracy: 98.32%\n",
      "18\tValidation loss: 0.075932\tBest loss: 0.064506\tAccuracy: 98.48%\n",
      "19\tValidation loss: 0.072128\tBest loss: 0.064506\tAccuracy: 98.51%\n",
      "20\tValidation loss: 0.078192\tBest loss: 0.064506\tAccuracy: 98.36%\n",
      "21\tValidation loss: 0.077933\tBest loss: 0.064506\tAccuracy: 98.59%\n",
      "22\tValidation loss: 0.078562\tBest loss: 0.064506\tAccuracy: 98.55%\n",
      "23\tValidation loss: 0.067230\tBest loss: 0.064506\tAccuracy: 98.51%\n",
      "24\tValidation loss: 0.089066\tBest loss: 0.064506\tAccuracy: 98.28%\n",
      "25\tValidation loss: 0.123047\tBest loss: 0.064506\tAccuracy: 97.77%\n",
      "26\tValidation loss: 0.085399\tBest loss: 0.064506\tAccuracy: 98.44%\n",
      "27\tValidation loss: 0.073884\tBest loss: 0.064506\tAccuracy: 98.55%\n",
      "28\tValidation loss: 0.087694\tBest loss: 0.064506\tAccuracy: 98.28%\n",
      "29\tValidation loss: 0.082780\tBest loss: 0.064506\tAccuracy: 98.28%\n",
      "30\tValidation loss: 0.089772\tBest loss: 0.064506\tAccuracy: 97.85%\n",
      "31\tValidation loss: 0.079688\tBest loss: 0.064506\tAccuracy: 98.40%\n",
      "32\tValidation loss: 0.116680\tBest loss: 0.064506\tAccuracy: 97.93%\n",
      "33\tValidation loss: 0.099635\tBest loss: 0.064506\tAccuracy: 98.32%\n",
      "34\tValidation loss: 0.095852\tBest loss: 0.064506\tAccuracy: 98.71%\n",
      "35\tValidation loss: 0.128991\tBest loss: 0.064506\tAccuracy: 98.12%\n",
      "36\tValidation loss: 0.108185\tBest loss: 0.064506\tAccuracy: 98.28%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x114259620>, total=  12.2s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.118695\tBest loss: 0.118695\tAccuracy: 96.76%\n",
      "1\tValidation loss: 0.075928\tBest loss: 0.075928\tAccuracy: 97.89%\n",
      "2\tValidation loss: 0.062562\tBest loss: 0.062562\tAccuracy: 98.32%\n",
      "3\tValidation loss: 0.082379\tBest loss: 0.062562\tAccuracy: 97.65%\n",
      "4\tValidation loss: 0.060277\tBest loss: 0.060277\tAccuracy: 98.20%\n",
      "5\tValidation loss: 0.066071\tBest loss: 0.060277\tAccuracy: 98.32%\n",
      "6\tValidation loss: 0.059187\tBest loss: 0.059187\tAccuracy: 98.44%\n",
      "7\tValidation loss: 0.062682\tBest loss: 0.059187\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.071768\tBest loss: 0.059187\tAccuracy: 98.01%\n",
      "9\tValidation loss: 0.046293\tBest loss: 0.046293\tAccuracy: 98.79%\n",
      "10\tValidation loss: 0.067719\tBest loss: 0.046293\tAccuracy: 98.44%\n",
      "11\tValidation loss: 0.052096\tBest loss: 0.046293\tAccuracy: 98.79%\n",
      "12\tValidation loss: 0.063291\tBest loss: 0.046293\tAccuracy: 98.20%\n",
      "13\tValidation loss: 0.060343\tBest loss: 0.046293\tAccuracy: 98.55%\n",
      "14\tValidation loss: 0.071471\tBest loss: 0.046293\tAccuracy: 98.55%\n",
      "15\tValidation loss: 0.062462\tBest loss: 0.046293\tAccuracy: 98.59%\n",
      "16\tValidation loss: 0.060522\tBest loss: 0.046293\tAccuracy: 98.48%\n",
      "17\tValidation loss: 0.048769\tBest loss: 0.046293\tAccuracy: 98.75%\n",
      "18\tValidation loss: 0.068874\tBest loss: 0.046293\tAccuracy: 98.75%\n",
      "19\tValidation loss: 0.067289\tBest loss: 0.046293\tAccuracy: 98.71%\n",
      "20\tValidation loss: 0.063137\tBest loss: 0.046293\tAccuracy: 98.79%\n",
      "21\tValidation loss: 0.065596\tBest loss: 0.046293\tAccuracy: 98.83%\n",
      "22\tValidation loss: 0.079936\tBest loss: 0.046293\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.085127\tBest loss: 0.046293\tAccuracy: 98.28%\n",
      "24\tValidation loss: 0.054981\tBest loss: 0.046293\tAccuracy: 98.83%\n",
      "25\tValidation loss: 0.062627\tBest loss: 0.046293\tAccuracy: 98.71%\n",
      "26\tValidation loss: 0.083365\tBest loss: 0.046293\tAccuracy: 98.40%\n",
      "27\tValidation loss: 0.069775\tBest loss: 0.046293\tAccuracy: 98.75%\n",
      "28\tValidation loss: 0.087948\tBest loss: 0.046293\tAccuracy: 98.51%\n",
      "29\tValidation loss: 0.066588\tBest loss: 0.046293\tAccuracy: 98.83%\n",
      "30\tValidation loss: 0.060948\tBest loss: 0.046293\tAccuracy: 99.10%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x114259620>, total=   8.8s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x114259620> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.115925\tBest loss: 0.115925\tAccuracy: 96.36%\n",
      "1\tValidation loss: 0.076872\tBest loss: 0.076872\tAccuracy: 97.58%\n",
      "2\tValidation loss: 0.067116\tBest loss: 0.067116\tAccuracy: 98.01%\n",
      "3\tValidation loss: 0.058144\tBest loss: 0.058144\tAccuracy: 98.36%\n",
      "4\tValidation loss: 0.074449\tBest loss: 0.058144\tAccuracy: 98.16%\n",
      "5\tValidation loss: 0.054461\tBest loss: 0.054461\tAccuracy: 98.28%\n",
      "6\tValidation loss: 0.056603\tBest loss: 0.054461\tAccuracy: 98.44%\n",
      "7\tValidation loss: 0.055495\tBest loss: 0.054461\tAccuracy: 98.51%\n",
      "8\tValidation loss: 0.074740\tBest loss: 0.054461\tAccuracy: 98.24%\n",
      "9\tValidation loss: 0.064255\tBest loss: 0.054461\tAccuracy: 98.51%\n",
      "10\tValidation loss: 0.060270\tBest loss: 0.054461\tAccuracy: 98.40%\n",
      "11\tValidation loss: 0.054939\tBest loss: 0.054461\tAccuracy: 98.59%\n",
      "12\tValidation loss: 0.065591\tBest loss: 0.054461\tAccuracy: 98.67%\n",
      "13\tValidation loss: 0.074305\tBest loss: 0.054461\tAccuracy: 98.63%\n",
      "14\tValidation loss: 0.067463\tBest loss: 0.054461\tAccuracy: 98.28%\n",
      "15\tValidation loss: 0.072535\tBest loss: 0.054461\tAccuracy: 98.83%\n",
      "16\tValidation loss: 0.076750\tBest loss: 0.054461\tAccuracy: 98.63%\n",
      "17\tValidation loss: 0.076333\tBest loss: 0.054461\tAccuracy: 98.48%\n",
      "18\tValidation loss: 0.090909\tBest loss: 0.054461\tAccuracy: 98.83%\n",
      "19\tValidation loss: 0.077159\tBest loss: 0.054461\tAccuracy: 98.71%\n",
      "20\tValidation loss: 0.068072\tBest loss: 0.054461\tAccuracy: 98.75%\n",
      "21\tValidation loss: 0.093293\tBest loss: 0.054461\tAccuracy: 98.71%\n",
      "22\tValidation loss: 0.063369\tBest loss: 0.054461\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.100262\tBest loss: 0.054461\tAccuracy: 98.55%\n",
      "24\tValidation loss: 0.089640\tBest loss: 0.054461\tAccuracy: 98.75%\n",
      "25\tValidation loss: 0.091219\tBest loss: 0.054461\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.078943\tBest loss: 0.054461\tAccuracy: 98.71%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=500, activation=<function relu at 0x114259620>, total=   8.0s\n",
      "[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 39.380558\tBest loss: 39.380558\tAccuracy: 28.77%\n",
      "1\tValidation loss: 11.570292\tBest loss: 11.570292\tAccuracy: 40.73%\n",
      "2\tValidation loss: 2548.848877\tBest loss: 11.570292\tAccuracy: 20.09%\n",
      "3\tValidation loss: 373.418579\tBest loss: 11.570292\tAccuracy: 63.02%\n",
      "4\tValidation loss: 397.213013\tBest loss: 11.570292\tAccuracy: 58.76%\n",
      "5\tValidation loss: 480.434326\tBest loss: 11.570292\tAccuracy: 62.00%\n",
      "6\tValidation loss: 236.977966\tBest loss: 11.570292\tAccuracy: 67.98%\n",
      "7\tValidation loss: 380.898590\tBest loss: 11.570292\tAccuracy: 64.82%\n",
      "8\tValidation loss: 191.740097\tBest loss: 11.570292\tAccuracy: 72.20%\n",
      "9\tValidation loss: 264.550568\tBest loss: 11.570292\tAccuracy: 58.05%\n",
      "10\tValidation loss: 222.660446\tBest loss: 11.570292\tAccuracy: 64.31%\n",
      "11\tValidation loss: 311.316010\tBest loss: 11.570292\tAccuracy: 75.25%\n",
      "12\tValidation loss: 584.060242\tBest loss: 11.570292\tAccuracy: 88.31%\n",
      "13\tValidation loss: 60.794495\tBest loss: 11.570292\tAccuracy: 84.91%\n",
      "14\tValidation loss: 55.055706\tBest loss: 11.570292\tAccuracy: 89.52%\n",
      "15\tValidation loss: 69.678535\tBest loss: 11.570292\tAccuracy: 86.36%\n",
      "16\tValidation loss: 46995.539062\tBest loss: 11.570292\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1104.038208\tBest loss: 11.570292\tAccuracy: 51.13%\n",
      "18\tValidation loss: 233.719681\tBest loss: 11.570292\tAccuracy: 75.92%\n",
      "19\tValidation loss: 234.548691\tBest loss: 11.570292\tAccuracy: 77.83%\n",
      "20\tValidation loss: 123.769485\tBest loss: 11.570292\tAccuracy: 85.42%\n",
      "21\tValidation loss: 166.432022\tBest loss: 11.570292\tAccuracy: 78.81%\n",
      "22\tValidation loss: 201.217331\tBest loss: 11.570292\tAccuracy: 85.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  39.6s\n",
      "[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 28.341219\tBest loss: 28.341219\tAccuracy: 52.66%\n",
      "1\tValidation loss: 11.196418\tBest loss: 11.196418\tAccuracy: 61.14%\n",
      "2\tValidation loss: 4.322415\tBest loss: 4.322415\tAccuracy: 74.35%\n",
      "3\tValidation loss: 1.632852\tBest loss: 1.632852\tAccuracy: 87.88%\n",
      "4\tValidation loss: 2.499480\tBest loss: 1.632852\tAccuracy: 82.49%\n",
      "5\tValidation loss: 2.683970\tBest loss: 1.632852\tAccuracy: 79.20%\n",
      "6\tValidation loss: 1.047337\tBest loss: 1.047337\tAccuracy: 92.18%\n",
      "7\tValidation loss: 8488.392578\tBest loss: 1.047337\tAccuracy: 19.23%\n",
      "8\tValidation loss: 1094.718872\tBest loss: 1.047337\tAccuracy: 18.88%\n",
      "9\tValidation loss: 1410.982544\tBest loss: 1.047337\tAccuracy: 48.83%\n",
      "10\tValidation loss: 262.397491\tBest loss: 1.047337\tAccuracy: 69.27%\n",
      "11\tValidation loss: 174.002930\tBest loss: 1.047337\tAccuracy: 79.05%\n",
      "12\tValidation loss: 161.230179\tBest loss: 1.047337\tAccuracy: 80.84%\n",
      "13\tValidation loss: 396.298035\tBest loss: 1.047337\tAccuracy: 65.05%\n",
      "14\tValidation loss: 185.492783\tBest loss: 1.047337\tAccuracy: 75.72%\n",
      "15\tValidation loss: 145.517838\tBest loss: 1.047337\tAccuracy: 78.73%\n",
      "16\tValidation loss: 187.492676\tBest loss: 1.047337\tAccuracy: 88.00%\n",
      "17\tValidation loss: 207.432983\tBest loss: 1.047337\tAccuracy: 72.83%\n",
      "18\tValidation loss: 305.350891\tBest loss: 1.047337\tAccuracy: 85.77%\n",
      "19\tValidation loss: 132.121490\tBest loss: 1.047337\tAccuracy: 87.72%\n",
      "20\tValidation loss: 163.453583\tBest loss: 1.047337\tAccuracy: 89.41%\n",
      "21\tValidation loss: 95.585358\tBest loss: 1.047337\tAccuracy: 89.64%\n",
      "22\tValidation loss: 104.370979\tBest loss: 1.047337\tAccuracy: 80.22%\n",
      "23\tValidation loss: 84.029350\tBest loss: 1.047337\tAccuracy: 81.59%\n",
      "24\tValidation loss: 42.285732\tBest loss: 1.047337\tAccuracy: 91.24%\n",
      "25\tValidation loss: 58.918552\tBest loss: 1.047337\tAccuracy: 83.93%\n",
      "26\tValidation loss: 30.012175\tBest loss: 1.047337\tAccuracy: 92.85%\n",
      "27\tValidation loss: 5817.991211\tBest loss: 1.047337\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  45.1s\n",
      "[CV] n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 327.081207\tBest loss: 327.081207\tAccuracy: 27.91%\n",
      "1\tValidation loss: 9.478352\tBest loss: 9.478352\tAccuracy: 41.24%\n",
      "2\tValidation loss: 2.358540\tBest loss: 2.358540\tAccuracy: 65.01%\n",
      "3\tValidation loss: 1.267310\tBest loss: 1.267310\tAccuracy: 78.89%\n",
      "4\tValidation loss: 0.817706\tBest loss: 0.817706\tAccuracy: 86.00%\n",
      "5\tValidation loss: 1.448321\tBest loss: 0.817706\tAccuracy: 81.00%\n",
      "6\tValidation loss: 0.726330\tBest loss: 0.726330\tAccuracy: 88.86%\n",
      "7\tValidation loss: 0.555002\tBest loss: 0.555002\tAccuracy: 89.52%\n",
      "8\tValidation loss: 2.130070\tBest loss: 0.555002\tAccuracy: 83.03%\n",
      "9\tValidation loss: 0.543977\tBest loss: 0.543977\tAccuracy: 92.65%\n",
      "10\tValidation loss: 0.584742\tBest loss: 0.543977\tAccuracy: 92.65%\n",
      "11\tValidation loss: 0.740846\tBest loss: 0.543977\tAccuracy: 89.29%\n",
      "12\tValidation loss: 6429.024414\tBest loss: 0.543977\tAccuracy: 35.57%\n",
      "13\tValidation loss: 9820.825195\tBest loss: 0.543977\tAccuracy: 26.54%\n",
      "14\tValidation loss: 76.936378\tBest loss: 0.543977\tAccuracy: 68.57%\n",
      "15\tValidation loss: 473.941895\tBest loss: 0.543977\tAccuracy: 36.24%\n",
      "16\tValidation loss: 111.152100\tBest loss: 0.543977\tAccuracy: 48.12%\n",
      "17\tValidation loss: 110.381844\tBest loss: 0.543977\tAccuracy: 41.79%\n",
      "18\tValidation loss: 46.989990\tBest loss: 0.543977\tAccuracy: 57.11%\n",
      "19\tValidation loss: 33.149994\tBest loss: 0.543977\tAccuracy: 63.41%\n",
      "20\tValidation loss: 59.773735\tBest loss: 0.543977\tAccuracy: 70.05%\n",
      "21\tValidation loss: 40.642258\tBest loss: 0.543977\tAccuracy: 66.93%\n",
      "22\tValidation loss: 19.505651\tBest loss: 0.543977\tAccuracy: 71.74%\n",
      "23\tValidation loss: 334207.843750\tBest loss: 0.543977\tAccuracy: 28.89%\n",
      "24\tValidation loss: 2767.947021\tBest loss: 0.543977\tAccuracy: 52.50%\n",
      "25\tValidation loss: 134.411316\tBest loss: 0.543977\tAccuracy: 52.85%\n",
      "26\tValidation loss: 180.537766\tBest loss: 0.543977\tAccuracy: 65.87%\n",
      "27\tValidation loss: 243.276611\tBest loss: 0.543977\tAccuracy: 67.90%\n",
      "28\tValidation loss: 174.611176\tBest loss: 0.543977\tAccuracy: 49.30%\n",
      "29\tValidation loss: 549.595642\tBest loss: 0.543977\tAccuracy: 45.90%\n",
      "30\tValidation loss: 27677.218750\tBest loss: 0.543977\tAccuracy: 62.63%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=90, learning_rate=0.05, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  50.7s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 57.873569\tBest loss: 57.873569\tAccuracy: 94.10%\n",
      "1\tValidation loss: 37.014317\tBest loss: 37.014317\tAccuracy: 91.16%\n",
      "2\tValidation loss: 13.864572\tBest loss: 13.864572\tAccuracy: 94.61%\n",
      "3\tValidation loss: 5.985952\tBest loss: 5.985952\tAccuracy: 96.33%\n",
      "4\tValidation loss: 5.117127\tBest loss: 5.117127\tAccuracy: 95.97%\n",
      "5\tValidation loss: 3.829853\tBest loss: 3.829853\tAccuracy: 95.74%\n",
      "6\tValidation loss: 2.736975\tBest loss: 2.736975\tAccuracy: 96.17%\n",
      "7\tValidation loss: 11.158143\tBest loss: 2.736975\tAccuracy: 92.42%\n",
      "8\tValidation loss: 7952236.500000\tBest loss: 2.736975\tAccuracy: 39.76%\n",
      "9\tValidation loss: 157717.359375\tBest loss: 2.736975\tAccuracy: 73.92%\n",
      "10\tValidation loss: 46322.695312\tBest loss: 2.736975\tAccuracy: 88.98%\n",
      "11\tValidation loss: 38651.398438\tBest loss: 2.736975\tAccuracy: 87.02%\n",
      "12\tValidation loss: 16216.562500\tBest loss: 2.736975\tAccuracy: 93.04%\n",
      "13\tValidation loss: 16141.016602\tBest loss: 2.736975\tAccuracy: 93.47%\n",
      "14\tValidation loss: 288146.875000\tBest loss: 2.736975\tAccuracy: 73.65%\n",
      "15\tValidation loss: 18488.847656\tBest loss: 2.736975\tAccuracy: 93.86%\n",
      "16\tValidation loss: 8545.469727\tBest loss: 2.736975\tAccuracy: 95.70%\n",
      "17\tValidation loss: 12040.608398\tBest loss: 2.736975\tAccuracy: 94.02%\n",
      "18\tValidation loss: 6288.642578\tBest loss: 2.736975\tAccuracy: 96.21%\n",
      "19\tValidation loss: 18597.832031\tBest loss: 2.736975\tAccuracy: 88.19%\n",
      "20\tValidation loss: 85439.640625\tBest loss: 2.736975\tAccuracy: 95.23%\n",
      "21\tValidation loss: 46793.343750\tBest loss: 2.736975\tAccuracy: 95.35%\n",
      "22\tValidation loss: 35822.312500\tBest loss: 2.736975\tAccuracy: 95.27%\n",
      "23\tValidation loss: 10138.593750\tBest loss: 2.736975\tAccuracy: 94.96%\n",
      "24\tValidation loss: 15265.300781\tBest loss: 2.736975\tAccuracy: 95.90%\n",
      "25\tValidation loss: 14096.343750\tBest loss: 2.736975\tAccuracy: 96.79%\n",
      "26\tValidation loss: 18269.326172\tBest loss: 2.736975\tAccuracy: 96.68%\n",
      "27\tValidation loss: 25656.521484\tBest loss: 2.736975\tAccuracy: 90.85%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90>, total=  39.2s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90> \n",
      "0\tValidation loss: 22.633339\tBest loss: 22.633339\tAccuracy: 88.94%\n",
      "1\tValidation loss: 12.758319\tBest loss: 12.758319\tAccuracy: 89.33%\n",
      "2\tValidation loss: 10.158163\tBest loss: 10.158163\tAccuracy: 90.27%\n",
      "3\tValidation loss: 5.751077\tBest loss: 5.751077\tAccuracy: 93.67%\n",
      "4\tValidation loss: 5980233.000000\tBest loss: 5.751077\tAccuracy: 35.03%\n",
      "5\tValidation loss: 23779.291016\tBest loss: 5.751077\tAccuracy: 87.69%\n",
      "6\tValidation loss: 14040.099609\tBest loss: 5.751077\tAccuracy: 88.66%\n",
      "7\tValidation loss: 28519.115234\tBest loss: 5.751077\tAccuracy: 90.38%\n",
      "8\tValidation loss: 17315.005859\tBest loss: 5.751077\tAccuracy: 94.25%\n",
      "9\tValidation loss: 20342.355469\tBest loss: 5.751077\tAccuracy: 87.14%\n",
      "10\tValidation loss: 8368.649414\tBest loss: 5.751077\tAccuracy: 93.59%\n",
      "11\tValidation loss: 10469.562500\tBest loss: 5.751077\tAccuracy: 92.42%\n",
      "12\tValidation loss: 5898.435059\tBest loss: 5.751077\tAccuracy: 96.21%\n",
      "13\tValidation loss: 7299.699707\tBest loss: 5.751077\tAccuracy: 95.86%\n",
      "14\tValidation loss: 6398.685547\tBest loss: 5.751077\tAccuracy: 95.19%\n",
      "15\tValidation loss: 15846.264648\tBest loss: 5.751077\tAccuracy: 95.31%\n",
      "16\tValidation loss: 6447.302246\tBest loss: 5.751077\tAccuracy: 95.70%\n",
      "17\tValidation loss: 7307.748047\tBest loss: 5.751077\tAccuracy: 95.66%\n",
      "18\tValidation loss: 21730652.000000\tBest loss: 5.751077\tAccuracy: 81.35%\n",
      "19\tValidation loss: 268083.250000\tBest loss: 5.751077\tAccuracy: 94.96%\n",
      "20\tValidation loss: 125284.601562\tBest loss: 5.751077\tAccuracy: 95.93%\n",
      "21\tValidation loss: 133064.187500\tBest loss: 5.751077\tAccuracy: 95.47%\n",
      "22\tValidation loss: 75307.570312\tBest loss: 5.751077\tAccuracy: 95.97%\n",
      "23\tValidation loss: 108290.304688\tBest loss: 5.751077\tAccuracy: 93.39%\n",
      "24\tValidation loss: 53918.410156\tBest loss: 5.751077\tAccuracy: 96.33%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90>, total=  34.6s\n",
      "[CV] n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90> \n",
      "0\tValidation loss: 730.446167\tBest loss: 730.446167\tAccuracy: 76.70%\n",
      "1\tValidation loss: 264.334015\tBest loss: 264.334015\tAccuracy: 85.46%\n",
      "2\tValidation loss: 187.733673\tBest loss: 187.733673\tAccuracy: 90.27%\n",
      "3\tValidation loss: 77.327103\tBest loss: 77.327103\tAccuracy: 95.27%\n",
      "4\tValidation loss: 129.192871\tBest loss: 77.327103\tAccuracy: 93.75%\n",
      "5\tValidation loss: 49.199871\tBest loss: 49.199871\tAccuracy: 96.05%\n",
      "6\tValidation loss: 65.060493\tBest loss: 49.199871\tAccuracy: 95.43%\n",
      "7\tValidation loss: 49.172379\tBest loss: 49.172379\tAccuracy: 96.99%\n",
      "8\tValidation loss: 401.186340\tBest loss: 49.172379\tAccuracy: 92.49%\n",
      "9\tValidation loss: 277979.593750\tBest loss: 49.172379\tAccuracy: 77.40%\n",
      "10\tValidation loss: 58960.351562\tBest loss: 49.172379\tAccuracy: 89.56%\n",
      "11\tValidation loss: 36619.285156\tBest loss: 49.172379\tAccuracy: 92.46%\n",
      "12\tValidation loss: 30501.619141\tBest loss: 49.172379\tAccuracy: 92.61%\n",
      "13\tValidation loss: 32583.349609\tBest loss: 49.172379\tAccuracy: 93.82%\n",
      "14\tValidation loss: 41452.378906\tBest loss: 49.172379\tAccuracy: 90.46%\n",
      "15\tValidation loss: 24000.250000\tBest loss: 49.172379\tAccuracy: 94.68%\n",
      "16\tValidation loss: 13432.306641\tBest loss: 49.172379\tAccuracy: 96.68%\n",
      "17\tValidation loss: 235393.375000\tBest loss: 49.172379\tAccuracy: 91.48%\n",
      "18\tValidation loss: 61257.363281\tBest loss: 49.172379\tAccuracy: 95.15%\n",
      "19\tValidation loss: 145136.562500\tBest loss: 49.172379\tAccuracy: 96.05%\n",
      "20\tValidation loss: 74547.406250\tBest loss: 49.172379\tAccuracy: 96.76%\n",
      "21\tValidation loss: 91984.765625\tBest loss: 49.172379\tAccuracy: 95.62%\n",
      "22\tValidation loss: 38245.687500\tBest loss: 49.172379\tAccuracy: 96.76%\n",
      "23\tValidation loss: 45153.789062\tBest loss: 49.172379\tAccuracy: 96.40%\n",
      "24\tValidation loss: 30272.058594\tBest loss: 49.172379\tAccuracy: 97.15%\n",
      "25\tValidation loss: 31625.695312\tBest loss: 49.172379\tAccuracy: 96.99%\n",
      "26\tValidation loss: 44482.062500\tBest loss: 49.172379\tAccuracy: 96.17%\n",
      "27\tValidation loss: 39371.347656\tBest loss: 49.172379\tAccuracy: 95.19%\n",
      "28\tValidation loss: 14127.515625\tBest loss: 49.172379\tAccuracy: 97.03%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=70, learning_rate=0.1, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90>, total=  42.4s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.110713\tBest loss: 0.110713\tAccuracy: 96.76%\n",
      "1\tValidation loss: 0.064351\tBest loss: 0.064351\tAccuracy: 97.93%\n",
      "2\tValidation loss: 0.066369\tBest loss: 0.064351\tAccuracy: 98.08%\n",
      "3\tValidation loss: 0.058562\tBest loss: 0.058562\tAccuracy: 98.24%\n",
      "4\tValidation loss: 0.054768\tBest loss: 0.054768\tAccuracy: 98.44%\n",
      "5\tValidation loss: 0.071421\tBest loss: 0.054768\tAccuracy: 98.44%\n",
      "6\tValidation loss: 0.063113\tBest loss: 0.054768\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.069125\tBest loss: 0.054768\tAccuracy: 98.20%\n",
      "8\tValidation loss: 0.048523\tBest loss: 0.048523\tAccuracy: 98.63%\n",
      "9\tValidation loss: 0.064202\tBest loss: 0.048523\tAccuracy: 98.55%\n",
      "10\tValidation loss: 0.069433\tBest loss: 0.048523\tAccuracy: 98.51%\n",
      "11\tValidation loss: 0.075778\tBest loss: 0.048523\tAccuracy: 98.16%\n",
      "12\tValidation loss: 0.070174\tBest loss: 0.048523\tAccuracy: 98.40%\n",
      "13\tValidation loss: 0.075186\tBest loss: 0.048523\tAccuracy: 98.01%\n",
      "14\tValidation loss: 0.058885\tBest loss: 0.048523\tAccuracy: 98.87%\n",
      "15\tValidation loss: 0.076379\tBest loss: 0.048523\tAccuracy: 98.40%\n",
      "16\tValidation loss: 0.073970\tBest loss: 0.048523\tAccuracy: 98.55%\n",
      "17\tValidation loss: 0.076764\tBest loss: 0.048523\tAccuracy: 98.40%\n",
      "18\tValidation loss: 0.066496\tBest loss: 0.048523\tAccuracy: 98.48%\n",
      "19\tValidation loss: 0.071280\tBest loss: 0.048523\tAccuracy: 98.79%\n",
      "20\tValidation loss: 0.086763\tBest loss: 0.048523\tAccuracy: 98.59%\n",
      "21\tValidation loss: 0.089708\tBest loss: 0.048523\tAccuracy: 98.63%\n",
      "22\tValidation loss: 0.082295\tBest loss: 0.048523\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.089119\tBest loss: 0.048523\tAccuracy: 98.55%\n",
      "24\tValidation loss: 0.095238\tBest loss: 0.048523\tAccuracy: 98.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\tValidation loss: 0.077421\tBest loss: 0.048523\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.103571\tBest loss: 0.048523\tAccuracy: 98.40%\n",
      "27\tValidation loss: 0.092265\tBest loss: 0.048523\tAccuracy: 98.40%\n",
      "28\tValidation loss: 2.922433\tBest loss: 0.048523\tAccuracy: 48.24%\n",
      "29\tValidation loss: 0.185260\tBest loss: 0.048523\tAccuracy: 96.25%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  37.3s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.105513\tBest loss: 0.105513\tAccuracy: 96.76%\n",
      "1\tValidation loss: 0.071437\tBest loss: 0.071437\tAccuracy: 97.93%\n",
      "2\tValidation loss: 0.058336\tBest loss: 0.058336\tAccuracy: 98.24%\n",
      "3\tValidation loss: 0.054771\tBest loss: 0.054771\tAccuracy: 98.36%\n",
      "4\tValidation loss: 0.064226\tBest loss: 0.054771\tAccuracy: 98.05%\n",
      "5\tValidation loss: 0.048000\tBest loss: 0.048000\tAccuracy: 98.71%\n",
      "6\tValidation loss: 0.073976\tBest loss: 0.048000\tAccuracy: 98.36%\n",
      "7\tValidation loss: 0.070444\tBest loss: 0.048000\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.050943\tBest loss: 0.048000\tAccuracy: 98.67%\n",
      "9\tValidation loss: 0.081636\tBest loss: 0.048000\tAccuracy: 98.16%\n",
      "10\tValidation loss: 0.061731\tBest loss: 0.048000\tAccuracy: 98.51%\n",
      "11\tValidation loss: 0.055993\tBest loss: 0.048000\tAccuracy: 98.59%\n",
      "12\tValidation loss: 0.063338\tBest loss: 0.048000\tAccuracy: 98.59%\n",
      "13\tValidation loss: 0.079309\tBest loss: 0.048000\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.061761\tBest loss: 0.048000\tAccuracy: 98.51%\n",
      "15\tValidation loss: 0.081926\tBest loss: 0.048000\tAccuracy: 98.16%\n",
      "16\tValidation loss: 0.079748\tBest loss: 0.048000\tAccuracy: 98.44%\n",
      "17\tValidation loss: 0.052107\tBest loss: 0.048000\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.064597\tBest loss: 0.048000\tAccuracy: 98.71%\n",
      "19\tValidation loss: 0.053452\tBest loss: 0.048000\tAccuracy: 98.71%\n",
      "20\tValidation loss: 0.082343\tBest loss: 0.048000\tAccuracy: 98.75%\n",
      "21\tValidation loss: 0.085008\tBest loss: 0.048000\tAccuracy: 98.75%\n",
      "22\tValidation loss: 0.067922\tBest loss: 0.048000\tAccuracy: 98.83%\n",
      "23\tValidation loss: 0.059270\tBest loss: 0.048000\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.077951\tBest loss: 0.048000\tAccuracy: 98.63%\n",
      "25\tValidation loss: 0.084042\tBest loss: 0.048000\tAccuracy: 98.91%\n",
      "26\tValidation loss: 0.075457\tBest loss: 0.048000\tAccuracy: 98.87%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  29.5s\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.121900\tBest loss: 0.121900\tAccuracy: 96.05%\n",
      "1\tValidation loss: 0.070815\tBest loss: 0.070815\tAccuracy: 98.01%\n",
      "2\tValidation loss: 0.062904\tBest loss: 0.062904\tAccuracy: 98.08%\n",
      "3\tValidation loss: 0.072543\tBest loss: 0.062904\tAccuracy: 98.24%\n",
      "4\tValidation loss: 0.059503\tBest loss: 0.059503\tAccuracy: 98.48%\n",
      "5\tValidation loss: 0.049878\tBest loss: 0.049878\tAccuracy: 98.32%\n",
      "6\tValidation loss: 0.049193\tBest loss: 0.049193\tAccuracy: 98.75%\n",
      "7\tValidation loss: 0.055689\tBest loss: 0.049193\tAccuracy: 98.48%\n",
      "8\tValidation loss: 0.058473\tBest loss: 0.049193\tAccuracy: 98.71%\n",
      "9\tValidation loss: 0.048049\tBest loss: 0.048049\tAccuracy: 98.67%\n",
      "10\tValidation loss: 0.084289\tBest loss: 0.048049\tAccuracy: 98.20%\n",
      "11\tValidation loss: 0.044217\tBest loss: 0.044217\tAccuracy: 98.63%\n",
      "12\tValidation loss: 0.065226\tBest loss: 0.044217\tAccuracy: 98.32%\n",
      "13\tValidation loss: 0.061583\tBest loss: 0.044217\tAccuracy: 98.71%\n",
      "14\tValidation loss: 0.049929\tBest loss: 0.044217\tAccuracy: 98.75%\n",
      "15\tValidation loss: 0.060161\tBest loss: 0.044217\tAccuracy: 98.63%\n",
      "16\tValidation loss: 0.082732\tBest loss: 0.044217\tAccuracy: 98.28%\n",
      "17\tValidation loss: 0.056293\tBest loss: 0.044217\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.065014\tBest loss: 0.044217\tAccuracy: 98.59%\n",
      "19\tValidation loss: 0.057854\tBest loss: 0.044217\tAccuracy: 98.83%\n",
      "20\tValidation loss: 0.057591\tBest loss: 0.044217\tAccuracy: 98.87%\n",
      "21\tValidation loss: 0.055668\tBest loss: 0.044217\tAccuracy: 98.87%\n",
      "22\tValidation loss: 0.051541\tBest loss: 0.044217\tAccuracy: 98.75%\n",
      "23\tValidation loss: 0.084357\tBest loss: 0.044217\tAccuracy: 98.51%\n",
      "24\tValidation loss: 0.089358\tBest loss: 0.044217\tAccuracy: 98.48%\n",
      "25\tValidation loss: 0.109182\tBest loss: 0.044217\tAccuracy: 98.16%\n",
      "26\tValidation loss: 0.120237\tBest loss: 0.044217\tAccuracy: 98.59%\n",
      "27\tValidation loss: 0.078667\tBest loss: 0.044217\tAccuracy: 98.24%\n",
      "28\tValidation loss: 0.061473\tBest loss: 0.044217\tAccuracy: 98.87%\n",
      "29\tValidation loss: 0.061884\tBest loss: 0.044217\tAccuracy: 98.94%\n",
      "30\tValidation loss: 0.076117\tBest loss: 0.044217\tAccuracy: 98.98%\n",
      "31\tValidation loss: 0.073174\tBest loss: 0.044217\tAccuracy: 98.87%\n",
      "32\tValidation loss: 0.074716\tBest loss: 0.044217\tAccuracy: 99.10%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  39.5s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.105184\tBest loss: 0.105184\tAccuracy: 96.68%\n",
      "1\tValidation loss: 0.061830\tBest loss: 0.061830\tAccuracy: 98.20%\n",
      "2\tValidation loss: 0.054045\tBest loss: 0.054045\tAccuracy: 98.44%\n",
      "3\tValidation loss: 0.047840\tBest loss: 0.047840\tAccuracy: 98.55%\n",
      "4\tValidation loss: 0.060265\tBest loss: 0.047840\tAccuracy: 98.20%\n",
      "5\tValidation loss: 0.072689\tBest loss: 0.047840\tAccuracy: 98.28%\n",
      "6\tValidation loss: 0.052102\tBest loss: 0.047840\tAccuracy: 98.59%\n",
      "7\tValidation loss: 0.041472\tBest loss: 0.041472\tAccuracy: 98.79%\n",
      "8\tValidation loss: 0.051089\tBest loss: 0.041472\tAccuracy: 98.91%\n",
      "9\tValidation loss: 0.061877\tBest loss: 0.041472\tAccuracy: 98.63%\n",
      "10\tValidation loss: 0.084304\tBest loss: 0.041472\tAccuracy: 98.55%\n",
      "11\tValidation loss: 0.082655\tBest loss: 0.041472\tAccuracy: 98.16%\n",
      "12\tValidation loss: 0.059508\tBest loss: 0.041472\tAccuracy: 98.55%\n",
      "13\tValidation loss: 0.067644\tBest loss: 0.041472\tAccuracy: 98.71%\n",
      "14\tValidation loss: 0.049003\tBest loss: 0.041472\tAccuracy: 98.91%\n",
      "15\tValidation loss: 0.088914\tBest loss: 0.041472\tAccuracy: 98.44%\n",
      "16\tValidation loss: 0.061374\tBest loss: 0.041472\tAccuracy: 98.94%\n",
      "17\tValidation loss: 0.061538\tBest loss: 0.041472\tAccuracy: 98.91%\n",
      "18\tValidation loss: 0.062615\tBest loss: 0.041472\tAccuracy: 98.83%\n",
      "19\tValidation loss: 0.065299\tBest loss: 0.041472\tAccuracy: 98.87%\n",
      "20\tValidation loss: 0.059910\tBest loss: 0.041472\tAccuracy: 98.75%\n",
      "21\tValidation loss: 0.088192\tBest loss: 0.041472\tAccuracy: 98.63%\n",
      "22\tValidation loss: 0.066206\tBest loss: 0.041472\tAccuracy: 98.94%\n",
      "23\tValidation loss: 0.074085\tBest loss: 0.041472\tAccuracy: 98.51%\n",
      "24\tValidation loss: 0.061096\tBest loss: 0.041472\tAccuracy: 98.79%\n",
      "25\tValidation loss: 0.059104\tBest loss: 0.041472\tAccuracy: 98.75%\n",
      "26\tValidation loss: 0.069336\tBest loss: 0.041472\tAccuracy: 98.75%\n",
      "27\tValidation loss: 0.072562\tBest loss: 0.041472\tAccuracy: 98.79%\n",
      "28\tValidation loss: 0.075016\tBest loss: 0.041472\tAccuracy: 98.67%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  23.6s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.096790\tBest loss: 0.096790\tAccuracy: 97.19%\n",
      "1\tValidation loss: 0.063292\tBest loss: 0.063292\tAccuracy: 98.16%\n",
      "2\tValidation loss: 0.051887\tBest loss: 0.051887\tAccuracy: 98.36%\n",
      "3\tValidation loss: 0.046959\tBest loss: 0.046959\tAccuracy: 98.63%\n",
      "4\tValidation loss: 0.050228\tBest loss: 0.046959\tAccuracy: 98.48%\n",
      "5\tValidation loss: 0.048024\tBest loss: 0.046959\tAccuracy: 98.67%\n",
      "6\tValidation loss: 0.049025\tBest loss: 0.046959\tAccuracy: 98.83%\n",
      "7\tValidation loss: 0.053871\tBest loss: 0.046959\tAccuracy: 98.28%\n",
      "8\tValidation loss: 0.049677\tBest loss: 0.046959\tAccuracy: 98.55%\n",
      "9\tValidation loss: 0.052436\tBest loss: 0.046959\tAccuracy: 98.83%\n",
      "10\tValidation loss: 0.059418\tBest loss: 0.046959\tAccuracy: 98.71%\n",
      "11\tValidation loss: 0.055974\tBest loss: 0.046959\tAccuracy: 98.71%\n",
      "12\tValidation loss: 0.040415\tBest loss: 0.040415\tAccuracy: 98.98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\tValidation loss: 0.052422\tBest loss: 0.040415\tAccuracy: 98.79%\n",
      "14\tValidation loss: 0.054438\tBest loss: 0.040415\tAccuracy: 98.71%\n",
      "15\tValidation loss: 0.062697\tBest loss: 0.040415\tAccuracy: 98.71%\n",
      "16\tValidation loss: 0.045300\tBest loss: 0.040415\tAccuracy: 98.91%\n",
      "17\tValidation loss: 0.048030\tBest loss: 0.040415\tAccuracy: 98.98%\n",
      "18\tValidation loss: 0.099778\tBest loss: 0.040415\tAccuracy: 98.16%\n",
      "19\tValidation loss: 0.061775\tBest loss: 0.040415\tAccuracy: 98.94%\n",
      "20\tValidation loss: 0.059954\tBest loss: 0.040415\tAccuracy: 98.87%\n",
      "21\tValidation loss: 0.065621\tBest loss: 0.040415\tAccuracy: 98.71%\n",
      "22\tValidation loss: 0.069489\tBest loss: 0.040415\tAccuracy: 98.71%\n",
      "23\tValidation loss: 0.079829\tBest loss: 0.040415\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.064601\tBest loss: 0.040415\tAccuracy: 98.83%\n",
      "25\tValidation loss: 0.092071\tBest loss: 0.040415\tAccuracy: 98.79%\n",
      "26\tValidation loss: 0.061713\tBest loss: 0.040415\tAccuracy: 99.18%\n",
      "27\tValidation loss: 0.056893\tBest loss: 0.040415\tAccuracy: 98.94%\n",
      "28\tValidation loss: 0.048476\tBest loss: 0.040415\tAccuracy: 99.02%\n",
      "29\tValidation loss: 0.063439\tBest loss: 0.040415\tAccuracy: 98.87%\n",
      "30\tValidation loss: 0.063379\tBest loss: 0.040415\tAccuracy: 99.22%\n",
      "31\tValidation loss: 0.079694\tBest loss: 0.040415\tAccuracy: 98.91%\n",
      "32\tValidation loss: 0.073644\tBest loss: 0.040415\tAccuracy: 98.75%\n",
      "33\tValidation loss: 0.073114\tBest loss: 0.040415\tAccuracy: 99.22%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  32.0s\n",
      "[CV] n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.082425\tBest loss: 0.082425\tAccuracy: 97.50%\n",
      "1\tValidation loss: 0.064242\tBest loss: 0.064242\tAccuracy: 98.24%\n",
      "2\tValidation loss: 0.060050\tBest loss: 0.060050\tAccuracy: 98.20%\n",
      "3\tValidation loss: 0.047084\tBest loss: 0.047084\tAccuracy: 98.48%\n",
      "4\tValidation loss: 0.054387\tBest loss: 0.047084\tAccuracy: 98.75%\n",
      "5\tValidation loss: 0.049171\tBest loss: 0.047084\tAccuracy: 98.71%\n",
      "6\tValidation loss: 0.059595\tBest loss: 0.047084\tAccuracy: 98.28%\n",
      "7\tValidation loss: 0.037176\tBest loss: 0.037176\tAccuracy: 98.94%\n",
      "8\tValidation loss: 0.047789\tBest loss: 0.037176\tAccuracy: 98.55%\n",
      "9\tValidation loss: 0.046825\tBest loss: 0.037176\tAccuracy: 98.91%\n",
      "10\tValidation loss: 0.054954\tBest loss: 0.037176\tAccuracy: 98.71%\n",
      "11\tValidation loss: 0.039740\tBest loss: 0.037176\tAccuracy: 98.94%\n",
      "12\tValidation loss: 0.063352\tBest loss: 0.037176\tAccuracy: 98.51%\n",
      "13\tValidation loss: 0.051676\tBest loss: 0.037176\tAccuracy: 98.75%\n",
      "14\tValidation loss: 0.061665\tBest loss: 0.037176\tAccuracy: 98.83%\n",
      "15\tValidation loss: 0.052292\tBest loss: 0.037176\tAccuracy: 98.59%\n",
      "16\tValidation loss: 0.043526\tBest loss: 0.037176\tAccuracy: 99.14%\n",
      "17\tValidation loss: 0.070006\tBest loss: 0.037176\tAccuracy: 98.98%\n",
      "18\tValidation loss: 0.063566\tBest loss: 0.037176\tAccuracy: 98.55%\n",
      "19\tValidation loss: 0.059968\tBest loss: 0.037176\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.053407\tBest loss: 0.037176\tAccuracy: 98.79%\n",
      "21\tValidation loss: 0.052489\tBest loss: 0.037176\tAccuracy: 98.75%\n",
      "22\tValidation loss: 0.051634\tBest loss: 0.037176\tAccuracy: 99.02%\n",
      "23\tValidation loss: 0.069670\tBest loss: 0.037176\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.045272\tBest loss: 0.037176\tAccuracy: 99.18%\n",
      "25\tValidation loss: 0.055879\tBest loss: 0.037176\tAccuracy: 99.14%\n",
      "26\tValidation loss: 0.063125\tBest loss: 0.037176\tAccuracy: 99.06%\n",
      "27\tValidation loss: 0.045583\tBest loss: 0.037176\tAccuracy: 99.18%\n",
      "28\tValidation loss: 0.065136\tBest loss: 0.037176\tAccuracy: 98.94%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=90, learning_rate=0.01, batch_size=500, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  21.1s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.125781\tBest loss: 0.125781\tAccuracy: 95.90%\n",
      "1\tValidation loss: 0.085376\tBest loss: 0.085376\tAccuracy: 97.38%\n",
      "2\tValidation loss: 0.083950\tBest loss: 0.083950\tAccuracy: 97.46%\n",
      "3\tValidation loss: 0.063494\tBest loss: 0.063494\tAccuracy: 97.97%\n",
      "4\tValidation loss: 0.067942\tBest loss: 0.063494\tAccuracy: 97.85%\n",
      "5\tValidation loss: 0.056756\tBest loss: 0.056756\tAccuracy: 98.63%\n",
      "6\tValidation loss: 0.059947\tBest loss: 0.056756\tAccuracy: 98.16%\n",
      "7\tValidation loss: 0.046515\tBest loss: 0.046515\tAccuracy: 98.59%\n",
      "8\tValidation loss: 0.054862\tBest loss: 0.046515\tAccuracy: 98.28%\n",
      "9\tValidation loss: 0.044640\tBest loss: 0.044640\tAccuracy: 98.87%\n",
      "10\tValidation loss: 0.055055\tBest loss: 0.044640\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.046934\tBest loss: 0.044640\tAccuracy: 98.67%\n",
      "12\tValidation loss: 0.067782\tBest loss: 0.044640\tAccuracy: 98.40%\n",
      "13\tValidation loss: 0.058338\tBest loss: 0.044640\tAccuracy: 98.55%\n",
      "14\tValidation loss: 0.065023\tBest loss: 0.044640\tAccuracy: 98.28%\n",
      "15\tValidation loss: 0.062243\tBest loss: 0.044640\tAccuracy: 98.59%\n",
      "16\tValidation loss: 0.055120\tBest loss: 0.044640\tAccuracy: 98.83%\n",
      "17\tValidation loss: 0.048076\tBest loss: 0.044640\tAccuracy: 98.91%\n",
      "18\tValidation loss: 0.072794\tBest loss: 0.044640\tAccuracy: 98.59%\n",
      "19\tValidation loss: 0.048900\tBest loss: 0.044640\tAccuracy: 98.94%\n",
      "20\tValidation loss: 0.067702\tBest loss: 0.044640\tAccuracy: 98.40%\n",
      "21\tValidation loss: 0.048727\tBest loss: 0.044640\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.052088\tBest loss: 0.044640\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.060039\tBest loss: 0.044640\tAccuracy: 98.71%\n",
      "24\tValidation loss: 0.041302\tBest loss: 0.041302\tAccuracy: 98.98%\n",
      "25\tValidation loss: 0.045016\tBest loss: 0.041302\tAccuracy: 98.98%\n",
      "26\tValidation loss: 0.060153\tBest loss: 0.041302\tAccuracy: 99.02%\n",
      "27\tValidation loss: 0.088435\tBest loss: 0.041302\tAccuracy: 98.36%\n",
      "28\tValidation loss: 0.053527\tBest loss: 0.041302\tAccuracy: 98.83%\n",
      "29\tValidation loss: 0.083227\tBest loss: 0.041302\tAccuracy: 98.59%\n",
      "30\tValidation loss: 0.058996\tBest loss: 0.041302\tAccuracy: 98.75%\n",
      "31\tValidation loss: 0.061226\tBest loss: 0.041302\tAccuracy: 98.79%\n",
      "32\tValidation loss: 0.064649\tBest loss: 0.041302\tAccuracy: 98.87%\n",
      "33\tValidation loss: 0.057538\tBest loss: 0.041302\tAccuracy: 99.02%\n",
      "34\tValidation loss: 0.065690\tBest loss: 0.041302\tAccuracy: 99.06%\n",
      "35\tValidation loss: 0.063142\tBest loss: 0.041302\tAccuracy: 99.14%\n",
      "36\tValidation loss: 0.074233\tBest loss: 0.041302\tAccuracy: 99.02%\n",
      "37\tValidation loss: 0.065216\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "38\tValidation loss: 0.065507\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "39\tValidation loss: 0.065976\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "40\tValidation loss: 0.066531\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "41\tValidation loss: 0.066948\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "42\tValidation loss: 0.067393\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "43\tValidation loss: 0.067791\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "44\tValidation loss: 0.068085\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "45\tValidation loss: 0.068449\tBest loss: 0.041302\tAccuracy: 99.18%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x114248840>, total=  56.3s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.133312\tBest loss: 0.133312\tAccuracy: 95.70%\n",
      "1\tValidation loss: 0.081172\tBest loss: 0.081172\tAccuracy: 97.46%\n",
      "2\tValidation loss: 0.067674\tBest loss: 0.067674\tAccuracy: 97.89%\n",
      "3\tValidation loss: 0.058299\tBest loss: 0.058299\tAccuracy: 98.28%\n",
      "4\tValidation loss: 0.065549\tBest loss: 0.058299\tAccuracy: 98.08%\n",
      "5\tValidation loss: 0.056145\tBest loss: 0.056145\tAccuracy: 98.51%\n",
      "6\tValidation loss: 0.057569\tBest loss: 0.056145\tAccuracy: 98.48%\n",
      "7\tValidation loss: 0.051622\tBest loss: 0.051622\tAccuracy: 98.71%\n",
      "8\tValidation loss: 0.043177\tBest loss: 0.043177\tAccuracy: 98.67%\n",
      "9\tValidation loss: 0.062158\tBest loss: 0.043177\tAccuracy: 98.48%\n",
      "10\tValidation loss: 0.053139\tBest loss: 0.043177\tAccuracy: 98.67%\n",
      "11\tValidation loss: 0.060810\tBest loss: 0.043177\tAccuracy: 98.75%\n",
      "12\tValidation loss: 0.043155\tBest loss: 0.043155\tAccuracy: 99.02%\n",
      "13\tValidation loss: 0.048998\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "14\tValidation loss: 0.058145\tBest loss: 0.043155\tAccuracy: 98.91%\n",
      "15\tValidation loss: 0.073840\tBest loss: 0.043155\tAccuracy: 98.36%\n",
      "16\tValidation loss: 0.066197\tBest loss: 0.043155\tAccuracy: 98.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\tValidation loss: 0.054387\tBest loss: 0.043155\tAccuracy: 98.91%\n",
      "18\tValidation loss: 0.051473\tBest loss: 0.043155\tAccuracy: 98.83%\n",
      "19\tValidation loss: 0.051254\tBest loss: 0.043155\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.055856\tBest loss: 0.043155\tAccuracy: 99.10%\n",
      "21\tValidation loss: 0.056403\tBest loss: 0.043155\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.056785\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.060307\tBest loss: 0.043155\tAccuracy: 98.63%\n",
      "24\tValidation loss: 0.080154\tBest loss: 0.043155\tAccuracy: 98.91%\n",
      "25\tValidation loss: 0.074093\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "26\tValidation loss: 0.058842\tBest loss: 0.043155\tAccuracy: 98.83%\n",
      "27\tValidation loss: 0.073931\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "28\tValidation loss: 0.071651\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "29\tValidation loss: 0.063792\tBest loss: 0.043155\tAccuracy: 98.79%\n",
      "30\tValidation loss: 0.106530\tBest loss: 0.043155\tAccuracy: 98.55%\n",
      "31\tValidation loss: 0.071836\tBest loss: 0.043155\tAccuracy: 98.91%\n",
      "32\tValidation loss: 0.093532\tBest loss: 0.043155\tAccuracy: 98.59%\n",
      "33\tValidation loss: 0.091583\tBest loss: 0.043155\tAccuracy: 98.75%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x114248840>, total=  38.4s\n",
      "[CV] n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.127921\tBest loss: 0.127921\tAccuracy: 96.33%\n",
      "1\tValidation loss: 0.078285\tBest loss: 0.078285\tAccuracy: 97.69%\n",
      "2\tValidation loss: 0.063931\tBest loss: 0.063931\tAccuracy: 98.08%\n",
      "3\tValidation loss: 0.061658\tBest loss: 0.061658\tAccuracy: 98.01%\n",
      "4\tValidation loss: 0.049285\tBest loss: 0.049285\tAccuracy: 98.44%\n",
      "5\tValidation loss: 0.047061\tBest loss: 0.047061\tAccuracy: 98.59%\n",
      "6\tValidation loss: 0.059377\tBest loss: 0.047061\tAccuracy: 98.40%\n",
      "7\tValidation loss: 0.045424\tBest loss: 0.045424\tAccuracy: 98.48%\n",
      "8\tValidation loss: 0.046374\tBest loss: 0.045424\tAccuracy: 98.40%\n",
      "9\tValidation loss: 0.050397\tBest loss: 0.045424\tAccuracy: 98.59%\n",
      "10\tValidation loss: 0.043048\tBest loss: 0.043048\tAccuracy: 98.79%\n",
      "11\tValidation loss: 0.048857\tBest loss: 0.043048\tAccuracy: 98.87%\n",
      "12\tValidation loss: 0.055433\tBest loss: 0.043048\tAccuracy: 98.51%\n",
      "13\tValidation loss: 0.045109\tBest loss: 0.043048\tAccuracy: 98.83%\n",
      "14\tValidation loss: 0.050064\tBest loss: 0.043048\tAccuracy: 98.75%\n",
      "15\tValidation loss: 0.071754\tBest loss: 0.043048\tAccuracy: 98.55%\n",
      "16\tValidation loss: 0.044163\tBest loss: 0.043048\tAccuracy: 98.94%\n",
      "17\tValidation loss: 0.050585\tBest loss: 0.043048\tAccuracy: 98.87%\n",
      "18\tValidation loss: 0.059931\tBest loss: 0.043048\tAccuracy: 98.75%\n",
      "19\tValidation loss: 0.053595\tBest loss: 0.043048\tAccuracy: 98.98%\n",
      "20\tValidation loss: 0.056391\tBest loss: 0.043048\tAccuracy: 98.98%\n",
      "21\tValidation loss: 0.060163\tBest loss: 0.043048\tAccuracy: 99.10%\n",
      "22\tValidation loss: 0.064913\tBest loss: 0.043048\tAccuracy: 98.75%\n",
      "23\tValidation loss: 0.050872\tBest loss: 0.043048\tAccuracy: 98.79%\n",
      "24\tValidation loss: 0.045827\tBest loss: 0.043048\tAccuracy: 99.02%\n",
      "25\tValidation loss: 0.066120\tBest loss: 0.043048\tAccuracy: 98.63%\n",
      "26\tValidation loss: 0.075146\tBest loss: 0.043048\tAccuracy: 98.83%\n",
      "27\tValidation loss: 0.053911\tBest loss: 0.043048\tAccuracy: 98.83%\n",
      "28\tValidation loss: 0.073190\tBest loss: 0.043048\tAccuracy: 98.59%\n",
      "29\tValidation loss: 0.075790\tBest loss: 0.043048\tAccuracy: 98.87%\n",
      "30\tValidation loss: 0.054938\tBest loss: 0.043048\tAccuracy: 99.18%\n",
      "31\tValidation loss: 0.047163\tBest loss: 0.043048\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.01, batch_size=500, activation=<function elu at 0x114248840>, total=  37.8s\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 1.637285\tBest loss: 1.637285\tAccuracy: 19.31%\n",
      "1\tValidation loss: 1.621573\tBest loss: 1.621573\tAccuracy: 19.31%\n",
      "2\tValidation loss: 1.619518\tBest loss: 1.619518\tAccuracy: 19.31%\n",
      "3\tValidation loss: 1.616241\tBest loss: 1.616241\tAccuracy: 19.31%\n",
      "4\tValidation loss: 1.654655\tBest loss: 1.616241\tAccuracy: 22.05%\n",
      "5\tValidation loss: 1.621192\tBest loss: 1.616241\tAccuracy: 22.05%\n",
      "6\tValidation loss: 1.632399\tBest loss: 1.616241\tAccuracy: 19.31%\n",
      "7\tValidation loss: 1.614556\tBest loss: 1.614556\tAccuracy: 19.31%\n",
      "8\tValidation loss: 1.642279\tBest loss: 1.614556\tAccuracy: 19.31%\n",
      "9\tValidation loss: 1.639489\tBest loss: 1.614556\tAccuracy: 19.31%\n",
      "10\tValidation loss: 1.615066\tBest loss: 1.614556\tAccuracy: 22.05%\n",
      "11\tValidation loss: 1.630252\tBest loss: 1.614556\tAccuracy: 22.05%\n",
      "12\tValidation loss: 1.636418\tBest loss: 1.614556\tAccuracy: 19.12%\n",
      "13\tValidation loss: 1.612809\tBest loss: 1.612809\tAccuracy: 19.31%\n",
      "14\tValidation loss: 1.628908\tBest loss: 1.612809\tAccuracy: 19.12%\n",
      "15\tValidation loss: 1.616831\tBest loss: 1.612809\tAccuracy: 22.05%\n",
      "16\tValidation loss: 1.626638\tBest loss: 1.612809\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.608287\tBest loss: 1.608287\tAccuracy: 22.05%\n",
      "18\tValidation loss: 1.624451\tBest loss: 1.608287\tAccuracy: 19.31%\n",
      "19\tValidation loss: 1.623751\tBest loss: 1.608287\tAccuracy: 20.95%\n",
      "20\tValidation loss: 1.615125\tBest loss: 1.608287\tAccuracy: 19.31%\n",
      "21\tValidation loss: 1.629337\tBest loss: 1.608287\tAccuracy: 19.31%\n",
      "22\tValidation loss: 1.618776\tBest loss: 1.608287\tAccuracy: 19.31%\n",
      "23\tValidation loss: 1.620755\tBest loss: 1.608287\tAccuracy: 22.05%\n",
      "24\tValidation loss: 1.633711\tBest loss: 1.608287\tAccuracy: 22.05%\n",
      "25\tValidation loss: 1.646809\tBest loss: 1.608287\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.613815\tBest loss: 1.608287\tAccuracy: 19.12%\n",
      "27\tValidation loss: 1.638242\tBest loss: 1.608287\tAccuracy: 19.31%\n",
      "28\tValidation loss: 1.655871\tBest loss: 1.608287\tAccuracy: 22.05%\n",
      "29\tValidation loss: 1.617478\tBest loss: 1.608287\tAccuracy: 18.73%\n",
      "30\tValidation loss: 1.641591\tBest loss: 1.608287\tAccuracy: 19.31%\n",
      "31\tValidation loss: 1.631202\tBest loss: 1.608287\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.640922\tBest loss: 1.608287\tAccuracy: 18.73%\n",
      "33\tValidation loss: 1.647128\tBest loss: 1.608287\tAccuracy: 19.12%\n",
      "34\tValidation loss: 1.609975\tBest loss: 1.608287\tAccuracy: 22.05%\n",
      "35\tValidation loss: 1.624348\tBest loss: 1.608287\tAccuracy: 19.12%\n",
      "36\tValidation loss: 1.630579\tBest loss: 1.608287\tAccuracy: 18.73%\n",
      "37\tValidation loss: 1.632027\tBest loss: 1.608287\tAccuracy: 18.73%\n",
      "38\tValidation loss: 1.630726\tBest loss: 1.608287\tAccuracy: 22.05%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x114259620>, total= 1.8min\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 1.631876\tBest loss: 1.631876\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1.644815\tBest loss: 1.631876\tAccuracy: 19.08%\n",
      "2\tValidation loss: 1.611659\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.614180\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "4\tValidation loss: 1.617810\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.624092\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.635104\tBest loss: 1.611659\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.630919\tBest loss: 1.611659\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.639868\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.611889\tBest loss: 1.611659\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.613289\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.614457\tBest loss: 1.611659\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.615519\tBest loss: 1.611659\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.609585\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.628127\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.613724\tBest loss: 1.609585\tAccuracy: 19.08%\n",
      "16\tValidation loss: 1.621641\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.612427\tBest loss: 1.609585\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.622173\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.611825\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.612151\tBest loss: 1.609585\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.657982\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.643388\tBest loss: 1.609585\tAccuracy: 19.27%\n",
      "23\tValidation loss: 1.647551\tBest loss: 1.609585\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.652206\tBest loss: 1.609585\tAccuracy: 18.73%\n",
      "25\tValidation loss: 1.610976\tBest loss: 1.609585\tAccuracy: 20.91%\n",
      "26\tValidation loss: 1.614099\tBest loss: 1.609585\tAccuracy: 20.91%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\tValidation loss: 1.614363\tBest loss: 1.609585\tAccuracy: 20.91%\n",
      "28\tValidation loss: 1.642987\tBest loss: 1.609585\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.614047\tBest loss: 1.609585\tAccuracy: 19.08%\n",
      "30\tValidation loss: 1.609289\tBest loss: 1.609289\tAccuracy: 22.01%\n",
      "31\tValidation loss: 1.634889\tBest loss: 1.609289\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.614009\tBest loss: 1.609289\tAccuracy: 19.08%\n",
      "33\tValidation loss: 1.619313\tBest loss: 1.609289\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.611677\tBest loss: 1.609289\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.638078\tBest loss: 1.609289\tAccuracy: 20.91%\n",
      "36\tValidation loss: 1.621398\tBest loss: 1.609289\tAccuracy: 22.01%\n",
      "37\tValidation loss: 1.617562\tBest loss: 1.609289\tAccuracy: 22.01%\n",
      "38\tValidation loss: 1.620767\tBest loss: 1.609289\tAccuracy: 20.91%\n",
      "39\tValidation loss: 1.627334\tBest loss: 1.609289\tAccuracy: 19.27%\n",
      "40\tValidation loss: 1.607883\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "41\tValidation loss: 1.634031\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "42\tValidation loss: 1.618327\tBest loss: 1.607883\tAccuracy: 20.91%\n",
      "43\tValidation loss: 1.618746\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "44\tValidation loss: 1.621665\tBest loss: 1.607883\tAccuracy: 19.08%\n",
      "45\tValidation loss: 1.629383\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "46\tValidation loss: 1.637658\tBest loss: 1.607883\tAccuracy: 18.73%\n",
      "47\tValidation loss: 1.609600\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "48\tValidation loss: 1.612162\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "49\tValidation loss: 1.643133\tBest loss: 1.607883\tAccuracy: 19.27%\n",
      "50\tValidation loss: 1.611987\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "51\tValidation loss: 1.619979\tBest loss: 1.607883\tAccuracy: 19.08%\n",
      "52\tValidation loss: 1.627026\tBest loss: 1.607883\tAccuracy: 19.08%\n",
      "53\tValidation loss: 1.618519\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "54\tValidation loss: 1.654812\tBest loss: 1.607883\tAccuracy: 18.73%\n",
      "55\tValidation loss: 1.645117\tBest loss: 1.607883\tAccuracy: 18.73%\n",
      "56\tValidation loss: 1.669471\tBest loss: 1.607883\tAccuracy: 19.08%\n",
      "57\tValidation loss: 1.619669\tBest loss: 1.607883\tAccuracy: 18.73%\n",
      "58\tValidation loss: 1.612549\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "59\tValidation loss: 1.619514\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "60\tValidation loss: 1.635963\tBest loss: 1.607883\tAccuracy: 18.73%\n",
      "61\tValidation loss: 1.627401\tBest loss: 1.607883\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x114259620>, total= 2.7min\n",
      "[CV] n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 1.644704\tBest loss: 1.644704\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.624599\tBest loss: 1.624599\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.614158\tBest loss: 1.614158\tAccuracy: 19.27%\n",
      "3\tValidation loss: 1.616997\tBest loss: 1.614158\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.614500\tBest loss: 1.614158\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.627706\tBest loss: 1.614158\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.648677\tBest loss: 1.614158\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.627603\tBest loss: 1.614158\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.625684\tBest loss: 1.614158\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.615558\tBest loss: 1.614158\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.619129\tBest loss: 1.614158\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.639600\tBest loss: 1.614158\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.627613\tBest loss: 1.614158\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.615011\tBest loss: 1.614158\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.624331\tBest loss: 1.614158\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.612441\tBest loss: 1.612441\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.627839\tBest loss: 1.612441\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.613220\tBest loss: 1.612441\tAccuracy: 18.73%\n",
      "18\tValidation loss: 1.615752\tBest loss: 1.612441\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.619096\tBest loss: 1.612441\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.609111\tBest loss: 1.609111\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.689011\tBest loss: 1.609111\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.618893\tBest loss: 1.609111\tAccuracy: 18.73%\n",
      "23\tValidation loss: 1.638928\tBest loss: 1.609111\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.660613\tBest loss: 1.609111\tAccuracy: 20.91%\n",
      "25\tValidation loss: 1.619334\tBest loss: 1.609111\tAccuracy: 19.27%\n",
      "26\tValidation loss: 1.611323\tBest loss: 1.609111\tAccuracy: 22.01%\n",
      "27\tValidation loss: 1.614304\tBest loss: 1.609111\tAccuracy: 20.91%\n",
      "28\tValidation loss: 1.628118\tBest loss: 1.609111\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.610351\tBest loss: 1.609111\tAccuracy: 20.91%\n",
      "30\tValidation loss: 1.626932\tBest loss: 1.609111\tAccuracy: 20.91%\n",
      "31\tValidation loss: 1.633272\tBest loss: 1.609111\tAccuracy: 18.73%\n",
      "32\tValidation loss: 1.609066\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "33\tValidation loss: 1.610652\tBest loss: 1.609066\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.616979\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.627558\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "36\tValidation loss: 1.609165\tBest loss: 1.609066\tAccuracy: 20.91%\n",
      "37\tValidation loss: 1.626449\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "38\tValidation loss: 1.614497\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "39\tValidation loss: 1.617754\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "40\tValidation loss: 1.623131\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "41\tValidation loss: 1.615777\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "42\tValidation loss: 1.617475\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "43\tValidation loss: 1.617776\tBest loss: 1.609066\tAccuracy: 19.08%\n",
      "44\tValidation loss: 1.626853\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "45\tValidation loss: 1.613777\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "46\tValidation loss: 1.618687\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "47\tValidation loss: 1.610630\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "48\tValidation loss: 1.614395\tBest loss: 1.609066\tAccuracy: 18.73%\n",
      "49\tValidation loss: 1.612180\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "50\tValidation loss: 1.623879\tBest loss: 1.609066\tAccuracy: 19.27%\n",
      "51\tValidation loss: 1.616768\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "52\tValidation loss: 1.620598\tBest loss: 1.609066\tAccuracy: 18.73%\n",
      "53\tValidation loss: 1.617682\tBest loss: 1.609066\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, learning_rate=0.1, batch_size=10, activation=<function relu at 0x114259620>, total= 2.5min\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.118921\tBest loss: 0.118921\tAccuracy: 96.99%\n",
      "1\tValidation loss: 0.122349\tBest loss: 0.118921\tAccuracy: 96.72%\n",
      "2\tValidation loss: 0.096356\tBest loss: 0.096356\tAccuracy: 96.79%\n",
      "3\tValidation loss: 0.087227\tBest loss: 0.087227\tAccuracy: 97.65%\n",
      "4\tValidation loss: 0.086301\tBest loss: 0.086301\tAccuracy: 97.65%\n",
      "5\tValidation loss: 0.085535\tBest loss: 0.085535\tAccuracy: 97.81%\n",
      "6\tValidation loss: 0.066096\tBest loss: 0.066096\tAccuracy: 98.01%\n",
      "7\tValidation loss: 0.104132\tBest loss: 0.066096\tAccuracy: 97.89%\n",
      "8\tValidation loss: 0.073867\tBest loss: 0.066096\tAccuracy: 98.40%\n",
      "9\tValidation loss: 0.079440\tBest loss: 0.066096\tAccuracy: 97.89%\n",
      "10\tValidation loss: 0.060554\tBest loss: 0.060554\tAccuracy: 98.28%\n",
      "11\tValidation loss: 0.091327\tBest loss: 0.060554\tAccuracy: 98.05%\n",
      "12\tValidation loss: 0.237233\tBest loss: 0.060554\tAccuracy: 95.70%\n",
      "13\tValidation loss: 0.125298\tBest loss: 0.060554\tAccuracy: 97.11%\n",
      "14\tValidation loss: 0.092448\tBest loss: 0.060554\tAccuracy: 98.01%\n",
      "15\tValidation loss: 0.090949\tBest loss: 0.060554\tAccuracy: 97.22%\n",
      "16\tValidation loss: 0.377645\tBest loss: 0.060554\tAccuracy: 92.30%\n",
      "17\tValidation loss: 0.188971\tBest loss: 0.060554\tAccuracy: 96.91%\n",
      "18\tValidation loss: 0.189500\tBest loss: 0.060554\tAccuracy: 95.43%\n",
      "19\tValidation loss: 0.139661\tBest loss: 0.060554\tAccuracy: 97.11%\n",
      "20\tValidation loss: 0.120771\tBest loss: 0.060554\tAccuracy: 97.03%\n",
      "21\tValidation loss: 0.094797\tBest loss: 0.060554\tAccuracy: 97.62%\n",
      "22\tValidation loss: 0.134729\tBest loss: 0.060554\tAccuracy: 97.19%\n",
      "23\tValidation loss: 0.083426\tBest loss: 0.060554\tAccuracy: 97.97%\n",
      "24\tValidation loss: 0.107642\tBest loss: 0.060554\tAccuracy: 97.54%\n",
      "25\tValidation loss: 0.116307\tBest loss: 0.060554\tAccuracy: 97.19%\n",
      "26\tValidation loss: 0.076370\tBest loss: 0.060554\tAccuracy: 98.08%\n",
      "27\tValidation loss: 0.095424\tBest loss: 0.060554\tAccuracy: 97.46%\n",
      "28\tValidation loss: 0.099132\tBest loss: 0.060554\tAccuracy: 98.24%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\tValidation loss: 0.076871\tBest loss: 0.060554\tAccuracy: 98.01%\n",
      "30\tValidation loss: 0.112050\tBest loss: 0.060554\tAccuracy: 97.77%\n",
      "31\tValidation loss: 0.094811\tBest loss: 0.060554\tAccuracy: 97.69%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x114259620>, total=  17.0s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.133343\tBest loss: 0.133343\tAccuracy: 96.52%\n",
      "1\tValidation loss: 0.084775\tBest loss: 0.084775\tAccuracy: 97.62%\n",
      "2\tValidation loss: 0.060354\tBest loss: 0.060354\tAccuracy: 98.28%\n",
      "3\tValidation loss: 0.106575\tBest loss: 0.060354\tAccuracy: 97.22%\n",
      "4\tValidation loss: 0.095059\tBest loss: 0.060354\tAccuracy: 98.01%\n",
      "5\tValidation loss: 0.065957\tBest loss: 0.060354\tAccuracy: 98.20%\n",
      "6\tValidation loss: 0.133254\tBest loss: 0.060354\tAccuracy: 96.79%\n",
      "7\tValidation loss: 0.095861\tBest loss: 0.060354\tAccuracy: 97.58%\n",
      "8\tValidation loss: 0.091637\tBest loss: 0.060354\tAccuracy: 98.20%\n",
      "9\tValidation loss: 0.107543\tBest loss: 0.060354\tAccuracy: 97.46%\n",
      "10\tValidation loss: 0.066731\tBest loss: 0.060354\tAccuracy: 98.16%\n",
      "11\tValidation loss: 0.162962\tBest loss: 0.060354\tAccuracy: 98.12%\n",
      "12\tValidation loss: 0.115070\tBest loss: 0.060354\tAccuracy: 97.38%\n",
      "13\tValidation loss: 0.084969\tBest loss: 0.060354\tAccuracy: 98.08%\n",
      "14\tValidation loss: 0.096687\tBest loss: 0.060354\tAccuracy: 97.85%\n",
      "15\tValidation loss: 0.095260\tBest loss: 0.060354\tAccuracy: 98.01%\n",
      "16\tValidation loss: 0.092766\tBest loss: 0.060354\tAccuracy: 97.73%\n",
      "17\tValidation loss: 0.071367\tBest loss: 0.060354\tAccuracy: 98.24%\n",
      "18\tValidation loss: 0.077481\tBest loss: 0.060354\tAccuracy: 98.08%\n",
      "19\tValidation loss: 0.073895\tBest loss: 0.060354\tAccuracy: 98.44%\n",
      "20\tValidation loss: 0.142387\tBest loss: 0.060354\tAccuracy: 94.72%\n",
      "21\tValidation loss: 0.105651\tBest loss: 0.060354\tAccuracy: 97.69%\n",
      "22\tValidation loss: 0.090625\tBest loss: 0.060354\tAccuracy: 97.69%\n",
      "23\tValidation loss: 0.139630\tBest loss: 0.060354\tAccuracy: 96.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x114259620>, total=  13.3s\n",
      "[CV] n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.101772\tBest loss: 0.101772\tAccuracy: 97.11%\n",
      "1\tValidation loss: 0.086306\tBest loss: 0.086306\tAccuracy: 97.93%\n",
      "2\tValidation loss: 0.095231\tBest loss: 0.086306\tAccuracy: 97.15%\n",
      "3\tValidation loss: 0.079535\tBest loss: 0.079535\tAccuracy: 97.85%\n",
      "4\tValidation loss: 0.070934\tBest loss: 0.070934\tAccuracy: 98.08%\n",
      "5\tValidation loss: 0.078911\tBest loss: 0.070934\tAccuracy: 98.01%\n",
      "6\tValidation loss: 0.064981\tBest loss: 0.064981\tAccuracy: 98.51%\n",
      "7\tValidation loss: 0.072277\tBest loss: 0.064981\tAccuracy: 97.89%\n",
      "8\tValidation loss: 0.085622\tBest loss: 0.064981\tAccuracy: 97.97%\n",
      "9\tValidation loss: 0.082567\tBest loss: 0.064981\tAccuracy: 98.28%\n",
      "10\tValidation loss: 0.100619\tBest loss: 0.064981\tAccuracy: 98.20%\n",
      "11\tValidation loss: 0.071738\tBest loss: 0.064981\tAccuracy: 98.32%\n",
      "12\tValidation loss: 0.322082\tBest loss: 0.064981\tAccuracy: 94.84%\n",
      "13\tValidation loss: 0.096671\tBest loss: 0.064981\tAccuracy: 97.42%\n",
      "14\tValidation loss: 0.058939\tBest loss: 0.058939\tAccuracy: 98.24%\n",
      "15\tValidation loss: 0.065456\tBest loss: 0.058939\tAccuracy: 98.28%\n",
      "16\tValidation loss: 0.057147\tBest loss: 0.057147\tAccuracy: 98.40%\n",
      "17\tValidation loss: 0.093079\tBest loss: 0.057147\tAccuracy: 98.40%\n",
      "18\tValidation loss: 0.060445\tBest loss: 0.057147\tAccuracy: 98.48%\n",
      "19\tValidation loss: 0.071742\tBest loss: 0.057147\tAccuracy: 98.55%\n",
      "20\tValidation loss: 0.053878\tBest loss: 0.053878\tAccuracy: 98.63%\n",
      "21\tValidation loss: 0.081815\tBest loss: 0.053878\tAccuracy: 98.05%\n",
      "22\tValidation loss: 0.084673\tBest loss: 0.053878\tAccuracy: 98.05%\n",
      "23\tValidation loss: 0.059837\tBest loss: 0.053878\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.165682\tBest loss: 0.053878\tAccuracy: 97.42%\n",
      "25\tValidation loss: 0.071975\tBest loss: 0.053878\tAccuracy: 98.51%\n",
      "26\tValidation loss: 0.070883\tBest loss: 0.053878\tAccuracy: 98.40%\n",
      "27\tValidation loss: 0.086402\tBest loss: 0.053878\tAccuracy: 98.08%\n",
      "28\tValidation loss: 0.073998\tBest loss: 0.053878\tAccuracy: 98.32%\n",
      "29\tValidation loss: 0.074789\tBest loss: 0.053878\tAccuracy: 98.48%\n",
      "30\tValidation loss: 0.065659\tBest loss: 0.053878\tAccuracy: 98.51%\n",
      "31\tValidation loss: 0.060367\tBest loss: 0.053878\tAccuracy: 98.32%\n",
      "32\tValidation loss: 0.055344\tBest loss: 0.053878\tAccuracy: 98.32%\n",
      "33\tValidation loss: 0.068653\tBest loss: 0.053878\tAccuracy: 98.51%\n",
      "34\tValidation loss: 0.080840\tBest loss: 0.053878\tAccuracy: 98.55%\n",
      "35\tValidation loss: 0.093383\tBest loss: 0.053878\tAccuracy: 97.19%\n",
      "36\tValidation loss: 0.078422\tBest loss: 0.053878\tAccuracy: 98.48%\n",
      "37\tValidation loss: 0.073297\tBest loss: 0.053878\tAccuracy: 98.44%\n",
      "38\tValidation loss: 0.088264\tBest loss: 0.053878\tAccuracy: 98.51%\n",
      "39\tValidation loss: 0.075295\tBest loss: 0.053878\tAccuracy: 98.63%\n",
      "40\tValidation loss: 0.093779\tBest loss: 0.053878\tAccuracy: 98.36%\n",
      "41\tValidation loss: 0.085175\tBest loss: 0.053878\tAccuracy: 98.48%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=30, learning_rate=0.02, batch_size=100, activation=<function relu at 0x114259620>, total=  22.4s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.261468\tBest loss: 0.261468\tAccuracy: 94.84%\n",
      "1\tValidation loss: 6.619938\tBest loss: 0.261468\tAccuracy: 49.06%\n",
      "2\tValidation loss: 29.470482\tBest loss: 0.261468\tAccuracy: 48.16%\n",
      "3\tValidation loss: 0.734940\tBest loss: 0.261468\tAccuracy: 80.41%\n",
      "4\tValidation loss: 0.545663\tBest loss: 0.261468\tAccuracy: 80.88%\n",
      "5\tValidation loss: 0.466796\tBest loss: 0.261468\tAccuracy: 84.79%\n",
      "6\tValidation loss: 0.522361\tBest loss: 0.261468\tAccuracy: 77.80%\n",
      "7\tValidation loss: 0.389447\tBest loss: 0.261468\tAccuracy: 85.73%\n",
      "8\tValidation loss: 0.319059\tBest loss: 0.261468\tAccuracy: 88.86%\n",
      "9\tValidation loss: 0.271565\tBest loss: 0.261468\tAccuracy: 91.09%\n",
      "10\tValidation loss: 0.289671\tBest loss: 0.261468\tAccuracy: 90.42%\n",
      "11\tValidation loss: 0.249145\tBest loss: 0.249145\tAccuracy: 92.61%\n",
      "12\tValidation loss: 0.213052\tBest loss: 0.213052\tAccuracy: 93.51%\n",
      "13\tValidation loss: 0.215579\tBest loss: 0.213052\tAccuracy: 93.59%\n",
      "14\tValidation loss: 0.193836\tBest loss: 0.193836\tAccuracy: 94.06%\n",
      "15\tValidation loss: 0.204822\tBest loss: 0.193836\tAccuracy: 93.43%\n",
      "16\tValidation loss: 0.179069\tBest loss: 0.179069\tAccuracy: 94.57%\n",
      "17\tValidation loss: 0.236857\tBest loss: 0.179069\tAccuracy: 93.35%\n",
      "18\tValidation loss: 0.166303\tBest loss: 0.166303\tAccuracy: 95.15%\n",
      "19\tValidation loss: 0.172111\tBest loss: 0.166303\tAccuracy: 95.11%\n",
      "20\tValidation loss: 0.186967\tBest loss: 0.166303\tAccuracy: 94.45%\n",
      "21\tValidation loss: 0.149736\tBest loss: 0.149736\tAccuracy: 95.74%\n",
      "22\tValidation loss: 0.171679\tBest loss: 0.149736\tAccuracy: 94.88%\n",
      "23\tValidation loss: 0.245850\tBest loss: 0.149736\tAccuracy: 94.68%\n",
      "24\tValidation loss: 0.164604\tBest loss: 0.149736\tAccuracy: 94.92%\n",
      "25\tValidation loss: 0.180849\tBest loss: 0.149736\tAccuracy: 94.76%\n",
      "26\tValidation loss: 0.273839\tBest loss: 0.149736\tAccuracy: 93.51%\n",
      "27\tValidation loss: 0.162228\tBest loss: 0.149736\tAccuracy: 95.00%\n",
      "28\tValidation loss: 0.154569\tBest loss: 0.149736\tAccuracy: 95.66%\n",
      "29\tValidation loss: 0.139718\tBest loss: 0.139718\tAccuracy: 96.21%\n",
      "30\tValidation loss: 0.163619\tBest loss: 0.139718\tAccuracy: 95.39%\n",
      "31\tValidation loss: 79.067963\tBest loss: 0.139718\tAccuracy: 29.12%\n",
      "32\tValidation loss: 4.086567\tBest loss: 0.139718\tAccuracy: 65.99%\n",
      "33\tValidation loss: 2.473543\tBest loss: 0.139718\tAccuracy: 62.63%\n",
      "34\tValidation loss: 1.673393\tBest loss: 0.139718\tAccuracy: 72.95%\n",
      "35\tValidation loss: 2.465292\tBest loss: 0.139718\tAccuracy: 61.69%\n",
      "36\tValidation loss: 1.979463\tBest loss: 0.139718\tAccuracy: 64.39%\n",
      "37\tValidation loss: 1.488093\tBest loss: 0.139718\tAccuracy: 71.42%\n",
      "38\tValidation loss: 1.523492\tBest loss: 0.139718\tAccuracy: 71.66%\n",
      "39\tValidation loss: 1.609838\tBest loss: 0.139718\tAccuracy: 70.84%\n",
      "40\tValidation loss: 1.824662\tBest loss: 0.139718\tAccuracy: 70.25%\n",
      "41\tValidation loss: 3.141187\tBest loss: 0.139718\tAccuracy: 43.08%\n",
      "42\tValidation loss: 1.168754\tBest loss: 0.139718\tAccuracy: 76.08%\n",
      "43\tValidation loss: 1.214164\tBest loss: 0.139718\tAccuracy: 77.29%\n",
      "44\tValidation loss: 1.808583\tBest loss: 0.139718\tAccuracy: 65.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\tValidation loss: 1.847732\tBest loss: 0.139718\tAccuracy: 68.18%\n",
      "46\tValidation loss: 2.453926\tBest loss: 0.139718\tAccuracy: 53.40%\n",
      "47\tValidation loss: 1.014692\tBest loss: 0.139718\tAccuracy: 78.69%\n",
      "48\tValidation loss: 0.807720\tBest loss: 0.139718\tAccuracy: 80.26%\n",
      "49\tValidation loss: 0.818569\tBest loss: 0.139718\tAccuracy: 77.21%\n",
      "50\tValidation loss: 0.743114\tBest loss: 0.139718\tAccuracy: 79.95%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  43.8s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 1.171303\tBest loss: 1.171303\tAccuracy: 82.76%\n",
      "1\tValidation loss: 1.483471\tBest loss: 1.171303\tAccuracy: 62.82%\n",
      "2\tValidation loss: 0.529090\tBest loss: 0.529090\tAccuracy: 82.33%\n",
      "3\tValidation loss: 0.442886\tBest loss: 0.442886\tAccuracy: 84.36%\n",
      "4\tValidation loss: 0.394027\tBest loss: 0.394027\tAccuracy: 86.51%\n",
      "5\tValidation loss: 0.303229\tBest loss: 0.303229\tAccuracy: 89.95%\n",
      "6\tValidation loss: 0.303782\tBest loss: 0.303229\tAccuracy: 90.58%\n",
      "7\tValidation loss: 0.261068\tBest loss: 0.261068\tAccuracy: 92.57%\n",
      "8\tValidation loss: 0.288500\tBest loss: 0.261068\tAccuracy: 91.01%\n",
      "9\tValidation loss: 0.306157\tBest loss: 0.261068\tAccuracy: 92.38%\n",
      "10\tValidation loss: 0.308907\tBest loss: 0.261068\tAccuracy: 93.67%\n",
      "11\tValidation loss: 319.091675\tBest loss: 0.261068\tAccuracy: 20.91%\n",
      "12\tValidation loss: 9.391209\tBest loss: 0.261068\tAccuracy: 38.78%\n",
      "13\tValidation loss: 4.857709\tBest loss: 0.261068\tAccuracy: 43.82%\n",
      "14\tValidation loss: 15.131380\tBest loss: 0.261068\tAccuracy: 42.18%\n",
      "15\tValidation loss: 2.922645\tBest loss: 0.261068\tAccuracy: 53.40%\n",
      "16\tValidation loss: 1.963021\tBest loss: 0.261068\tAccuracy: 67.63%\n",
      "17\tValidation loss: 3.730367\tBest loss: 0.261068\tAccuracy: 50.98%\n",
      "18\tValidation loss: 1.821167\tBest loss: 0.261068\tAccuracy: 68.14%\n",
      "19\tValidation loss: 1.442904\tBest loss: 0.261068\tAccuracy: 78.89%\n",
      "20\tValidation loss: 2.742723\tBest loss: 0.261068\tAccuracy: 64.39%\n",
      "21\tValidation loss: 1.634990\tBest loss: 0.261068\tAccuracy: 76.08%\n",
      "22\tValidation loss: 1.266895\tBest loss: 0.261068\tAccuracy: 78.73%\n",
      "23\tValidation loss: 1.758556\tBest loss: 0.261068\tAccuracy: 75.41%\n",
      "24\tValidation loss: 1.483691\tBest loss: 0.261068\tAccuracy: 85.03%\n",
      "25\tValidation loss: 1.019931\tBest loss: 0.261068\tAccuracy: 84.79%\n",
      "26\tValidation loss: 1.657514\tBest loss: 0.261068\tAccuracy: 79.32%\n",
      "27\tValidation loss: 1.173990\tBest loss: 0.261068\tAccuracy: 83.23%\n",
      "28\tValidation loss: 1.255277\tBest loss: 0.261068\tAccuracy: 84.21%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  25.4s\n",
      "[CV] n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158> \n",
      "0\tValidation loss: 0.440570\tBest loss: 0.440570\tAccuracy: 76.78%\n",
      "1\tValidation loss: 8.650978\tBest loss: 0.440570\tAccuracy: 26.15%\n",
      "2\tValidation loss: 4.608110\tBest loss: 0.440570\tAccuracy: 23.14%\n",
      "3\tValidation loss: 9.573427\tBest loss: 0.440570\tAccuracy: 27.25%\n",
      "4\tValidation loss: 1.999298\tBest loss: 0.440570\tAccuracy: 39.95%\n",
      "5\tValidation loss: 1.459538\tBest loss: 0.440570\tAccuracy: 42.92%\n",
      "6\tValidation loss: 1.434055\tBest loss: 0.440570\tAccuracy: 44.68%\n",
      "7\tValidation loss: 1.286995\tBest loss: 0.440570\tAccuracy: 46.13%\n",
      "8\tValidation loss: 1.330514\tBest loss: 0.440570\tAccuracy: 55.20%\n",
      "9\tValidation loss: 0.945036\tBest loss: 0.440570\tAccuracy: 66.46%\n",
      "10\tValidation loss: 0.878962\tBest loss: 0.440570\tAccuracy: 68.45%\n",
      "11\tValidation loss: 0.897051\tBest loss: 0.440570\tAccuracy: 67.90%\n",
      "12\tValidation loss: 0.610218\tBest loss: 0.440570\tAccuracy: 80.18%\n",
      "13\tValidation loss: 0.759499\tBest loss: 0.440570\tAccuracy: 71.85%\n",
      "14\tValidation loss: 0.532662\tBest loss: 0.440570\tAccuracy: 81.35%\n",
      "15\tValidation loss: 0.481329\tBest loss: 0.440570\tAccuracy: 85.03%\n",
      "16\tValidation loss: 0.464332\tBest loss: 0.440570\tAccuracy: 84.32%\n",
      "17\tValidation loss: 0.412916\tBest loss: 0.412916\tAccuracy: 86.55%\n",
      "18\tValidation loss: 0.486458\tBest loss: 0.412916\tAccuracy: 84.01%\n",
      "19\tValidation loss: 0.387033\tBest loss: 0.387033\tAccuracy: 87.53%\n",
      "20\tValidation loss: 1536.039429\tBest loss: 0.387033\tAccuracy: 19.27%\n",
      "21\tValidation loss: 15.464260\tBest loss: 0.387033\tAccuracy: 43.67%\n",
      "22\tValidation loss: 2.873704\tBest loss: 0.387033\tAccuracy: 53.05%\n",
      "23\tValidation loss: 3.592885\tBest loss: 0.387033\tAccuracy: 52.42%\n",
      "24\tValidation loss: 2.970427\tBest loss: 0.387033\tAccuracy: 50.94%\n",
      "25\tValidation loss: 3.366452\tBest loss: 0.387033\tAccuracy: 43.39%\n",
      "26\tValidation loss: 2.564163\tBest loss: 0.387033\tAccuracy: 57.04%\n",
      "27\tValidation loss: 2.502818\tBest loss: 0.387033\tAccuracy: 52.97%\n",
      "28\tValidation loss: 2.113280\tBest loss: 0.387033\tAccuracy: 55.16%\n",
      "29\tValidation loss: 2.466887\tBest loss: 0.387033\tAccuracy: 52.58%\n",
      "30\tValidation loss: 1.856460\tBest loss: 0.387033\tAccuracy: 54.53%\n",
      "31\tValidation loss: 1.798836\tBest loss: 0.387033\tAccuracy: 63.76%\n",
      "32\tValidation loss: 2.422352\tBest loss: 0.387033\tAccuracy: 45.19%\n",
      "33\tValidation loss: 1.278139\tBest loss: 0.387033\tAccuracy: 68.65%\n",
      "34\tValidation loss: 1.142493\tBest loss: 0.387033\tAccuracy: 72.40%\n",
      "35\tValidation loss: 534.802979\tBest loss: 0.387033\tAccuracy: 24.00%\n",
      "36\tValidation loss: 324.609741\tBest loss: 0.387033\tAccuracy: 44.02%\n",
      "37\tValidation loss: 102.136467\tBest loss: 0.387033\tAccuracy: 47.03%\n",
      "38\tValidation loss: 6.397295\tBest loss: 0.387033\tAccuracy: 78.19%\n",
      "39\tValidation loss: 7.621279\tBest loss: 0.387033\tAccuracy: 74.32%\n",
      "40\tValidation loss: 5.547560\tBest loss: 0.387033\tAccuracy: 83.62%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=50, learning_rate=0.05, batch_size=100, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>, total=  36.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 20.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.082453\tBest loss: 0.082453\tAccuracy: 97.46%\n",
      "1\tValidation loss: 0.068512\tBest loss: 0.068512\tAccuracy: 97.85%\n",
      "2\tValidation loss: 0.055776\tBest loss: 0.055776\tAccuracy: 98.32%\n",
      "3\tValidation loss: 0.053412\tBest loss: 0.053412\tAccuracy: 98.55%\n",
      "4\tValidation loss: 0.047264\tBest loss: 0.047264\tAccuracy: 98.79%\n",
      "5\tValidation loss: 0.048654\tBest loss: 0.047264\tAccuracy: 98.59%\n",
      "6\tValidation loss: 0.043234\tBest loss: 0.043234\tAccuracy: 98.91%\n",
      "7\tValidation loss: 0.042582\tBest loss: 0.042582\tAccuracy: 98.59%\n",
      "8\tValidation loss: 0.047947\tBest loss: 0.042582\tAccuracy: 98.83%\n",
      "9\tValidation loss: 0.025608\tBest loss: 0.025608\tAccuracy: 99.18%\n",
      "10\tValidation loss: 0.037706\tBest loss: 0.025608\tAccuracy: 98.91%\n",
      "11\tValidation loss: 0.043738\tBest loss: 0.025608\tAccuracy: 98.91%\n",
      "12\tValidation loss: 0.038992\tBest loss: 0.025608\tAccuracy: 99.02%\n",
      "13\tValidation loss: 0.044022\tBest loss: 0.025608\tAccuracy: 98.87%\n",
      "14\tValidation loss: 0.037122\tBest loss: 0.025608\tAccuracy: 99.10%\n",
      "15\tValidation loss: 0.093689\tBest loss: 0.025608\tAccuracy: 98.67%\n",
      "16\tValidation loss: 0.063811\tBest loss: 0.025608\tAccuracy: 98.91%\n",
      "17\tValidation loss: 0.047150\tBest loss: 0.025608\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.035529\tBest loss: 0.025608\tAccuracy: 99.26%\n",
      "19\tValidation loss: 0.051811\tBest loss: 0.025608\tAccuracy: 98.67%\n",
      "20\tValidation loss: 0.045480\tBest loss: 0.025608\tAccuracy: 98.94%\n",
      "21\tValidation loss: 0.039727\tBest loss: 0.025608\tAccuracy: 99.02%\n",
      "22\tValidation loss: 0.046786\tBest loss: 0.025608\tAccuracy: 99.18%\n",
      "23\tValidation loss: 0.044062\tBest loss: 0.025608\tAccuracy: 98.98%\n",
      "24\tValidation loss: 0.041506\tBest loss: 0.025608\tAccuracy: 99.18%\n",
      "25\tValidation loss: 0.064966\tBest loss: 0.025608\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.042941\tBest loss: 0.025608\tAccuracy: 99.02%\n",
      "27\tValidation loss: 0.050579\tBest loss: 0.025608\tAccuracy: 98.94%\n",
      "28\tValidation loss: 0.059657\tBest loss: 0.025608\tAccuracy: 98.94%\n",
      "29\tValidation loss: 0.043723\tBest loss: 0.025608\tAccuracy: 99.10%\n",
      "30\tValidation loss: 0.084153\tBest loss: 0.025608\tAccuracy: 98.63%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=DNNClassifier(activation=<function elu at 0x114248840>,\n",
       "                                           batch_norm_momentum=None,\n",
       "                                           batch_size=20, dropout_rate=None,\n",
       "                                           initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "                                           learning_rate=0.01,\n",
       "                                           n_hidden_layers=5, n_neurons=100,\n",
       "                                           optimizer_class=<class 'tensorflow.python.tra...\n",
       "                                                       <function elu at 0x114248840>,\n",
       "                                                       <function leaky_relu.<locals>.parametrized_leaky_relu at 0x121e30158>,\n",
       "                                                       <function leaky_relu.<locals>.parametrized_leaky_relu at 0x12332ed90>],\n",
       "                                        'batch_size': [10, 50, 100, 500],\n",
       "                                        'learning_rate': [0.01, 0.02, 0.05,\n",
       "                                                          0.1],\n",
       "                                        'n_neurons': [10, 30, 50, 70, 90, 100,\n",
       "                                                      120, 140, 160]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def leaky_relu(alpha=0.01): # definition of Leaky ReLU\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "param_distribs = {\"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "                  \"batch_size\": [10, 50, 100, 500],\n",
    "                  \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "                  \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)]}\n",
    "                  # You could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "                  # \"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "                  # \"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)]\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs,\n",
    "                                n_iter=10, cv=3, random_state=42, verbose=2) # default: n_iter=50\n",
    "rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 140,\n",
       " 'learning_rate': 0.01,\n",
       " 'batch_size': 500,\n",
       " 'activation': <function tensorflow.python.ops.gen_nn_ops.elu(features, name=None)>}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the accuracy on the test set and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9908542517999611\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnd_search.predict(X_test1)\n",
    "print(accuracy_score(y_test1, y_pred))\n",
    "rnd_search.best_estimator_.save(\"./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.081199\tBest loss: 0.081199\tAccuracy: 97.54%\n",
      "1\tValidation loss: 0.050252\tBest loss: 0.050252\tAccuracy: 98.44%\n",
      "2\tValidation loss: 0.045627\tBest loss: 0.045627\tAccuracy: 98.79%\n",
      "3\tValidation loss: 0.050210\tBest loss: 0.045627\tAccuracy: 98.28%\n",
      "4\tValidation loss: 0.040341\tBest loss: 0.040341\tAccuracy: 98.71%\n",
      "5\tValidation loss: 0.057163\tBest loss: 0.040341\tAccuracy: 98.67%\n",
      "6\tValidation loss: 0.038779\tBest loss: 0.038779\tAccuracy: 98.71%\n",
      "7\tValidation loss: 0.040396\tBest loss: 0.038779\tAccuracy: 98.83%\n",
      "8\tValidation loss: 0.050711\tBest loss: 0.038779\tAccuracy: 98.83%\n",
      "9\tValidation loss: 0.054118\tBest loss: 0.038779\tAccuracy: 98.71%\n",
      "10\tValidation loss: 0.039527\tBest loss: 0.038779\tAccuracy: 98.94%\n",
      "11\tValidation loss: 0.046451\tBest loss: 0.038779\tAccuracy: 98.87%\n",
      "12\tValidation loss: 0.034430\tBest loss: 0.034430\tAccuracy: 99.06%\n",
      "13\tValidation loss: 0.046117\tBest loss: 0.034430\tAccuracy: 98.98%\n",
      "14\tValidation loss: 0.043815\tBest loss: 0.034430\tAccuracy: 98.94%\n",
      "15\tValidation loss: 0.059159\tBest loss: 0.034430\tAccuracy: 98.83%\n",
      "16\tValidation loss: 0.066488\tBest loss: 0.034430\tAccuracy: 99.06%\n",
      "17\tValidation loss: 0.063479\tBest loss: 0.034430\tAccuracy: 98.75%\n",
      "18\tValidation loss: 0.065873\tBest loss: 0.034430\tAccuracy: 98.75%\n",
      "19\tValidation loss: 58.318413\tBest loss: 0.034430\tAccuracy: 38.70%\n",
      "20\tValidation loss: 0.953413\tBest loss: 0.034430\tAccuracy: 95.97%\n",
      "21\tValidation loss: 0.326758\tBest loss: 0.034430\tAccuracy: 96.99%\n",
      "22\tValidation loss: 0.256935\tBest loss: 0.034430\tAccuracy: 97.30%\n",
      "23\tValidation loss: 0.221039\tBest loss: 0.034430\tAccuracy: 97.81%\n",
      "24\tValidation loss: 0.229534\tBest loss: 0.034430\tAccuracy: 96.72%\n",
      "25\tValidation loss: 0.246314\tBest loss: 0.034430\tAccuracy: 97.65%\n",
      "26\tValidation loss: 0.200308\tBest loss: 0.034430\tAccuracy: 98.05%\n",
      "27\tValidation loss: 0.221914\tBest loss: 0.034430\tAccuracy: 98.05%\n",
      "28\tValidation loss: 0.191674\tBest loss: 0.034430\tAccuracy: 98.16%\n",
      "29\tValidation loss: 0.207982\tBest loss: 0.034430\tAccuracy: 97.93%\n",
      "30\tValidation loss: 0.193499\tBest loss: 0.034430\tAccuracy: 98.12%\n",
      "31\tValidation loss: 0.220170\tBest loss: 0.034430\tAccuracy: 98.05%\n",
      "32\tValidation loss: 0.189725\tBest loss: 0.034430\tAccuracy: 98.28%\n",
      "33\tValidation loss: 0.197884\tBest loss: 0.034430\tAccuracy: 98.40%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x12593f8c8>,\n",
       "              batch_norm_momentum=None, batch_size=500, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=140,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and get the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933839268340144"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add batch normalization. Reminder: The `batch_norm_momentum` hyperparameter is used to calculate the running averages of the input features' mean and standard deviation. (During training, the average mean and standard deviation can be computed rightaway for each batch. But for inference, there are no \"average\" mean and standard deviation for a single instance, so  the running averages that have been kept track of during training are used, there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.045022\tBest loss: 0.045022\tAccuracy: 98.83%\n",
      "1\tValidation loss: 0.042978\tBest loss: 0.042978\tAccuracy: 98.67%\n",
      "2\tValidation loss: 0.039541\tBest loss: 0.039541\tAccuracy: 98.67%\n",
      "3\tValidation loss: 0.044707\tBest loss: 0.039541\tAccuracy: 98.59%\n",
      "4\tValidation loss: 0.049904\tBest loss: 0.039541\tAccuracy: 98.71%\n",
      "5\tValidation loss: 0.030305\tBest loss: 0.030305\tAccuracy: 98.94%\n",
      "6\tValidation loss: 0.044365\tBest loss: 0.030305\tAccuracy: 98.75%\n",
      "7\tValidation loss: 0.030341\tBest loss: 0.030305\tAccuracy: 99.10%\n",
      "8\tValidation loss: 0.039531\tBest loss: 0.030305\tAccuracy: 98.75%\n",
      "9\tValidation loss: 0.047017\tBest loss: 0.030305\tAccuracy: 98.71%\n",
      "10\tValidation loss: 0.031616\tBest loss: 0.030305\tAccuracy: 98.94%\n",
      "11\tValidation loss: 0.040761\tBest loss: 0.030305\tAccuracy: 98.98%\n",
      "12\tValidation loss: 0.035022\tBest loss: 0.030305\tAccuracy: 98.83%\n",
      "13\tValidation loss: 0.030176\tBest loss: 0.030176\tAccuracy: 99.30%\n",
      "14\tValidation loss: 0.049001\tBest loss: 0.030176\tAccuracy: 98.71%\n",
      "15\tValidation loss: 0.035036\tBest loss: 0.030176\tAccuracy: 99.18%\n",
      "16\tValidation loss: 0.031799\tBest loss: 0.030176\tAccuracy: 99.06%\n",
      "17\tValidation loss: 0.026485\tBest loss: 0.026485\tAccuracy: 99.34%\n",
      "18\tValidation loss: 0.032115\tBest loss: 0.026485\tAccuracy: 98.98%\n",
      "19\tValidation loss: 0.040251\tBest loss: 0.026485\tAccuracy: 99.02%\n",
      "20\tValidation loss: 0.037589\tBest loss: 0.026485\tAccuracy: 99.22%\n",
      "21\tValidation loss: 0.036594\tBest loss: 0.026485\tAccuracy: 99.26%\n",
      "22\tValidation loss: 0.026604\tBest loss: 0.026485\tAccuracy: 99.34%\n",
      "23\tValidation loss: 0.034518\tBest loss: 0.026485\tAccuracy: 99.22%\n",
      "24\tValidation loss: 0.032586\tBest loss: 0.026485\tAccuracy: 99.26%\n",
      "25\tValidation loss: 0.036867\tBest loss: 0.026485\tAccuracy: 99.06%\n",
      "26\tValidation loss: 0.034030\tBest loss: 0.026485\tAccuracy: 99.34%\n",
      "27\tValidation loss: 0.040952\tBest loss: 0.026485\tAccuracy: 99.22%\n",
      "28\tValidation loss: 0.074211\tBest loss: 0.026485\tAccuracy: 98.44%\n",
      "29\tValidation loss: 0.050602\tBest loss: 0.026485\tAccuracy: 98.98%\n",
      "30\tValidation loss: 0.038048\tBest loss: 0.026485\tAccuracy: 99.22%\n",
      "31\tValidation loss: 0.022793\tBest loss: 0.022793\tAccuracy: 99.45%\n",
      "32\tValidation loss: 0.032076\tBest loss: 0.022793\tAccuracy: 99.34%\n",
      "33\tValidation loss: 0.034298\tBest loss: 0.022793\tAccuracy: 99.18%\n",
      "34\tValidation loss: 0.039964\tBest loss: 0.022793\tAccuracy: 99.02%\n",
      "35\tValidation loss: 0.052146\tBest loss: 0.022793\tAccuracy: 99.02%\n",
      "36\tValidation loss: 0.038579\tBest loss: 0.022793\tAccuracy: 98.98%\n",
      "37\tValidation loss: 0.033443\tBest loss: 0.022793\tAccuracy: 99.26%\n",
      "38\tValidation loss: 0.040468\tBest loss: 0.022793\tAccuracy: 99.10%\n",
      "39\tValidation loss: 0.044699\tBest loss: 0.022793\tAccuracy: 99.22%\n",
      "40\tValidation loss: 0.048886\tBest loss: 0.022793\tAccuracy: 98.94%\n",
      "41\tValidation loss: 0.022517\tBest loss: 0.022517\tAccuracy: 99.30%\n",
      "42\tValidation loss: 0.041568\tBest loss: 0.022517\tAccuracy: 99.02%\n",
      "43\tValidation loss: 0.036382\tBest loss: 0.022517\tAccuracy: 99.18%\n",
      "44\tValidation loss: 0.034551\tBest loss: 0.022517\tAccuracy: 99.37%\n",
      "45\tValidation loss: 0.040824\tBest loss: 0.022517\tAccuracy: 99.30%\n",
      "46\tValidation loss: 0.028170\tBest loss: 0.022517\tAccuracy: 99.41%\n",
      "47\tValidation loss: 0.046156\tBest loss: 0.022517\tAccuracy: 99.02%\n",
      "48\tValidation loss: 0.039088\tBest loss: 0.022517\tAccuracy: 99.26%\n",
      "49\tValidation loss: 0.046562\tBest loss: 0.022517\tAccuracy: 99.18%\n",
      "50\tValidation loss: 0.039854\tBest loss: 0.022517\tAccuracy: 99.10%\n",
      "51\tValidation loss: 0.040495\tBest loss: 0.022517\tAccuracy: 99.14%\n",
      "52\tValidation loss: 0.032986\tBest loss: 0.022517\tAccuracy: 99.41%\n",
      "53\tValidation loss: 0.037743\tBest loss: 0.022517\tAccuracy: 99.34%\n",
      "54\tValidation loss: 0.038275\tBest loss: 0.022517\tAccuracy: 99.30%\n",
      "55\tValidation loss: 0.034192\tBest loss: 0.022517\tAccuracy: 99.22%\n",
      "56\tValidation loss: 0.040358\tBest loss: 0.022517\tAccuracy: 99.34%\n",
      "57\tValidation loss: 0.039574\tBest loss: 0.022517\tAccuracy: 99.02%\n",
      "58\tValidation loss: 0.044493\tBest loss: 0.022517\tAccuracy: 99.02%\n",
      "59\tValidation loss: 0.039697\tBest loss: 0.022517\tAccuracy: 99.14%\n",
      "60\tValidation loss: 0.053170\tBest loss: 0.022517\tAccuracy: 99.02%\n",
      "61\tValidation loss: 0.055180\tBest loss: 0.022517\tAccuracy: 98.98%\n",
      "62\tValidation loss: 0.052771\tBest loss: 0.022517\tAccuracy: 99.06%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x1292b6e18>,\n",
       "              batch_norm_momentum=0.95, batch_size=500, dropout_rate=None,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42,\n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912434325744308"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite well. Let's start the hyperparameter search! Now, this includes the new `batch_norm_momentum` hyperparameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.246385\tBest loss: 0.246385\tAccuracy: 93.24%\n",
      "1\tValidation loss: 0.120125\tBest loss: 0.120125\tAccuracy: 96.09%\n",
      "2\tValidation loss: 0.098414\tBest loss: 0.098414\tAccuracy: 97.03%\n",
      "3\tValidation loss: 0.081643\tBest loss: 0.081643\tAccuracy: 97.22%\n",
      "4\tValidation loss: 0.074249\tBest loss: 0.074249\tAccuracy: 97.89%\n",
      "5\tValidation loss: 0.104024\tBest loss: 0.074249\tAccuracy: 97.07%\n",
      "6\tValidation loss: 0.087593\tBest loss: 0.074249\tAccuracy: 97.46%\n",
      "7\tValidation loss: 0.073051\tBest loss: 0.073051\tAccuracy: 98.05%\n",
      "8\tValidation loss: 0.067671\tBest loss: 0.067671\tAccuracy: 98.01%\n",
      "9\tValidation loss: 0.053383\tBest loss: 0.053383\tAccuracy: 98.48%\n",
      "10\tValidation loss: 0.061203\tBest loss: 0.053383\tAccuracy: 98.59%\n",
      "11\tValidation loss: 0.050658\tBest loss: 0.050658\tAccuracy: 98.75%\n",
      "12\tValidation loss: 0.047744\tBest loss: 0.047744\tAccuracy: 98.71%\n",
      "13\tValidation loss: 0.049029\tBest loss: 0.047744\tAccuracy: 98.51%\n",
      "14\tValidation loss: 0.063525\tBest loss: 0.047744\tAccuracy: 98.40%\n",
      "15\tValidation loss: 0.079776\tBest loss: 0.047744\tAccuracy: 98.05%\n",
      "16\tValidation loss: 0.060221\tBest loss: 0.047744\tAccuracy: 98.59%\n",
      "17\tValidation loss: 0.066807\tBest loss: 0.047744\tAccuracy: 98.48%\n",
      "18\tValidation loss: 0.077709\tBest loss: 0.047744\tAccuracy: 98.24%\n",
      "19\tValidation loss: 0.074256\tBest loss: 0.047744\tAccuracy: 98.40%\n",
      "20\tValidation loss: 0.056676\tBest loss: 0.047744\tAccuracy: 98.55%\n",
      "21\tValidation loss: 0.070077\tBest loss: 0.047744\tAccuracy: 98.55%\n",
      "22\tValidation loss: 0.068155\tBest loss: 0.047744\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.065706\tBest loss: 0.047744\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.074057\tBest loss: 0.047744\tAccuracy: 98.51%\n",
      "25\tValidation loss: 0.069876\tBest loss: 0.047744\tAccuracy: 98.75%\n",
      "26\tValidation loss: 0.111027\tBest loss: 0.047744\tAccuracy: 98.05%\n",
      "27\tValidation loss: 0.092817\tBest loss: 0.047744\tAccuracy: 98.20%\n",
      "28\tValidation loss: 0.088215\tBest loss: 0.047744\tAccuracy: 98.12%\n",
      "29\tValidation loss: 0.073731\tBest loss: 0.047744\tAccuracy: 98.79%\n",
      "30\tValidation loss: 0.081745\tBest loss: 0.047744\tAccuracy: 98.55%\n",
      "31\tValidation loss: 0.085657\tBest loss: 0.047744\tAccuracy: 98.44%\n",
      "32\tValidation loss: 0.075316\tBest loss: 0.047744\tAccuracy: 98.83%\n",
      "33\tValidation loss: 0.082059\tBest loss: 0.047744\tAccuracy: 98.71%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=<function elu at 0x114248840>, total= 1.6min\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.166692\tBest loss: 0.166692\tAccuracy: 95.15%\n",
      "1\tValidation loss: 0.118710\tBest loss: 0.118710\tAccuracy: 97.03%\n",
      "2\tValidation loss: 0.078874\tBest loss: 0.078874\tAccuracy: 97.65%\n",
      "3\tValidation loss: 0.081918\tBest loss: 0.078874\tAccuracy: 97.65%\n",
      "4\tValidation loss: 0.080409\tBest loss: 0.078874\tAccuracy: 97.85%\n",
      "5\tValidation loss: 0.075155\tBest loss: 0.075155\tAccuracy: 97.62%\n",
      "6\tValidation loss: 0.067110\tBest loss: 0.067110\tAccuracy: 98.12%\n",
      "7\tValidation loss: 0.069453\tBest loss: 0.067110\tAccuracy: 97.85%\n",
      "8\tValidation loss: 0.083360\tBest loss: 0.067110\tAccuracy: 97.85%\n",
      "9\tValidation loss: 0.084008\tBest loss: 0.067110\tAccuracy: 98.05%\n",
      "10\tValidation loss: 0.106929\tBest loss: 0.067110\tAccuracy: 97.54%\n",
      "11\tValidation loss: 0.065448\tBest loss: 0.065448\tAccuracy: 98.44%\n",
      "12\tValidation loss: 0.077003\tBest loss: 0.065448\tAccuracy: 98.44%\n",
      "13\tValidation loss: 0.073125\tBest loss: 0.065448\tAccuracy: 98.12%\n",
      "14\tValidation loss: 0.078837\tBest loss: 0.065448\tAccuracy: 98.32%\n",
      "15\tValidation loss: 0.089116\tBest loss: 0.065448\tAccuracy: 98.24%\n",
      "16\tValidation loss: 0.069582\tBest loss: 0.065448\tAccuracy: 98.63%\n",
      "17\tValidation loss: 0.091682\tBest loss: 0.065448\tAccuracy: 98.36%\n",
      "18\tValidation loss: 0.111035\tBest loss: 0.065448\tAccuracy: 97.54%\n",
      "19\tValidation loss: 0.087013\tBest loss: 0.065448\tAccuracy: 98.48%\n",
      "20\tValidation loss: 0.087471\tBest loss: 0.065448\tAccuracy: 98.44%\n",
      "21\tValidation loss: 0.069746\tBest loss: 0.065448\tAccuracy: 98.59%\n",
      "22\tValidation loss: 0.094722\tBest loss: 0.065448\tAccuracy: 98.59%\n",
      "23\tValidation loss: 0.083563\tBest loss: 0.065448\tAccuracy: 98.44%\n",
      "24\tValidation loss: 0.067169\tBest loss: 0.065448\tAccuracy: 98.36%\n",
      "25\tValidation loss: 0.064494\tBest loss: 0.064494\tAccuracy: 98.98%\n",
      "26\tValidation loss: 0.070078\tBest loss: 0.064494\tAccuracy: 98.79%\n",
      "27\tValidation loss: 0.084688\tBest loss: 0.064494\tAccuracy: 98.59%\n",
      "28\tValidation loss: 0.074474\tBest loss: 0.064494\tAccuracy: 98.87%\n",
      "29\tValidation loss: 0.079533\tBest loss: 0.064494\tAccuracy: 98.51%\n",
      "30\tValidation loss: 0.079808\tBest loss: 0.064494\tAccuracy: 98.55%\n",
      "31\tValidation loss: 0.099199\tBest loss: 0.064494\tAccuracy: 98.24%\n",
      "32\tValidation loss: 0.079329\tBest loss: 0.064494\tAccuracy: 98.71%\n",
      "33\tValidation loss: 0.099028\tBest loss: 0.064494\tAccuracy: 98.40%\n",
      "34\tValidation loss: 0.080903\tBest loss: 0.064494\tAccuracy: 98.67%\n",
      "35\tValidation loss: 0.075972\tBest loss: 0.064494\tAccuracy: 98.67%\n",
      "36\tValidation loss: 0.087708\tBest loss: 0.064494\tAccuracy: 98.36%\n",
      "37\tValidation loss: 0.099632\tBest loss: 0.064494\tAccuracy: 98.08%\n",
      "38\tValidation loss: 0.092446\tBest loss: 0.064494\tAccuracy: 98.79%\n",
      "39\tValidation loss: 0.081735\tBest loss: 0.064494\tAccuracy: 98.87%\n",
      "40\tValidation loss: 0.082936\tBest loss: 0.064494\tAccuracy: 98.87%\n",
      "41\tValidation loss: 0.077961\tBest loss: 0.064494\tAccuracy: 98.83%\n",
      "42\tValidation loss: 0.085506\tBest loss: 0.064494\tAccuracy: 98.55%\n",
      "43\tValidation loss: 0.104353\tBest loss: 0.064494\tAccuracy: 98.55%\n",
      "44\tValidation loss: 0.086365\tBest loss: 0.064494\tAccuracy: 98.63%\n",
      "45\tValidation loss: 0.095109\tBest loss: 0.064494\tAccuracy: 98.75%\n",
      "46\tValidation loss: 0.071803\tBest loss: 0.064494\tAccuracy: 98.83%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=<function elu at 0x114248840>, total= 2.4min\n",
      "[CV] n_neurons=100, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.198589\tBest loss: 0.198589\tAccuracy: 94.21%\n",
      "1\tValidation loss: 0.105398\tBest loss: 0.105398\tAccuracy: 96.44%\n",
      "2\tValidation loss: 0.089114\tBest loss: 0.089114\tAccuracy: 96.99%\n",
      "3\tValidation loss: 0.080140\tBest loss: 0.080140\tAccuracy: 97.58%\n",
      "4\tValidation loss: 0.081436\tBest loss: 0.080140\tAccuracy: 97.73%\n",
      "5\tValidation loss: 0.076460\tBest loss: 0.076460\tAccuracy: 97.58%\n",
      "6\tValidation loss: 0.058670\tBest loss: 0.058670\tAccuracy: 98.32%\n",
      "7\tValidation loss: 0.055948\tBest loss: 0.055948\tAccuracy: 98.36%\n",
      "8\tValidation loss: 0.064117\tBest loss: 0.055948\tAccuracy: 98.24%\n",
      "9\tValidation loss: 0.049358\tBest loss: 0.049358\tAccuracy: 98.67%\n",
      "10\tValidation loss: 0.056293\tBest loss: 0.049358\tAccuracy: 98.40%\n",
      "11\tValidation loss: 0.061390\tBest loss: 0.049358\tAccuracy: 98.32%\n",
      "12\tValidation loss: 0.073481\tBest loss: 0.049358\tAccuracy: 98.20%\n",
      "13\tValidation loss: 0.075533\tBest loss: 0.049358\tAccuracy: 98.32%\n",
      "14\tValidation loss: 0.103358\tBest loss: 0.049358\tAccuracy: 97.22%\n",
      "15\tValidation loss: 0.065816\tBest loss: 0.049358\tAccuracy: 98.59%\n",
      "16\tValidation loss: 0.054417\tBest loss: 0.049358\tAccuracy: 98.83%\n",
      "17\tValidation loss: 0.057702\tBest loss: 0.049358\tAccuracy: 98.63%\n",
      "18\tValidation loss: 0.049611\tBest loss: 0.049358\tAccuracy: 98.94%\n",
      "19\tValidation loss: 0.061683\tBest loss: 0.049358\tAccuracy: 98.87%\n",
      "20\tValidation loss: 0.079636\tBest loss: 0.049358\tAccuracy: 98.63%\n",
      "21\tValidation loss: 0.090830\tBest loss: 0.049358\tAccuracy: 98.28%\n",
      "22\tValidation loss: 0.062583\tBest loss: 0.049358\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.053668\tBest loss: 0.049358\tAccuracy: 98.91%\n",
      "24\tValidation loss: 0.045686\tBest loss: 0.045686\tAccuracy: 99.02%\n",
      "25\tValidation loss: 0.064839\tBest loss: 0.045686\tAccuracy: 98.75%\n",
      "26\tValidation loss: 0.055397\tBest loss: 0.045686\tAccuracy: 98.87%\n",
      "27\tValidation loss: 0.073014\tBest loss: 0.045686\tAccuracy: 98.63%\n",
      "28\tValidation loss: 0.080853\tBest loss: 0.045686\tAccuracy: 98.71%\n",
      "29\tValidation loss: 0.111883\tBest loss: 0.045686\tAccuracy: 98.12%\n",
      "30\tValidation loss: 0.071935\tBest loss: 0.045686\tAccuracy: 98.24%\n",
      "31\tValidation loss: 0.051485\tBest loss: 0.045686\tAccuracy: 98.98%\n",
      "32\tValidation loss: 0.075264\tBest loss: 0.045686\tAccuracy: 98.51%\n",
      "33\tValidation loss: 0.079464\tBest loss: 0.045686\tAccuracy: 98.83%\n",
      "34\tValidation loss: 0.081558\tBest loss: 0.045686\tAccuracy: 98.71%\n",
      "35\tValidation loss: 0.072084\tBest loss: 0.045686\tAccuracy: 98.75%\n",
      "36\tValidation loss: 0.060870\tBest loss: 0.045686\tAccuracy: 98.98%\n",
      "37\tValidation loss: 0.067781\tBest loss: 0.045686\tAccuracy: 98.91%\n",
      "38\tValidation loss: 0.056804\tBest loss: 0.045686\tAccuracy: 99.26%\n",
      "39\tValidation loss: 0.058846\tBest loss: 0.045686\tAccuracy: 98.94%\n",
      "40\tValidation loss: 0.087080\tBest loss: 0.045686\tAccuracy: 99.06%\n",
      "41\tValidation loss: 0.079307\tBest loss: 0.045686\tAccuracy: 98.67%\n",
      "42\tValidation loss: 0.109933\tBest loss: 0.045686\tAccuracy: 98.16%\n",
      "43\tValidation loss: 0.078379\tBest loss: 0.045686\tAccuracy: 98.83%\n",
      "44\tValidation loss: 0.051995\tBest loss: 0.045686\tAccuracy: 99.06%\n",
      "45\tValidation loss: 0.073134\tBest loss: 0.045686\tAccuracy: 98.98%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.9, activation=<function elu at 0x114248840>, total= 2.6min\n",
      "[CV] n_neurons=140, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 24.418676\tBest loss: 24.418676\tAccuracy: 61.38%\n",
      "1\tValidation loss: 1.030865\tBest loss: 1.030865\tAccuracy: 87.92%\n",
      "2\tValidation loss: 0.244193\tBest loss: 0.244193\tAccuracy: 95.04%\n",
      "3\tValidation loss: 0.115898\tBest loss: 0.115898\tAccuracy: 97.22%\n",
      "4\tValidation loss: 0.084234\tBest loss: 0.084234\tAccuracy: 97.50%\n",
      "5\tValidation loss: 0.074384\tBest loss: 0.074384\tAccuracy: 97.54%\n",
      "6\tValidation loss: 0.078472\tBest loss: 0.074384\tAccuracy: 97.38%\n",
      "7\tValidation loss: 0.142125\tBest loss: 0.074384\tAccuracy: 97.11%\n",
      "8\tValidation loss: 0.221056\tBest loss: 0.074384\tAccuracy: 94.14%\n",
      "9\tValidation loss: 0.073689\tBest loss: 0.073689\tAccuracy: 98.08%\n",
      "10\tValidation loss: 0.075745\tBest loss: 0.073689\tAccuracy: 98.32%\n",
      "11\tValidation loss: 0.098886\tBest loss: 0.073689\tAccuracy: 97.22%\n",
      "12\tValidation loss: 0.067200\tBest loss: 0.067200\tAccuracy: 98.55%\n",
      "13\tValidation loss: 0.097996\tBest loss: 0.067200\tAccuracy: 97.89%\n",
      "14\tValidation loss: 0.102627\tBest loss: 0.067200\tAccuracy: 97.69%\n",
      "15\tValidation loss: 0.089890\tBest loss: 0.067200\tAccuracy: 98.32%\n",
      "16\tValidation loss: 0.077110\tBest loss: 0.067200\tAccuracy: 98.28%\n",
      "17\tValidation loss: 0.084902\tBest loss: 0.067200\tAccuracy: 98.55%\n",
      "18\tValidation loss: 0.098154\tBest loss: 0.067200\tAccuracy: 98.05%\n",
      "19\tValidation loss: 0.137912\tBest loss: 0.067200\tAccuracy: 98.05%\n",
      "20\tValidation loss: 0.132757\tBest loss: 0.067200\tAccuracy: 97.93%\n",
      "21\tValidation loss: 0.096341\tBest loss: 0.067200\tAccuracy: 98.24%\n",
      "22\tValidation loss: 0.110194\tBest loss: 0.067200\tAccuracy: 98.32%\n",
      "23\tValidation loss: 0.078389\tBest loss: 0.067200\tAccuracy: 98.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\tValidation loss: 0.101534\tBest loss: 0.067200\tAccuracy: 98.55%\n",
      "25\tValidation loss: 0.069874\tBest loss: 0.067200\tAccuracy: 98.75%\n",
      "26\tValidation loss: 0.112708\tBest loss: 0.067200\tAccuracy: 98.01%\n",
      "27\tValidation loss: 0.114719\tBest loss: 0.067200\tAccuracy: 97.89%\n",
      "28\tValidation loss: 0.113369\tBest loss: 0.067200\tAccuracy: 97.93%\n",
      "29\tValidation loss: 0.169526\tBest loss: 0.067200\tAccuracy: 97.81%\n",
      "30\tValidation loss: 0.093978\tBest loss: 0.067200\tAccuracy: 98.36%\n",
      "31\tValidation loss: 0.144040\tBest loss: 0.067200\tAccuracy: 97.58%\n",
      "32\tValidation loss: 0.108443\tBest loss: 0.067200\tAccuracy: 98.05%\n",
      "33\tValidation loss: 0.085185\tBest loss: 0.067200\tAccuracy: 98.55%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=<function elu at 0x114248840>, total= 2.8min\n",
      "[CV] n_neurons=140, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 2.948307\tBest loss: 2.948307\tAccuracy: 91.56%\n",
      "1\tValidation loss: 0.528537\tBest loss: 0.528537\tAccuracy: 94.84%\n",
      "2\tValidation loss: 0.263488\tBest loss: 0.263488\tAccuracy: 95.35%\n",
      "3\tValidation loss: 0.126847\tBest loss: 0.126847\tAccuracy: 96.60%\n",
      "4\tValidation loss: 0.137312\tBest loss: 0.126847\tAccuracy: 96.91%\n",
      "5\tValidation loss: 0.147715\tBest loss: 0.126847\tAccuracy: 96.17%\n",
      "6\tValidation loss: 0.117051\tBest loss: 0.117051\tAccuracy: 96.48%\n",
      "7\tValidation loss: 0.083987\tBest loss: 0.083987\tAccuracy: 97.73%\n",
      "8\tValidation loss: 0.111973\tBest loss: 0.083987\tAccuracy: 96.72%\n",
      "9\tValidation loss: 0.119608\tBest loss: 0.083987\tAccuracy: 97.07%\n",
      "10\tValidation loss: 0.071636\tBest loss: 0.071636\tAccuracy: 98.24%\n",
      "11\tValidation loss: 0.094929\tBest loss: 0.071636\tAccuracy: 97.69%\n",
      "12\tValidation loss: 0.068756\tBest loss: 0.068756\tAccuracy: 98.44%\n",
      "13\tValidation loss: 0.068882\tBest loss: 0.068756\tAccuracy: 98.40%\n",
      "14\tValidation loss: 0.115047\tBest loss: 0.068756\tAccuracy: 97.85%\n",
      "15\tValidation loss: 0.090376\tBest loss: 0.068756\tAccuracy: 98.24%\n",
      "16\tValidation loss: 0.067580\tBest loss: 0.067580\tAccuracy: 98.44%\n",
      "17\tValidation loss: 0.080397\tBest loss: 0.067580\tAccuracy: 98.40%\n",
      "18\tValidation loss: 0.084132\tBest loss: 0.067580\tAccuracy: 98.51%\n",
      "19\tValidation loss: 0.124698\tBest loss: 0.067580\tAccuracy: 97.73%\n",
      "20\tValidation loss: 0.097677\tBest loss: 0.067580\tAccuracy: 98.36%\n",
      "21\tValidation loss: 0.083251\tBest loss: 0.067580\tAccuracy: 98.51%\n",
      "22\tValidation loss: 0.083332\tBest loss: 0.067580\tAccuracy: 98.44%\n",
      "23\tValidation loss: 0.094620\tBest loss: 0.067580\tAccuracy: 98.51%\n",
      "24\tValidation loss: 0.076104\tBest loss: 0.067580\tAccuracy: 98.67%\n",
      "25\tValidation loss: 0.091064\tBest loss: 0.067580\tAccuracy: 98.28%\n",
      "26\tValidation loss: 0.071944\tBest loss: 0.067580\tAccuracy: 98.87%\n",
      "27\tValidation loss: 0.072720\tBest loss: 0.067580\tAccuracy: 98.67%\n",
      "28\tValidation loss: 0.097627\tBest loss: 0.067580\tAccuracy: 98.51%\n",
      "29\tValidation loss: 0.103614\tBest loss: 0.067580\tAccuracy: 98.48%\n",
      "30\tValidation loss: 0.180705\tBest loss: 0.067580\tAccuracy: 97.50%\n",
      "31\tValidation loss: 0.074540\tBest loss: 0.067580\tAccuracy: 98.75%\n",
      "32\tValidation loss: 0.067193\tBest loss: 0.067193\tAccuracy: 98.94%\n",
      "33\tValidation loss: 0.072906\tBest loss: 0.067193\tAccuracy: 98.75%\n",
      "34\tValidation loss: 0.067450\tBest loss: 0.067193\tAccuracy: 98.94%\n",
      "35\tValidation loss: 0.114555\tBest loss: 0.067193\tAccuracy: 98.36%\n",
      "36\tValidation loss: 0.083116\tBest loss: 0.067193\tAccuracy: 98.83%\n",
      "37\tValidation loss: 0.125396\tBest loss: 0.067193\tAccuracy: 98.59%\n",
      "38\tValidation loss: 0.114344\tBest loss: 0.067193\tAccuracy: 98.44%\n",
      "39\tValidation loss: 0.124219\tBest loss: 0.067193\tAccuracy: 98.40%\n",
      "40\tValidation loss: 0.103664\tBest loss: 0.067193\tAccuracy: 98.44%\n",
      "41\tValidation loss: 0.081214\tBest loss: 0.067193\tAccuracy: 98.67%\n",
      "42\tValidation loss: 0.187865\tBest loss: 0.067193\tAccuracy: 97.97%\n",
      "43\tValidation loss: 0.073378\tBest loss: 0.067193\tAccuracy: 98.71%\n",
      "44\tValidation loss: 0.105954\tBest loss: 0.067193\tAccuracy: 98.32%\n",
      "45\tValidation loss: 0.061544\tBest loss: 0.061544\tAccuracy: 98.87%\n",
      "46\tValidation loss: 0.077620\tBest loss: 0.061544\tAccuracy: 98.98%\n",
      "47\tValidation loss: 0.073636\tBest loss: 0.061544\tAccuracy: 98.67%\n",
      "48\tValidation loss: 0.069191\tBest loss: 0.061544\tAccuracy: 98.75%\n",
      "49\tValidation loss: 0.075597\tBest loss: 0.061544\tAccuracy: 98.94%\n",
      "50\tValidation loss: 0.071837\tBest loss: 0.061544\tAccuracy: 99.02%\n",
      "51\tValidation loss: 0.087596\tBest loss: 0.061544\tAccuracy: 98.83%\n",
      "52\tValidation loss: 0.164033\tBest loss: 0.061544\tAccuracy: 98.16%\n",
      "53\tValidation loss: 0.089136\tBest loss: 0.061544\tAccuracy: 98.98%\n",
      "54\tValidation loss: 0.092173\tBest loss: 0.061544\tAccuracy: 98.59%\n",
      "55\tValidation loss: 0.096778\tBest loss: 0.061544\tAccuracy: 98.75%\n",
      "56\tValidation loss: 0.109980\tBest loss: 0.061544\tAccuracy: 98.55%\n",
      "57\tValidation loss: 0.241543\tBest loss: 0.061544\tAccuracy: 97.30%\n",
      "58\tValidation loss: 0.129153\tBest loss: 0.061544\tAccuracy: 98.55%\n",
      "59\tValidation loss: 0.102786\tBest loss: 0.061544\tAccuracy: 98.48%\n",
      "60\tValidation loss: 0.202641\tBest loss: 0.061544\tAccuracy: 97.81%\n",
      "61\tValidation loss: 0.142764\tBest loss: 0.061544\tAccuracy: 98.40%\n",
      "62\tValidation loss: 0.146100\tBest loss: 0.061544\tAccuracy: 98.16%\n",
      "63\tValidation loss: 0.087019\tBest loss: 0.061544\tAccuracy: 98.94%\n",
      "64\tValidation loss: 0.075767\tBest loss: 0.061544\tAccuracy: 98.91%\n",
      "65\tValidation loss: 0.074078\tBest loss: 0.061544\tAccuracy: 99.02%\n",
      "66\tValidation loss: 0.075856\tBest loss: 0.061544\tAccuracy: 98.91%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=<function elu at 0x114248840>, total= 4.5min\n",
      "[CV] n_neurons=140, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 2.872304\tBest loss: 2.872304\tAccuracy: 88.12%\n",
      "1\tValidation loss: 0.373185\tBest loss: 0.373185\tAccuracy: 95.74%\n",
      "2\tValidation loss: 0.137312\tBest loss: 0.137312\tAccuracy: 96.87%\n",
      "3\tValidation loss: 0.080977\tBest loss: 0.080977\tAccuracy: 97.81%\n",
      "4\tValidation loss: 0.099086\tBest loss: 0.080977\tAccuracy: 97.42%\n",
      "5\tValidation loss: 0.070136\tBest loss: 0.070136\tAccuracy: 97.97%\n",
      "6\tValidation loss: 0.066640\tBest loss: 0.066640\tAccuracy: 97.73%\n",
      "7\tValidation loss: 0.067667\tBest loss: 0.066640\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.055920\tBest loss: 0.055920\tAccuracy: 98.55%\n",
      "9\tValidation loss: 0.068904\tBest loss: 0.055920\tAccuracy: 98.32%\n",
      "10\tValidation loss: 0.083111\tBest loss: 0.055920\tAccuracy: 98.16%\n",
      "11\tValidation loss: 0.064332\tBest loss: 0.055920\tAccuracy: 98.67%\n",
      "12\tValidation loss: 0.082160\tBest loss: 0.055920\tAccuracy: 97.93%\n",
      "13\tValidation loss: 0.127157\tBest loss: 0.055920\tAccuracy: 97.54%\n",
      "14\tValidation loss: 0.110164\tBest loss: 0.055920\tAccuracy: 98.01%\n",
      "15\tValidation loss: 0.101391\tBest loss: 0.055920\tAccuracy: 98.16%\n",
      "16\tValidation loss: 0.080424\tBest loss: 0.055920\tAccuracy: 98.44%\n",
      "17\tValidation loss: 0.091356\tBest loss: 0.055920\tAccuracy: 98.67%\n",
      "18\tValidation loss: 0.070927\tBest loss: 0.055920\tAccuracy: 98.59%\n",
      "19\tValidation loss: 0.085970\tBest loss: 0.055920\tAccuracy: 98.40%\n",
      "20\tValidation loss: 0.084895\tBest loss: 0.055920\tAccuracy: 98.59%\n",
      "21\tValidation loss: 0.167489\tBest loss: 0.055920\tAccuracy: 97.15%\n",
      "22\tValidation loss: 0.094574\tBest loss: 0.055920\tAccuracy: 98.48%\n",
      "23\tValidation loss: 0.091880\tBest loss: 0.055920\tAccuracy: 98.63%\n",
      "24\tValidation loss: 0.113055\tBest loss: 0.055920\tAccuracy: 98.16%\n",
      "25\tValidation loss: 0.109535\tBest loss: 0.055920\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.080110\tBest loss: 0.055920\tAccuracy: 98.63%\n",
      "27\tValidation loss: 0.073953\tBest loss: 0.055920\tAccuracy: 98.59%\n",
      "28\tValidation loss: 0.071203\tBest loss: 0.055920\tAccuracy: 98.87%\n",
      "29\tValidation loss: 0.086723\tBest loss: 0.055920\tAccuracy: 98.40%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, batch_size=500, batch_norm_momentum=0.99, activation=<function elu at 0x114248840>, total= 1.9min\n",
      "[CV] n_neurons=100, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.301613\tBest loss: 0.301613\tAccuracy: 95.39%\n",
      "1\tValidation loss: 0.083245\tBest loss: 0.083245\tAccuracy: 98.05%\n",
      "2\tValidation loss: 0.075980\tBest loss: 0.075980\tAccuracy: 97.85%\n",
      "3\tValidation loss: 0.065985\tBest loss: 0.065985\tAccuracy: 98.01%\n",
      "4\tValidation loss: 0.057423\tBest loss: 0.057423\tAccuracy: 98.63%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\tValidation loss: 0.073600\tBest loss: 0.057423\tAccuracy: 98.16%\n",
      "6\tValidation loss: 0.053849\tBest loss: 0.053849\tAccuracy: 98.32%\n",
      "7\tValidation loss: 0.074458\tBest loss: 0.053849\tAccuracy: 98.12%\n",
      "8\tValidation loss: 0.056261\tBest loss: 0.053849\tAccuracy: 98.91%\n",
      "9\tValidation loss: 0.064026\tBest loss: 0.053849\tAccuracy: 98.44%\n",
      "10\tValidation loss: 0.060376\tBest loss: 0.053849\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.091545\tBest loss: 0.053849\tAccuracy: 98.36%\n",
      "12\tValidation loss: 0.100419\tBest loss: 0.053849\tAccuracy: 98.01%\n",
      "13\tValidation loss: 0.058458\tBest loss: 0.053849\tAccuracy: 98.79%\n",
      "14\tValidation loss: 0.072631\tBest loss: 0.053849\tAccuracy: 98.59%\n",
      "15\tValidation loss: 0.083454\tBest loss: 0.053849\tAccuracy: 98.67%\n",
      "16\tValidation loss: 0.072419\tBest loss: 0.053849\tAccuracy: 98.51%\n",
      "17\tValidation loss: 0.077286\tBest loss: 0.053849\tAccuracy: 98.44%\n",
      "18\tValidation loss: 0.111012\tBest loss: 0.053849\tAccuracy: 97.85%\n",
      "19\tValidation loss: 0.073006\tBest loss: 0.053849\tAccuracy: 98.44%\n",
      "20\tValidation loss: 0.085111\tBest loss: 0.053849\tAccuracy: 98.40%\n",
      "21\tValidation loss: 0.092816\tBest loss: 0.053849\tAccuracy: 98.32%\n",
      "22\tValidation loss: 0.063403\tBest loss: 0.053849\tAccuracy: 98.71%\n",
      "23\tValidation loss: 0.049933\tBest loss: 0.049933\tAccuracy: 99.06%\n",
      "24\tValidation loss: 0.052695\tBest loss: 0.049933\tAccuracy: 98.87%\n",
      "25\tValidation loss: 0.057963\tBest loss: 0.049933\tAccuracy: 98.98%\n",
      "26\tValidation loss: 0.057713\tBest loss: 0.049933\tAccuracy: 98.91%\n",
      "27\tValidation loss: 0.064211\tBest loss: 0.049933\tAccuracy: 98.83%\n",
      "28\tValidation loss: 0.069848\tBest loss: 0.049933\tAccuracy: 98.75%\n",
      "29\tValidation loss: 0.077748\tBest loss: 0.049933\tAccuracy: 98.83%\n",
      "30\tValidation loss: 0.075176\tBest loss: 0.049933\tAccuracy: 98.91%\n",
      "31\tValidation loss: 0.078898\tBest loss: 0.049933\tAccuracy: 98.55%\n",
      "32\tValidation loss: 0.095285\tBest loss: 0.049933\tAccuracy: 98.40%\n",
      "33\tValidation loss: 0.086135\tBest loss: 0.049933\tAccuracy: 98.55%\n",
      "34\tValidation loss: 0.077596\tBest loss: 0.049933\tAccuracy: 98.51%\n",
      "35\tValidation loss: 0.069143\tBest loss: 0.049933\tAccuracy: 98.59%\n",
      "36\tValidation loss: 0.084865\tBest loss: 0.049933\tAccuracy: 98.24%\n",
      "37\tValidation loss: 0.074134\tBest loss: 0.049933\tAccuracy: 98.36%\n",
      "38\tValidation loss: 0.061458\tBest loss: 0.049933\tAccuracy: 98.79%\n",
      "39\tValidation loss: 0.062188\tBest loss: 0.049933\tAccuracy: 99.10%\n",
      "40\tValidation loss: 0.050954\tBest loss: 0.049933\tAccuracy: 99.18%\n",
      "41\tValidation loss: 0.056800\tBest loss: 0.049933\tAccuracy: 99.06%\n",
      "42\tValidation loss: 0.052160\tBest loss: 0.049933\tAccuracy: 99.06%\n",
      "43\tValidation loss: 0.051741\tBest loss: 0.049933\tAccuracy: 99.10%\n",
      "44\tValidation loss: 0.055129\tBest loss: 0.049933\tAccuracy: 98.98%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=<function elu at 0x114248840>, total= 2.4min\n",
      "[CV] n_neurons=100, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.334820\tBest loss: 0.334820\tAccuracy: 94.49%\n",
      "1\tValidation loss: 0.079840\tBest loss: 0.079840\tAccuracy: 97.85%\n",
      "2\tValidation loss: 0.067514\tBest loss: 0.067514\tAccuracy: 98.08%\n",
      "3\tValidation loss: 0.077020\tBest loss: 0.067514\tAccuracy: 97.73%\n",
      "4\tValidation loss: 0.060565\tBest loss: 0.060565\tAccuracy: 98.51%\n",
      "5\tValidation loss: 0.071745\tBest loss: 0.060565\tAccuracy: 97.97%\n",
      "6\tValidation loss: 0.068674\tBest loss: 0.060565\tAccuracy: 98.28%\n",
      "7\tValidation loss: 0.070788\tBest loss: 0.060565\tAccuracy: 98.08%\n",
      "8\tValidation loss: 0.077843\tBest loss: 0.060565\tAccuracy: 98.32%\n",
      "9\tValidation loss: 0.070233\tBest loss: 0.060565\tAccuracy: 98.05%\n",
      "10\tValidation loss: 0.061352\tBest loss: 0.060565\tAccuracy: 98.36%\n",
      "11\tValidation loss: 0.080710\tBest loss: 0.060565\tAccuracy: 98.01%\n",
      "12\tValidation loss: 0.071240\tBest loss: 0.060565\tAccuracy: 98.28%\n",
      "13\tValidation loss: 0.098389\tBest loss: 0.060565\tAccuracy: 97.58%\n",
      "14\tValidation loss: 0.058344\tBest loss: 0.058344\tAccuracy: 98.75%\n",
      "15\tValidation loss: 0.065424\tBest loss: 0.058344\tAccuracy: 98.32%\n",
      "16\tValidation loss: 0.052867\tBest loss: 0.052867\tAccuracy: 98.48%\n",
      "17\tValidation loss: 0.072877\tBest loss: 0.052867\tAccuracy: 98.32%\n",
      "18\tValidation loss: 0.073607\tBest loss: 0.052867\tAccuracy: 98.32%\n",
      "19\tValidation loss: 0.051378\tBest loss: 0.051378\tAccuracy: 98.87%\n",
      "20\tValidation loss: 0.084876\tBest loss: 0.051378\tAccuracy: 98.28%\n",
      "21\tValidation loss: 0.064600\tBest loss: 0.051378\tAccuracy: 98.55%\n",
      "22\tValidation loss: 0.069383\tBest loss: 0.051378\tAccuracy: 98.67%\n",
      "23\tValidation loss: 0.056749\tBest loss: 0.051378\tAccuracy: 98.83%\n",
      "24\tValidation loss: 0.063560\tBest loss: 0.051378\tAccuracy: 98.59%\n",
      "25\tValidation loss: 0.064416\tBest loss: 0.051378\tAccuracy: 98.51%\n",
      "26\tValidation loss: 0.065531\tBest loss: 0.051378\tAccuracy: 98.55%\n",
      "27\tValidation loss: 0.066407\tBest loss: 0.051378\tAccuracy: 98.51%\n",
      "28\tValidation loss: 0.049713\tBest loss: 0.049713\tAccuracy: 98.71%\n",
      "29\tValidation loss: 0.061684\tBest loss: 0.049713\tAccuracy: 98.59%\n",
      "30\tValidation loss: 0.055402\tBest loss: 0.049713\tAccuracy: 98.71%\n",
      "31\tValidation loss: 0.068965\tBest loss: 0.049713\tAccuracy: 98.87%\n",
      "32\tValidation loss: 0.063055\tBest loss: 0.049713\tAccuracy: 98.71%\n",
      "33\tValidation loss: 0.084323\tBest loss: 0.049713\tAccuracy: 98.32%\n",
      "34\tValidation loss: 0.066752\tBest loss: 0.049713\tAccuracy: 98.83%\n",
      "35\tValidation loss: 0.075657\tBest loss: 0.049713\tAccuracy: 98.48%\n",
      "36\tValidation loss: 0.071962\tBest loss: 0.049713\tAccuracy: 98.75%\n",
      "37\tValidation loss: 0.074299\tBest loss: 0.049713\tAccuracy: 98.63%\n",
      "38\tValidation loss: 0.082228\tBest loss: 0.049713\tAccuracy: 98.59%\n",
      "39\tValidation loss: 0.084430\tBest loss: 0.049713\tAccuracy: 98.44%\n",
      "40\tValidation loss: 0.068667\tBest loss: 0.049713\tAccuracy: 98.67%\n",
      "41\tValidation loss: 0.071342\tBest loss: 0.049713\tAccuracy: 98.67%\n",
      "42\tValidation loss: 0.071881\tBest loss: 0.049713\tAccuracy: 98.63%\n",
      "43\tValidation loss: 0.069895\tBest loss: 0.049713\tAccuracy: 98.75%\n",
      "44\tValidation loss: 0.072859\tBest loss: 0.049713\tAccuracy: 98.79%\n",
      "45\tValidation loss: 0.060943\tBest loss: 0.049713\tAccuracy: 98.83%\n",
      "46\tValidation loss: 0.073197\tBest loss: 0.049713\tAccuracy: 98.87%\n",
      "47\tValidation loss: 0.075006\tBest loss: 0.049713\tAccuracy: 98.75%\n",
      "48\tValidation loss: 0.085587\tBest loss: 0.049713\tAccuracy: 98.51%\n",
      "49\tValidation loss: 0.080247\tBest loss: 0.049713\tAccuracy: 98.51%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=<function elu at 0x114248840>, total= 2.9min\n",
      "[CV] n_neurons=100, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.288587\tBest loss: 0.288587\tAccuracy: 95.82%\n",
      "1\tValidation loss: 0.086066\tBest loss: 0.086066\tAccuracy: 97.81%\n",
      "2\tValidation loss: 0.062364\tBest loss: 0.062364\tAccuracy: 98.16%\n",
      "3\tValidation loss: 0.075410\tBest loss: 0.062364\tAccuracy: 97.69%\n",
      "4\tValidation loss: 0.055800\tBest loss: 0.055800\tAccuracy: 98.32%\n",
      "5\tValidation loss: 0.064337\tBest loss: 0.055800\tAccuracy: 98.12%\n",
      "6\tValidation loss: 0.058809\tBest loss: 0.055800\tAccuracy: 98.44%\n",
      "7\tValidation loss: 0.081516\tBest loss: 0.055800\tAccuracy: 97.85%\n",
      "8\tValidation loss: 0.054865\tBest loss: 0.054865\tAccuracy: 98.59%\n",
      "9\tValidation loss: 0.052232\tBest loss: 0.052232\tAccuracy: 98.87%\n",
      "10\tValidation loss: 0.066332\tBest loss: 0.052232\tAccuracy: 98.63%\n",
      "11\tValidation loss: 0.052442\tBest loss: 0.052232\tAccuracy: 98.91%\n",
      "12\tValidation loss: 0.047398\tBest loss: 0.047398\tAccuracy: 98.98%\n",
      "13\tValidation loss: 0.065850\tBest loss: 0.047398\tAccuracy: 98.48%\n",
      "14\tValidation loss: 0.050696\tBest loss: 0.047398\tAccuracy: 98.79%\n",
      "15\tValidation loss: 0.059742\tBest loss: 0.047398\tAccuracy: 98.83%\n",
      "16\tValidation loss: 0.047623\tBest loss: 0.047398\tAccuracy: 98.67%\n",
      "17\tValidation loss: 0.059795\tBest loss: 0.047398\tAccuracy: 98.87%\n",
      "18\tValidation loss: 0.071115\tBest loss: 0.047398\tAccuracy: 98.59%\n",
      "19\tValidation loss: 0.077305\tBest loss: 0.047398\tAccuracy: 98.67%\n",
      "20\tValidation loss: 0.055194\tBest loss: 0.047398\tAccuracy: 98.63%\n",
      "21\tValidation loss: 0.060251\tBest loss: 0.047398\tAccuracy: 98.71%\n",
      "22\tValidation loss: 0.067864\tBest loss: 0.047398\tAccuracy: 98.40%\n",
      "23\tValidation loss: 0.058980\tBest loss: 0.047398\tAccuracy: 98.79%\n",
      "24\tValidation loss: 0.058734\tBest loss: 0.047398\tAccuracy: 99.02%\n",
      "25\tValidation loss: 0.058051\tBest loss: 0.047398\tAccuracy: 98.87%\n",
      "26\tValidation loss: 0.047626\tBest loss: 0.047398\tAccuracy: 98.98%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\tValidation loss: 0.061297\tBest loss: 0.047398\tAccuracy: 98.87%\n",
      "28\tValidation loss: 0.061545\tBest loss: 0.047398\tAccuracy: 98.71%\n",
      "29\tValidation loss: 0.071443\tBest loss: 0.047398\tAccuracy: 98.67%\n",
      "30\tValidation loss: 0.049545\tBest loss: 0.047398\tAccuracy: 99.10%\n",
      "31\tValidation loss: 0.065636\tBest loss: 0.047398\tAccuracy: 98.71%\n",
      "32\tValidation loss: 0.051596\tBest loss: 0.047398\tAccuracy: 99.06%\n",
      "33\tValidation loss: 0.046723\tBest loss: 0.046723\tAccuracy: 98.94%\n",
      "34\tValidation loss: 0.059041\tBest loss: 0.046723\tAccuracy: 98.87%\n",
      "35\tValidation loss: 0.062100\tBest loss: 0.046723\tAccuracy: 98.63%\n",
      "36\tValidation loss: 0.073708\tBest loss: 0.046723\tAccuracy: 98.71%\n",
      "37\tValidation loss: 0.062993\tBest loss: 0.046723\tAccuracy: 98.75%\n",
      "38\tValidation loss: 0.072243\tBest loss: 0.046723\tAccuracy: 98.87%\n",
      "39\tValidation loss: 0.099301\tBest loss: 0.046723\tAccuracy: 98.44%\n",
      "40\tValidation loss: 0.061864\tBest loss: 0.046723\tAccuracy: 98.91%\n",
      "41\tValidation loss: 0.067246\tBest loss: 0.046723\tAccuracy: 98.67%\n",
      "42\tValidation loss: 0.060796\tBest loss: 0.046723\tAccuracy: 98.87%\n",
      "43\tValidation loss: 0.056075\tBest loss: 0.046723\tAccuracy: 98.83%\n",
      "44\tValidation loss: 0.048827\tBest loss: 0.046723\tAccuracy: 99.06%\n",
      "45\tValidation loss: 0.047729\tBest loss: 0.046723\tAccuracy: 98.94%\n",
      "46\tValidation loss: 0.045832\tBest loss: 0.045832\tAccuracy: 98.98%\n",
      "47\tValidation loss: 0.051896\tBest loss: 0.045832\tAccuracy: 99.02%\n",
      "48\tValidation loss: 0.045156\tBest loss: 0.045156\tAccuracy: 99.10%\n",
      "49\tValidation loss: 0.051064\tBest loss: 0.045156\tAccuracy: 98.91%\n",
      "50\tValidation loss: 0.081779\tBest loss: 0.045156\tAccuracy: 98.75%\n",
      "51\tValidation loss: 0.078355\tBest loss: 0.045156\tAccuracy: 98.79%\n",
      "52\tValidation loss: 0.073915\tBest loss: 0.045156\tAccuracy: 98.59%\n",
      "53\tValidation loss: 0.077751\tBest loss: 0.045156\tAccuracy: 98.71%\n",
      "54\tValidation loss: 0.057557\tBest loss: 0.045156\tAccuracy: 99.02%\n",
      "55\tValidation loss: 0.061969\tBest loss: 0.045156\tAccuracy: 98.91%\n",
      "56\tValidation loss: 0.056056\tBest loss: 0.045156\tAccuracy: 99.06%\n",
      "57\tValidation loss: 0.062150\tBest loss: 0.045156\tAccuracy: 99.02%\n",
      "58\tValidation loss: 0.054069\tBest loss: 0.045156\tAccuracy: 99.18%\n",
      "59\tValidation loss: 0.061773\tBest loss: 0.045156\tAccuracy: 99.02%\n",
      "60\tValidation loss: 0.057841\tBest loss: 0.045156\tAccuracy: 99.14%\n",
      "61\tValidation loss: 0.057976\tBest loss: 0.045156\tAccuracy: 99.10%\n",
      "62\tValidation loss: 0.057616\tBest loss: 0.045156\tAccuracy: 99.10%\n",
      "63\tValidation loss: 0.058465\tBest loss: 0.045156\tAccuracy: 99.06%\n",
      "64\tValidation loss: 0.057832\tBest loss: 0.045156\tAccuracy: 99.10%\n",
      "65\tValidation loss: 0.057737\tBest loss: 0.045156\tAccuracy: 99.10%\n",
      "66\tValidation loss: 0.057623\tBest loss: 0.045156\tAccuracy: 99.10%\n",
      "67\tValidation loss: 0.057801\tBest loss: 0.045156\tAccuracy: 99.06%\n",
      "68\tValidation loss: 0.058173\tBest loss: 0.045156\tAccuracy: 99.02%\n",
      "69\tValidation loss: 0.057827\tBest loss: 0.045156\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, batch_size=500, batch_norm_momentum=0.98, activation=<function elu at 0x114248840>, total= 4.6min\n",
      "[CV] n_neurons=120, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.090158\tBest loss: 0.090158\tAccuracy: 97.22%\n",
      "1\tValidation loss: 0.072676\tBest loss: 0.072676\tAccuracy: 97.81%\n",
      "2\tValidation loss: 0.093023\tBest loss: 0.072676\tAccuracy: 97.22%\n",
      "3\tValidation loss: 0.062919\tBest loss: 0.062919\tAccuracy: 98.12%\n",
      "4\tValidation loss: 0.068935\tBest loss: 0.062919\tAccuracy: 97.89%\n",
      "5\tValidation loss: 0.069176\tBest loss: 0.062919\tAccuracy: 98.12%\n",
      "6\tValidation loss: 0.040124\tBest loss: 0.040124\tAccuracy: 98.91%\n",
      "7\tValidation loss: 0.071803\tBest loss: 0.040124\tAccuracy: 98.08%\n",
      "8\tValidation loss: 0.057076\tBest loss: 0.040124\tAccuracy: 98.51%\n",
      "9\tValidation loss: 0.054784\tBest loss: 0.040124\tAccuracy: 98.87%\n",
      "10\tValidation loss: 0.042925\tBest loss: 0.040124\tAccuracy: 98.91%\n",
      "11\tValidation loss: 0.069727\tBest loss: 0.040124\tAccuracy: 98.48%\n",
      "12\tValidation loss: 0.052318\tBest loss: 0.040124\tAccuracy: 98.91%\n",
      "13\tValidation loss: 0.056975\tBest loss: 0.040124\tAccuracy: 98.75%\n",
      "14\tValidation loss: 0.053472\tBest loss: 0.040124\tAccuracy: 98.63%\n",
      "15\tValidation loss: 0.072449\tBest loss: 0.040124\tAccuracy: 98.51%\n",
      "16\tValidation loss: 0.045685\tBest loss: 0.040124\tAccuracy: 98.79%\n",
      "17\tValidation loss: 0.057063\tBest loss: 0.040124\tAccuracy: 98.55%\n",
      "18\tValidation loss: 0.065565\tBest loss: 0.040124\tAccuracy: 98.63%\n",
      "19\tValidation loss: 0.056576\tBest loss: 0.040124\tAccuracy: 98.63%\n",
      "20\tValidation loss: 0.040044\tBest loss: 0.040044\tAccuracy: 99.06%\n",
      "21\tValidation loss: 0.090137\tBest loss: 0.040044\tAccuracy: 98.16%\n",
      "22\tValidation loss: 0.044317\tBest loss: 0.040044\tAccuracy: 98.94%\n",
      "23\tValidation loss: 0.066554\tBest loss: 0.040044\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.056010\tBest loss: 0.040044\tAccuracy: 98.79%\n",
      "25\tValidation loss: 0.047189\tBest loss: 0.040044\tAccuracy: 98.94%\n",
      "26\tValidation loss: 0.047177\tBest loss: 0.040044\tAccuracy: 98.94%\n",
      "27\tValidation loss: 0.066856\tBest loss: 0.040044\tAccuracy: 98.67%\n",
      "28\tValidation loss: 0.047517\tBest loss: 0.040044\tAccuracy: 98.91%\n",
      "29\tValidation loss: 0.078915\tBest loss: 0.040044\tAccuracy: 98.48%\n",
      "30\tValidation loss: 0.065756\tBest loss: 0.040044\tAccuracy: 98.79%\n",
      "31\tValidation loss: 0.084980\tBest loss: 0.040044\tAccuracy: 98.40%\n",
      "32\tValidation loss: 0.049195\tBest loss: 0.040044\tAccuracy: 99.10%\n",
      "33\tValidation loss: 0.056349\tBest loss: 0.040044\tAccuracy: 98.94%\n",
      "34\tValidation loss: 0.097635\tBest loss: 0.040044\tAccuracy: 98.67%\n",
      "35\tValidation loss: 0.070766\tBest loss: 0.040044\tAccuracy: 98.83%\n",
      "36\tValidation loss: 0.060442\tBest loss: 0.040044\tAccuracy: 98.59%\n",
      "37\tValidation loss: 0.056326\tBest loss: 0.040044\tAccuracy: 98.91%\n",
      "38\tValidation loss: 0.059127\tBest loss: 0.040044\tAccuracy: 98.87%\n",
      "39\tValidation loss: 0.059813\tBest loss: 0.040044\tAccuracy: 98.87%\n",
      "40\tValidation loss: 0.045442\tBest loss: 0.040044\tAccuracy: 99.14%\n",
      "41\tValidation loss: 0.047077\tBest loss: 0.040044\tAccuracy: 99.10%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=<function elu at 0x114248840>, total= 5.0min\n",
      "[CV] n_neurons=120, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.141432\tBest loss: 0.141432\tAccuracy: 95.93%\n",
      "1\tValidation loss: 0.067142\tBest loss: 0.067142\tAccuracy: 97.97%\n",
      "2\tValidation loss: 0.061901\tBest loss: 0.061901\tAccuracy: 97.93%\n",
      "3\tValidation loss: 0.084312\tBest loss: 0.061901\tAccuracy: 97.50%\n",
      "4\tValidation loss: 0.071360\tBest loss: 0.061901\tAccuracy: 98.12%\n",
      "5\tValidation loss: 0.050220\tBest loss: 0.050220\tAccuracy: 98.55%\n",
      "6\tValidation loss: 0.083967\tBest loss: 0.050220\tAccuracy: 97.65%\n",
      "7\tValidation loss: 0.060183\tBest loss: 0.050220\tAccuracy: 98.20%\n",
      "8\tValidation loss: 0.035391\tBest loss: 0.035391\tAccuracy: 98.98%\n",
      "9\tValidation loss: 0.074349\tBest loss: 0.035391\tAccuracy: 98.05%\n",
      "10\tValidation loss: 0.075555\tBest loss: 0.035391\tAccuracy: 98.40%\n",
      "11\tValidation loss: 0.084811\tBest loss: 0.035391\tAccuracy: 97.73%\n",
      "12\tValidation loss: 0.049411\tBest loss: 0.035391\tAccuracy: 98.98%\n",
      "13\tValidation loss: 0.055606\tBest loss: 0.035391\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.064066\tBest loss: 0.035391\tAccuracy: 98.28%\n",
      "15\tValidation loss: 0.075259\tBest loss: 0.035391\tAccuracy: 98.40%\n",
      "16\tValidation loss: 0.058602\tBest loss: 0.035391\tAccuracy: 98.83%\n",
      "17\tValidation loss: 0.104724\tBest loss: 0.035391\tAccuracy: 98.12%\n",
      "18\tValidation loss: 0.062455\tBest loss: 0.035391\tAccuracy: 98.94%\n",
      "19\tValidation loss: 0.052425\tBest loss: 0.035391\tAccuracy: 98.48%\n",
      "20\tValidation loss: 0.053543\tBest loss: 0.035391\tAccuracy: 98.91%\n",
      "21\tValidation loss: 0.054058\tBest loss: 0.035391\tAccuracy: 98.79%\n",
      "22\tValidation loss: 0.065854\tBest loss: 0.035391\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.082960\tBest loss: 0.035391\tAccuracy: 98.44%\n",
      "24\tValidation loss: 0.060785\tBest loss: 0.035391\tAccuracy: 98.79%\n",
      "25\tValidation loss: 0.061853\tBest loss: 0.035391\tAccuracy: 98.87%\n",
      "26\tValidation loss: 0.059896\tBest loss: 0.035391\tAccuracy: 98.98%\n",
      "27\tValidation loss: 0.059467\tBest loss: 0.035391\tAccuracy: 98.59%\n",
      "28\tValidation loss: 0.078398\tBest loss: 0.035391\tAccuracy: 98.48%\n",
      "29\tValidation loss: 0.058327\tBest loss: 0.035391\tAccuracy: 98.87%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=<function elu at 0x114248840>, total= 2.3min\n",
      "[CV] n_neurons=120, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.105967\tBest loss: 0.105967\tAccuracy: 96.91%\n",
      "1\tValidation loss: 0.077574\tBest loss: 0.077574\tAccuracy: 97.65%\n",
      "2\tValidation loss: 0.067257\tBest loss: 0.067257\tAccuracy: 97.85%\n",
      "3\tValidation loss: 0.055954\tBest loss: 0.055954\tAccuracy: 98.12%\n",
      "4\tValidation loss: 0.067487\tBest loss: 0.055954\tAccuracy: 98.20%\n",
      "5\tValidation loss: 0.037416\tBest loss: 0.037416\tAccuracy: 98.83%\n",
      "6\tValidation loss: 0.059225\tBest loss: 0.037416\tAccuracy: 98.12%\n",
      "7\tValidation loss: 0.100498\tBest loss: 0.037416\tAccuracy: 97.77%\n",
      "8\tValidation loss: 0.042558\tBest loss: 0.037416\tAccuracy: 98.91%\n",
      "9\tValidation loss: 0.161193\tBest loss: 0.037416\tAccuracy: 96.52%\n",
      "10\tValidation loss: 0.041999\tBest loss: 0.037416\tAccuracy: 98.75%\n",
      "11\tValidation loss: 0.047045\tBest loss: 0.037416\tAccuracy: 98.91%\n",
      "12\tValidation loss: 0.045482\tBest loss: 0.037416\tAccuracy: 99.02%\n",
      "13\tValidation loss: 0.046452\tBest loss: 0.037416\tAccuracy: 98.87%\n",
      "14\tValidation loss: 0.049978\tBest loss: 0.037416\tAccuracy: 98.79%\n",
      "15\tValidation loss: 0.045064\tBest loss: 0.037416\tAccuracy: 98.87%\n",
      "16\tValidation loss: 0.046420\tBest loss: 0.037416\tAccuracy: 98.83%\n",
      "17\tValidation loss: 0.047737\tBest loss: 0.037416\tAccuracy: 98.91%\n",
      "18\tValidation loss: 0.065463\tBest loss: 0.037416\tAccuracy: 98.67%\n",
      "19\tValidation loss: 0.076976\tBest loss: 0.037416\tAccuracy: 98.16%\n",
      "20\tValidation loss: 0.048369\tBest loss: 0.037416\tAccuracy: 98.59%\n",
      "21\tValidation loss: 0.054670\tBest loss: 0.037416\tAccuracy: 99.06%\n",
      "22\tValidation loss: 0.051721\tBest loss: 0.037416\tAccuracy: 98.83%\n",
      "23\tValidation loss: 0.064105\tBest loss: 0.037416\tAccuracy: 98.91%\n",
      "24\tValidation loss: 0.045531\tBest loss: 0.037416\tAccuracy: 98.98%\n",
      "25\tValidation loss: 0.063470\tBest loss: 0.037416\tAccuracy: 98.71%\n",
      "26\tValidation loss: 0.048347\tBest loss: 0.037416\tAccuracy: 98.79%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.98, activation=<function elu at 0x114248840>, total= 2.0min\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8> \n",
      "0\tValidation loss: 0.123139\tBest loss: 0.123139\tAccuracy: 96.56%\n",
      "1\tValidation loss: 0.105057\tBest loss: 0.105057\tAccuracy: 97.11%\n",
      "2\tValidation loss: 0.100191\tBest loss: 0.100191\tAccuracy: 97.30%\n",
      "3\tValidation loss: 0.077369\tBest loss: 0.077369\tAccuracy: 97.69%\n",
      "4\tValidation loss: 0.068372\tBest loss: 0.068372\tAccuracy: 97.97%\n",
      "5\tValidation loss: 0.079161\tBest loss: 0.068372\tAccuracy: 97.93%\n",
      "6\tValidation loss: 0.070477\tBest loss: 0.068372\tAccuracy: 97.97%\n",
      "7\tValidation loss: 0.059538\tBest loss: 0.059538\tAccuracy: 98.32%\n",
      "8\tValidation loss: 0.070822\tBest loss: 0.059538\tAccuracy: 97.77%\n",
      "9\tValidation loss: 0.079526\tBest loss: 0.059538\tAccuracy: 98.05%\n",
      "10\tValidation loss: 0.060285\tBest loss: 0.059538\tAccuracy: 98.32%\n",
      "11\tValidation loss: 0.064553\tBest loss: 0.059538\tAccuracy: 98.12%\n",
      "12\tValidation loss: 0.071660\tBest loss: 0.059538\tAccuracy: 98.12%\n",
      "13\tValidation loss: 0.067169\tBest loss: 0.059538\tAccuracy: 98.44%\n",
      "14\tValidation loss: 0.062090\tBest loss: 0.059538\tAccuracy: 98.36%\n",
      "15\tValidation loss: 0.069768\tBest loss: 0.059538\tAccuracy: 97.97%\n",
      "16\tValidation loss: 0.063435\tBest loss: 0.059538\tAccuracy: 98.08%\n",
      "17\tValidation loss: 0.058384\tBest loss: 0.058384\tAccuracy: 98.44%\n",
      "18\tValidation loss: 0.087565\tBest loss: 0.058384\tAccuracy: 97.77%\n",
      "19\tValidation loss: 0.076495\tBest loss: 0.058384\tAccuracy: 98.16%\n",
      "20\tValidation loss: 0.076450\tBest loss: 0.058384\tAccuracy: 97.81%\n",
      "21\tValidation loss: 0.085623\tBest loss: 0.058384\tAccuracy: 97.85%\n",
      "22\tValidation loss: 0.074134\tBest loss: 0.058384\tAccuracy: 98.08%\n",
      "23\tValidation loss: 0.061895\tBest loss: 0.058384\tAccuracy: 98.55%\n",
      "24\tValidation loss: 0.077515\tBest loss: 0.058384\tAccuracy: 98.20%\n",
      "25\tValidation loss: 0.073750\tBest loss: 0.058384\tAccuracy: 98.20%\n",
      "26\tValidation loss: 0.074827\tBest loss: 0.058384\tAccuracy: 98.20%\n",
      "27\tValidation loss: 0.095379\tBest loss: 0.058384\tAccuracy: 97.93%\n",
      "28\tValidation loss: 0.066683\tBest loss: 0.058384\tAccuracy: 98.51%\n",
      "29\tValidation loss: 0.070072\tBest loss: 0.058384\tAccuracy: 98.28%\n",
      "30\tValidation loss: 0.067906\tBest loss: 0.058384\tAccuracy: 98.32%\n",
      "31\tValidation loss: 0.106049\tBest loss: 0.058384\tAccuracy: 97.42%\n",
      "32\tValidation loss: 0.069395\tBest loss: 0.058384\tAccuracy: 98.44%\n",
      "33\tValidation loss: 0.084477\tBest loss: 0.058384\tAccuracy: 97.97%\n",
      "34\tValidation loss: 0.078939\tBest loss: 0.058384\tAccuracy: 98.12%\n",
      "35\tValidation loss: 0.071715\tBest loss: 0.058384\tAccuracy: 98.32%\n",
      "36\tValidation loss: 0.077815\tBest loss: 0.058384\tAccuracy: 98.05%\n",
      "37\tValidation loss: 0.068374\tBest loss: 0.058384\tAccuracy: 98.36%\n",
      "38\tValidation loss: 0.063902\tBest loss: 0.058384\tAccuracy: 98.51%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>, total= 1.9min\n",
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8> \n",
      "0\tValidation loss: 0.097970\tBest loss: 0.097970\tAccuracy: 97.11%\n",
      "1\tValidation loss: 0.075388\tBest loss: 0.075388\tAccuracy: 97.89%\n",
      "2\tValidation loss: 0.082928\tBest loss: 0.075388\tAccuracy: 97.54%\n",
      "3\tValidation loss: 0.070829\tBest loss: 0.070829\tAccuracy: 97.77%\n",
      "4\tValidation loss: 0.095975\tBest loss: 0.070829\tAccuracy: 97.46%\n",
      "5\tValidation loss: 0.066974\tBest loss: 0.066974\tAccuracy: 97.81%\n",
      "6\tValidation loss: 0.079315\tBest loss: 0.066974\tAccuracy: 97.81%\n",
      "7\tValidation loss: 0.076749\tBest loss: 0.066974\tAccuracy: 97.69%\n",
      "8\tValidation loss: 0.103628\tBest loss: 0.066974\tAccuracy: 97.50%\n",
      "9\tValidation loss: 0.064683\tBest loss: 0.064683\tAccuracy: 98.16%\n",
      "10\tValidation loss: 0.075190\tBest loss: 0.064683\tAccuracy: 97.85%\n",
      "11\tValidation loss: 0.079363\tBest loss: 0.064683\tAccuracy: 98.05%\n",
      "12\tValidation loss: 0.107591\tBest loss: 0.064683\tAccuracy: 96.79%\n",
      "13\tValidation loss: 0.073854\tBest loss: 0.064683\tAccuracy: 98.05%\n",
      "14\tValidation loss: 0.064002\tBest loss: 0.064002\tAccuracy: 98.20%\n",
      "15\tValidation loss: 0.065107\tBest loss: 0.064002\tAccuracy: 98.28%\n",
      "16\tValidation loss: 0.083809\tBest loss: 0.064002\tAccuracy: 97.69%\n",
      "17\tValidation loss: 0.068021\tBest loss: 0.064002\tAccuracy: 97.85%\n",
      "18\tValidation loss: 0.065154\tBest loss: 0.064002\tAccuracy: 98.01%\n",
      "19\tValidation loss: 0.087417\tBest loss: 0.064002\tAccuracy: 97.65%\n",
      "20\tValidation loss: 0.070948\tBest loss: 0.064002\tAccuracy: 97.69%\n",
      "21\tValidation loss: 0.071798\tBest loss: 0.064002\tAccuracy: 98.05%\n",
      "22\tValidation loss: 0.093852\tBest loss: 0.064002\tAccuracy: 97.46%\n",
      "23\tValidation loss: 0.075052\tBest loss: 0.064002\tAccuracy: 97.85%\n",
      "24\tValidation loss: 0.063700\tBest loss: 0.063700\tAccuracy: 98.28%\n",
      "25\tValidation loss: 0.070493\tBest loss: 0.063700\tAccuracy: 98.12%\n",
      "26\tValidation loss: 0.073840\tBest loss: 0.063700\tAccuracy: 98.01%\n",
      "27\tValidation loss: 0.077753\tBest loss: 0.063700\tAccuracy: 97.89%\n",
      "28\tValidation loss: 0.066836\tBest loss: 0.063700\tAccuracy: 98.28%\n",
      "29\tValidation loss: 0.076544\tBest loss: 0.063700\tAccuracy: 98.16%\n",
      "30\tValidation loss: 0.087963\tBest loss: 0.063700\tAccuracy: 97.65%\n",
      "31\tValidation loss: 0.076906\tBest loss: 0.063700\tAccuracy: 97.73%\n",
      "32\tValidation loss: 0.079745\tBest loss: 0.063700\tAccuracy: 98.12%\n",
      "33\tValidation loss: 0.077104\tBest loss: 0.063700\tAccuracy: 97.97%\n",
      "34\tValidation loss: 0.112569\tBest loss: 0.063700\tAccuracy: 97.50%\n",
      "35\tValidation loss: 0.096624\tBest loss: 0.063700\tAccuracy: 97.73%\n",
      "36\tValidation loss: 0.140858\tBest loss: 0.063700\tAccuracy: 96.91%\n",
      "37\tValidation loss: 0.068908\tBest loss: 0.063700\tAccuracy: 98.20%\n",
      "38\tValidation loss: 0.084741\tBest loss: 0.063700\tAccuracy: 98.08%\n",
      "39\tValidation loss: 0.077864\tBest loss: 0.063700\tAccuracy: 98.01%\n",
      "40\tValidation loss: 0.112614\tBest loss: 0.063700\tAccuracy: 98.01%\n",
      "41\tValidation loss: 0.088669\tBest loss: 0.063700\tAccuracy: 98.01%\n",
      "42\tValidation loss: 0.088402\tBest loss: 0.063700\tAccuracy: 97.73%\n",
      "43\tValidation loss: 0.104199\tBest loss: 0.063700\tAccuracy: 98.08%\n",
      "44\tValidation loss: 0.095809\tBest loss: 0.063700\tAccuracy: 97.97%\n",
      "45\tValidation loss: 0.090266\tBest loss: 0.063700\tAccuracy: 98.08%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>, total= 2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] n_neurons=10, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8> \n",
      "0\tValidation loss: 0.129564\tBest loss: 0.129564\tAccuracy: 96.44%\n",
      "1\tValidation loss: 0.089077\tBest loss: 0.089077\tAccuracy: 97.26%\n",
      "2\tValidation loss: 0.076385\tBest loss: 0.076385\tAccuracy: 97.93%\n",
      "3\tValidation loss: 0.085467\tBest loss: 0.076385\tAccuracy: 97.42%\n",
      "4\tValidation loss: 0.075992\tBest loss: 0.075992\tAccuracy: 97.73%\n",
      "5\tValidation loss: 0.068054\tBest loss: 0.068054\tAccuracy: 98.08%\n",
      "6\tValidation loss: 0.178507\tBest loss: 0.068054\tAccuracy: 95.86%\n",
      "7\tValidation loss: 0.067216\tBest loss: 0.067216\tAccuracy: 98.20%\n",
      "8\tValidation loss: 0.061578\tBest loss: 0.061578\tAccuracy: 98.32%\n",
      "9\tValidation loss: 0.058254\tBest loss: 0.058254\tAccuracy: 98.24%\n",
      "10\tValidation loss: 0.066615\tBest loss: 0.058254\tAccuracy: 98.44%\n",
      "11\tValidation loss: 0.083343\tBest loss: 0.058254\tAccuracy: 97.89%\n",
      "12\tValidation loss: 0.080362\tBest loss: 0.058254\tAccuracy: 98.12%\n",
      "13\tValidation loss: 0.082361\tBest loss: 0.058254\tAccuracy: 97.85%\n",
      "14\tValidation loss: 0.074317\tBest loss: 0.058254\tAccuracy: 97.81%\n",
      "15\tValidation loss: 0.060645\tBest loss: 0.058254\tAccuracy: 98.40%\n",
      "16\tValidation loss: 0.071156\tBest loss: 0.058254\tAccuracy: 98.08%\n",
      "17\tValidation loss: 0.070639\tBest loss: 0.058254\tAccuracy: 98.24%\n",
      "18\tValidation loss: 0.060018\tBest loss: 0.058254\tAccuracy: 98.51%\n",
      "19\tValidation loss: 0.066878\tBest loss: 0.058254\tAccuracy: 98.32%\n",
      "20\tValidation loss: 0.067252\tBest loss: 0.058254\tAccuracy: 97.89%\n",
      "21\tValidation loss: 0.065190\tBest loss: 0.058254\tAccuracy: 98.28%\n",
      "22\tValidation loss: 0.075618\tBest loss: 0.058254\tAccuracy: 97.58%\n",
      "23\tValidation loss: 0.055328\tBest loss: 0.055328\tAccuracy: 98.32%\n",
      "24\tValidation loss: 0.060468\tBest loss: 0.055328\tAccuracy: 98.28%\n",
      "25\tValidation loss: 0.062851\tBest loss: 0.055328\tAccuracy: 98.36%\n",
      "26\tValidation loss: 0.064819\tBest loss: 0.055328\tAccuracy: 98.24%\n",
      "27\tValidation loss: 0.064987\tBest loss: 0.055328\tAccuracy: 98.16%\n",
      "28\tValidation loss: 0.069353\tBest loss: 0.055328\tAccuracy: 98.24%\n",
      "29\tValidation loss: 0.107199\tBest loss: 0.055328\tAccuracy: 97.77%\n",
      "30\tValidation loss: 0.068378\tBest loss: 0.055328\tAccuracy: 98.44%\n",
      "31\tValidation loss: 0.067624\tBest loss: 0.055328\tAccuracy: 98.36%\n",
      "32\tValidation loss: 0.084745\tBest loss: 0.055328\tAccuracy: 97.97%\n",
      "33\tValidation loss: 0.082461\tBest loss: 0.055328\tAccuracy: 98.12%\n",
      "34\tValidation loss: 0.094839\tBest loss: 0.055328\tAccuracy: 98.01%\n",
      "35\tValidation loss: 0.089661\tBest loss: 0.055328\tAccuracy: 97.85%\n",
      "36\tValidation loss: 0.079157\tBest loss: 0.055328\tAccuracy: 98.36%\n",
      "37\tValidation loss: 0.080028\tBest loss: 0.055328\tAccuracy: 97.93%\n",
      "38\tValidation loss: 0.105451\tBest loss: 0.055328\tAccuracy: 97.73%\n",
      "39\tValidation loss: 0.072845\tBest loss: 0.055328\tAccuracy: 98.44%\n",
      "40\tValidation loss: 0.118400\tBest loss: 0.055328\tAccuracy: 96.87%\n",
      "41\tValidation loss: 0.092598\tBest loss: 0.055328\tAccuracy: 98.16%\n",
      "42\tValidation loss: 0.075582\tBest loss: 0.055328\tAccuracy: 98.12%\n",
      "43\tValidation loss: 0.096852\tBest loss: 0.055328\tAccuracy: 98.24%\n",
      "44\tValidation loss: 0.094723\tBest loss: 0.055328\tAccuracy: 97.50%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, batch_size=50, batch_norm_momentum=0.95, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>, total= 2.1min\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488> \n",
      "0\tValidation loss: 0.123749\tBest loss: 0.123749\tAccuracy: 95.78%\n",
      "1\tValidation loss: 0.112075\tBest loss: 0.112075\tAccuracy: 96.68%\n",
      "2\tValidation loss: 0.096359\tBest loss: 0.096359\tAccuracy: 97.11%\n",
      "3\tValidation loss: 0.103863\tBest loss: 0.096359\tAccuracy: 96.72%\n",
      "4\tValidation loss: 0.079250\tBest loss: 0.079250\tAccuracy: 97.73%\n",
      "5\tValidation loss: 0.086837\tBest loss: 0.079250\tAccuracy: 97.58%\n",
      "6\tValidation loss: 0.090780\tBest loss: 0.079250\tAccuracy: 97.30%\n",
      "7\tValidation loss: 0.074941\tBest loss: 0.074941\tAccuracy: 97.65%\n",
      "8\tValidation loss: 0.094702\tBest loss: 0.074941\tAccuracy: 97.62%\n",
      "9\tValidation loss: 0.105915\tBest loss: 0.074941\tAccuracy: 96.83%\n",
      "10\tValidation loss: 0.099263\tBest loss: 0.074941\tAccuracy: 97.38%\n",
      "11\tValidation loss: 0.081393\tBest loss: 0.074941\tAccuracy: 97.73%\n",
      "12\tValidation loss: 0.079183\tBest loss: 0.074941\tAccuracy: 97.97%\n",
      "13\tValidation loss: 0.075848\tBest loss: 0.074941\tAccuracy: 98.05%\n",
      "14\tValidation loss: 0.072107\tBest loss: 0.072107\tAccuracy: 97.97%\n",
      "15\tValidation loss: 0.076199\tBest loss: 0.072107\tAccuracy: 98.12%\n",
      "16\tValidation loss: 0.075892\tBest loss: 0.072107\tAccuracy: 97.85%\n",
      "17\tValidation loss: 0.066802\tBest loss: 0.066802\tAccuracy: 98.12%\n",
      "18\tValidation loss: 0.083790\tBest loss: 0.066802\tAccuracy: 97.26%\n",
      "19\tValidation loss: 0.106010\tBest loss: 0.066802\tAccuracy: 97.11%\n",
      "20\tValidation loss: 0.068839\tBest loss: 0.066802\tAccuracy: 97.89%\n",
      "21\tValidation loss: 0.074891\tBest loss: 0.066802\tAccuracy: 98.08%\n",
      "22\tValidation loss: 0.069131\tBest loss: 0.066802\tAccuracy: 97.97%\n",
      "23\tValidation loss: 0.071372\tBest loss: 0.066802\tAccuracy: 97.85%\n",
      "24\tValidation loss: 0.090276\tBest loss: 0.066802\tAccuracy: 97.38%\n",
      "25\tValidation loss: 0.068522\tBest loss: 0.066802\tAccuracy: 97.93%\n",
      "26\tValidation loss: 0.068962\tBest loss: 0.066802\tAccuracy: 98.12%\n",
      "27\tValidation loss: 0.071964\tBest loss: 0.066802\tAccuracy: 98.05%\n",
      "28\tValidation loss: 0.069531\tBest loss: 0.066802\tAccuracy: 98.12%\n",
      "29\tValidation loss: 0.063853\tBest loss: 0.063853\tAccuracy: 98.20%\n",
      "30\tValidation loss: 0.071433\tBest loss: 0.063853\tAccuracy: 98.20%\n",
      "31\tValidation loss: 0.075701\tBest loss: 0.063853\tAccuracy: 97.62%\n",
      "32\tValidation loss: 0.067640\tBest loss: 0.063853\tAccuracy: 98.28%\n",
      "33\tValidation loss: 0.071860\tBest loss: 0.063853\tAccuracy: 97.65%\n",
      "34\tValidation loss: 0.080848\tBest loss: 0.063853\tAccuracy: 97.77%\n",
      "35\tValidation loss: 0.074760\tBest loss: 0.063853\tAccuracy: 97.62%\n",
      "36\tValidation loss: 0.068718\tBest loss: 0.063853\tAccuracy: 97.97%\n",
      "37\tValidation loss: 0.084078\tBest loss: 0.063853\tAccuracy: 97.65%\n",
      "38\tValidation loss: 0.063843\tBest loss: 0.063843\tAccuracy: 98.24%\n",
      "39\tValidation loss: 0.062698\tBest loss: 0.062698\tAccuracy: 98.32%\n",
      "40\tValidation loss: 0.075978\tBest loss: 0.062698\tAccuracy: 97.81%\n",
      "41\tValidation loss: 0.076170\tBest loss: 0.062698\tAccuracy: 97.77%\n",
      "42\tValidation loss: 0.073222\tBest loss: 0.062698\tAccuracy: 98.08%\n",
      "43\tValidation loss: 0.084073\tBest loss: 0.062698\tAccuracy: 97.42%\n",
      "44\tValidation loss: 0.064826\tBest loss: 0.062698\tAccuracy: 98.05%\n",
      "45\tValidation loss: 0.061530\tBest loss: 0.061530\tAccuracy: 98.24%\n",
      "46\tValidation loss: 0.061077\tBest loss: 0.061077\tAccuracy: 98.40%\n",
      "47\tValidation loss: 0.086906\tBest loss: 0.061077\tAccuracy: 97.26%\n",
      "48\tValidation loss: 0.065035\tBest loss: 0.061077\tAccuracy: 98.12%\n",
      "49\tValidation loss: 0.078691\tBest loss: 0.061077\tAccuracy: 97.58%\n",
      "50\tValidation loss: 0.065756\tBest loss: 0.061077\tAccuracy: 98.08%\n",
      "51\tValidation loss: 0.069191\tBest loss: 0.061077\tAccuracy: 97.97%\n",
      "52\tValidation loss: 0.068405\tBest loss: 0.061077\tAccuracy: 98.05%\n",
      "53\tValidation loss: 0.062009\tBest loss: 0.061077\tAccuracy: 98.63%\n",
      "54\tValidation loss: 0.064852\tBest loss: 0.061077\tAccuracy: 98.24%\n",
      "55\tValidation loss: 0.078308\tBest loss: 0.061077\tAccuracy: 97.89%\n",
      "56\tValidation loss: 0.070426\tBest loss: 0.061077\tAccuracy: 98.05%\n",
      "57\tValidation loss: 0.067431\tBest loss: 0.061077\tAccuracy: 98.20%\n",
      "58\tValidation loss: 0.069021\tBest loss: 0.061077\tAccuracy: 98.28%\n",
      "59\tValidation loss: 0.070647\tBest loss: 0.061077\tAccuracy: 97.97%\n",
      "60\tValidation loss: 0.076454\tBest loss: 0.061077\tAccuracy: 97.81%\n",
      "61\tValidation loss: 0.068810\tBest loss: 0.061077\tAccuracy: 97.97%\n",
      "62\tValidation loss: 0.064803\tBest loss: 0.061077\tAccuracy: 98.01%\n",
      "63\tValidation loss: 0.085499\tBest loss: 0.061077\tAccuracy: 97.54%\n",
      "64\tValidation loss: 0.076301\tBest loss: 0.061077\tAccuracy: 97.62%\n",
      "65\tValidation loss: 0.081930\tBest loss: 0.061077\tAccuracy: 97.65%\n",
      "66\tValidation loss: 0.069177\tBest loss: 0.061077\tAccuracy: 97.97%\n",
      "67\tValidation loss: 0.069462\tBest loss: 0.061077\tAccuracy: 97.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488>, total=15.5min\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.129066\tBest loss: 0.129066\tAccuracy: 96.05%\n",
      "1\tValidation loss: 0.119103\tBest loss: 0.119103\tAccuracy: 96.05%\n",
      "2\tValidation loss: 0.110452\tBest loss: 0.110452\tAccuracy: 96.29%\n",
      "3\tValidation loss: 0.084407\tBest loss: 0.084407\tAccuracy: 97.42%\n",
      "4\tValidation loss: 0.090249\tBest loss: 0.084407\tAccuracy: 97.58%\n",
      "5\tValidation loss: 0.088656\tBest loss: 0.084407\tAccuracy: 97.22%\n",
      "6\tValidation loss: 0.085422\tBest loss: 0.084407\tAccuracy: 97.73%\n",
      "7\tValidation loss: 0.097903\tBest loss: 0.084407\tAccuracy: 96.95%\n",
      "8\tValidation loss: 0.096914\tBest loss: 0.084407\tAccuracy: 96.91%\n",
      "9\tValidation loss: 0.086989\tBest loss: 0.084407\tAccuracy: 97.19%\n",
      "10\tValidation loss: 0.088931\tBest loss: 0.084407\tAccuracy: 97.30%\n",
      "11\tValidation loss: 0.071637\tBest loss: 0.071637\tAccuracy: 98.01%\n",
      "12\tValidation loss: 0.077396\tBest loss: 0.071637\tAccuracy: 97.81%\n",
      "13\tValidation loss: 0.075810\tBest loss: 0.071637\tAccuracy: 97.77%\n",
      "14\tValidation loss: 0.071892\tBest loss: 0.071637\tAccuracy: 97.54%\n",
      "15\tValidation loss: 0.102677\tBest loss: 0.071637\tAccuracy: 96.60%\n",
      "16\tValidation loss: 0.096230\tBest loss: 0.071637\tAccuracy: 96.91%\n",
      "17\tValidation loss: 0.074533\tBest loss: 0.071637\tAccuracy: 97.62%\n",
      "18\tValidation loss: 0.083239\tBest loss: 0.071637\tAccuracy: 97.34%\n",
      "19\tValidation loss: 0.090240\tBest loss: 0.071637\tAccuracy: 97.38%\n",
      "20\tValidation loss: 0.095190\tBest loss: 0.071637\tAccuracy: 97.46%\n",
      "21\tValidation loss: 0.067713\tBest loss: 0.067713\tAccuracy: 98.16%\n",
      "22\tValidation loss: 0.083785\tBest loss: 0.067713\tAccuracy: 97.77%\n",
      "23\tValidation loss: 0.067637\tBest loss: 0.067637\tAccuracy: 98.16%\n",
      "24\tValidation loss: 0.073796\tBest loss: 0.067637\tAccuracy: 97.93%\n",
      "25\tValidation loss: 0.080212\tBest loss: 0.067637\tAccuracy: 97.93%\n",
      "26\tValidation loss: 0.071630\tBest loss: 0.067637\tAccuracy: 97.89%\n",
      "27\tValidation loss: 0.076569\tBest loss: 0.067637\tAccuracy: 98.05%\n",
      "28\tValidation loss: 0.063399\tBest loss: 0.063399\tAccuracy: 97.97%\n",
      "29\tValidation loss: 0.076770\tBest loss: 0.063399\tAccuracy: 97.54%\n",
      "30\tValidation loss: 0.069144\tBest loss: 0.063399\tAccuracy: 97.81%\n",
      "31\tValidation loss: 0.065937\tBest loss: 0.063399\tAccuracy: 98.05%\n",
      "32\tValidation loss: 0.074908\tBest loss: 0.063399\tAccuracy: 97.65%\n",
      "33\tValidation loss: 0.069545\tBest loss: 0.063399\tAccuracy: 98.05%\n",
      "34\tValidation loss: 0.076419\tBest loss: 0.063399\tAccuracy: 97.85%\n",
      "35\tValidation loss: 0.080095\tBest loss: 0.063399\tAccuracy: 97.65%\n",
      "36\tValidation loss: 0.089232\tBest loss: 0.063399\tAccuracy: 97.50%\n",
      "37\tValidation loss: 0.075997\tBest loss: 0.063399\tAccuracy: 97.73%\n",
      "38\tValidation loss: 0.087542\tBest loss: 0.063399\tAccuracy: 97.58%\n",
      "39\tValidation loss: 0.076990\tBest loss: 0.063399\tAccuracy: 97.38%\n",
      "40\tValidation loss: 0.091746\tBest loss: 0.063399\tAccuracy: 97.19%\n",
      "41\tValidation loss: 0.069172\tBest loss: 0.063399\tAccuracy: 98.01%\n",
      "42\tValidation loss: 0.077724\tBest loss: 0.063399\tAccuracy: 97.85%\n",
      "43\tValidation loss: 0.081878\tBest loss: 0.063399\tAccuracy: 97.38%\n",
      "44\tValidation loss: 0.069132\tBest loss: 0.063399\tAccuracy: 98.05%\n",
      "45\tValidation loss: 0.068924\tBest loss: 0.063399\tAccuracy: 97.73%\n",
      "46\tValidation loss: 0.073588\tBest loss: 0.063399\tAccuracy: 97.85%\n",
      "47\tValidation loss: 0.069530\tBest loss: 0.063399\tAccuracy: 97.97%\n",
      "48\tValidation loss: 0.078407\tBest loss: 0.063399\tAccuracy: 97.89%\n",
      "49\tValidation loss: 0.090866\tBest loss: 0.063399\tAccuracy: 97.15%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488>, total= 9.9min\n",
      "[CV] n_neurons=10, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488> \n",
      "0\tValidation loss: 0.134835\tBest loss: 0.134835\tAccuracy: 96.44%\n",
      "1\tValidation loss: 0.096154\tBest loss: 0.096154\tAccuracy: 97.26%\n",
      "2\tValidation loss: 0.099466\tBest loss: 0.096154\tAccuracy: 96.99%\n",
      "3\tValidation loss: 0.115866\tBest loss: 0.096154\tAccuracy: 96.64%\n",
      "4\tValidation loss: 0.098416\tBest loss: 0.096154\tAccuracy: 97.03%\n",
      "5\tValidation loss: 0.092711\tBest loss: 0.092711\tAccuracy: 97.42%\n",
      "6\tValidation loss: 0.097286\tBest loss: 0.092711\tAccuracy: 97.15%\n",
      "7\tValidation loss: 0.081492\tBest loss: 0.081492\tAccuracy: 97.42%\n",
      "8\tValidation loss: 0.078828\tBest loss: 0.078828\tAccuracy: 97.54%\n",
      "9\tValidation loss: 0.084434\tBest loss: 0.078828\tAccuracy: 97.73%\n",
      "10\tValidation loss: 0.075940\tBest loss: 0.075940\tAccuracy: 97.81%\n",
      "11\tValidation loss: 0.083865\tBest loss: 0.075940\tAccuracy: 97.42%\n",
      "12\tValidation loss: 0.101602\tBest loss: 0.075940\tAccuracy: 97.03%\n",
      "13\tValidation loss: 0.078843\tBest loss: 0.075940\tAccuracy: 97.65%\n",
      "14\tValidation loss: 0.077503\tBest loss: 0.075940\tAccuracy: 97.65%\n",
      "15\tValidation loss: 0.073091\tBest loss: 0.073091\tAccuracy: 97.69%\n",
      "16\tValidation loss: 0.083623\tBest loss: 0.073091\tAccuracy: 97.34%\n",
      "17\tValidation loss: 0.079461\tBest loss: 0.073091\tAccuracy: 97.69%\n",
      "18\tValidation loss: 0.085411\tBest loss: 0.073091\tAccuracy: 97.58%\n",
      "19\tValidation loss: 0.084853\tBest loss: 0.073091\tAccuracy: 97.69%\n",
      "20\tValidation loss: 0.070222\tBest loss: 0.070222\tAccuracy: 97.93%\n",
      "21\tValidation loss: 0.059965\tBest loss: 0.059965\tAccuracy: 97.97%\n",
      "22\tValidation loss: 0.063116\tBest loss: 0.059965\tAccuracy: 98.08%\n",
      "23\tValidation loss: 0.059703\tBest loss: 0.059703\tAccuracy: 98.16%\n",
      "24\tValidation loss: 0.065901\tBest loss: 0.059703\tAccuracy: 97.73%\n",
      "25\tValidation loss: 0.061795\tBest loss: 0.059703\tAccuracy: 97.85%\n",
      "26\tValidation loss: 0.079971\tBest loss: 0.059703\tAccuracy: 97.58%\n",
      "27\tValidation loss: 0.060256\tBest loss: 0.059703\tAccuracy: 98.12%\n",
      "28\tValidation loss: 0.066575\tBest loss: 0.059703\tAccuracy: 97.93%\n",
      "29\tValidation loss: 0.082800\tBest loss: 0.059703\tAccuracy: 97.15%\n",
      "30\tValidation loss: 0.073813\tBest loss: 0.059703\tAccuracy: 97.58%\n",
      "31\tValidation loss: 0.073783\tBest loss: 0.059703\tAccuracy: 97.73%\n",
      "32\tValidation loss: 0.067582\tBest loss: 0.059703\tAccuracy: 97.93%\n",
      "33\tValidation loss: 0.077592\tBest loss: 0.059703\tAccuracy: 97.46%\n",
      "34\tValidation loss: 0.078670\tBest loss: 0.059703\tAccuracy: 97.58%\n",
      "35\tValidation loss: 0.071268\tBest loss: 0.059703\tAccuracy: 97.69%\n",
      "36\tValidation loss: 0.085733\tBest loss: 0.059703\tAccuracy: 96.83%\n",
      "37\tValidation loss: 0.069003\tBest loss: 0.059703\tAccuracy: 98.01%\n",
      "38\tValidation loss: 0.063662\tBest loss: 0.059703\tAccuracy: 97.85%\n",
      "39\tValidation loss: 0.068953\tBest loss: 0.059703\tAccuracy: 97.69%\n",
      "40\tValidation loss: 0.070051\tBest loss: 0.059703\tAccuracy: 98.05%\n",
      "41\tValidation loss: 0.066752\tBest loss: 0.059703\tAccuracy: 97.97%\n",
      "42\tValidation loss: 0.075887\tBest loss: 0.059703\tAccuracy: 97.73%\n",
      "43\tValidation loss: 0.061495\tBest loss: 0.059703\tAccuracy: 97.97%\n",
      "44\tValidation loss: 0.067128\tBest loss: 0.059703\tAccuracy: 97.69%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, batch_size=10, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488>, total= 9.7min\n",
      "[CV] n_neurons=140, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.99, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.302578\tBest loss: 0.302578\tAccuracy: 93.32%\n",
      "1\tValidation loss: 0.149300\tBest loss: 0.149300\tAccuracy: 95.54%\n",
      "2\tValidation loss: 0.113780\tBest loss: 0.113780\tAccuracy: 96.76%\n",
      "3\tValidation loss: 0.073953\tBest loss: 0.073953\tAccuracy: 97.81%\n",
      "4\tValidation loss: 0.108833\tBest loss: 0.073953\tAccuracy: 96.48%\n",
      "5\tValidation loss: 0.062195\tBest loss: 0.062195\tAccuracy: 98.05%\n",
      "6\tValidation loss: 0.087730\tBest loss: 0.062195\tAccuracy: 97.19%\n",
      "7\tValidation loss: 0.049656\tBest loss: 0.049656\tAccuracy: 98.40%\n",
      "8\tValidation loss: 0.063912\tBest loss: 0.049656\tAccuracy: 98.36%\n",
      "9\tValidation loss: 0.110080\tBest loss: 0.049656\tAccuracy: 97.34%\n",
      "10\tValidation loss: 0.066702\tBest loss: 0.049656\tAccuracy: 98.05%\n",
      "11\tValidation loss: 0.090383\tBest loss: 0.049656\tAccuracy: 97.69%\n",
      "12\tValidation loss: 0.052952\tBest loss: 0.049656\tAccuracy: 98.55%\n",
      "13\tValidation loss: 0.071343\tBest loss: 0.049656\tAccuracy: 98.28%\n",
      "14\tValidation loss: 0.086011\tBest loss: 0.049656\tAccuracy: 97.69%\n",
      "15\tValidation loss: 0.093767\tBest loss: 0.049656\tAccuracy: 97.81%\n",
      "16\tValidation loss: 0.073231\tBest loss: 0.049656\tAccuracy: 98.36%\n",
      "17\tValidation loss: 0.060410\tBest loss: 0.049656\tAccuracy: 98.83%\n",
      "18\tValidation loss: 0.088873\tBest loss: 0.049656\tAccuracy: 98.01%\n",
      "19\tValidation loss: 0.068188\tBest loss: 0.049656\tAccuracy: 98.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\tValidation loss: 0.061067\tBest loss: 0.049656\tAccuracy: 98.59%\n",
      "21\tValidation loss: 0.078561\tBest loss: 0.049656\tAccuracy: 98.44%\n",
      "22\tValidation loss: 0.091250\tBest loss: 0.049656\tAccuracy: 98.36%\n",
      "23\tValidation loss: 0.075670\tBest loss: 0.049656\tAccuracy: 98.32%\n",
      "24\tValidation loss: 0.073809\tBest loss: 0.049656\tAccuracy: 98.20%\n",
      "25\tValidation loss: 0.057361\tBest loss: 0.049656\tAccuracy: 98.87%\n",
      "26\tValidation loss: 0.085066\tBest loss: 0.049656\tAccuracy: 98.24%\n",
      "27\tValidation loss: 0.132603\tBest loss: 0.049656\tAccuracy: 97.81%\n",
      "28\tValidation loss: 0.083821\tBest loss: 0.049656\tAccuracy: 98.12%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.99, activation=<function relu at 0x114259620>, total=10.3min\n",
      "[CV] n_neurons=140, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.99, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.366540\tBest loss: 0.366540\tAccuracy: 93.16%\n",
      "1\tValidation loss: 0.101116\tBest loss: 0.101116\tAccuracy: 97.11%\n",
      "2\tValidation loss: 0.103913\tBest loss: 0.101116\tAccuracy: 96.60%\n",
      "3\tValidation loss: 0.071260\tBest loss: 0.071260\tAccuracy: 97.93%\n",
      "4\tValidation loss: 0.078699\tBest loss: 0.071260\tAccuracy: 97.58%\n",
      "5\tValidation loss: 0.182920\tBest loss: 0.071260\tAccuracy: 94.68%\n",
      "6\tValidation loss: 0.070155\tBest loss: 0.070155\tAccuracy: 97.77%\n",
      "7\tValidation loss: 0.056035\tBest loss: 0.056035\tAccuracy: 98.36%\n",
      "8\tValidation loss: 0.107131\tBest loss: 0.056035\tAccuracy: 96.83%\n",
      "9\tValidation loss: 0.065436\tBest loss: 0.056035\tAccuracy: 98.36%\n",
      "10\tValidation loss: 0.069305\tBest loss: 0.056035\tAccuracy: 98.16%\n",
      "11\tValidation loss: 0.124564\tBest loss: 0.056035\tAccuracy: 96.83%\n",
      "12\tValidation loss: 0.063491\tBest loss: 0.056035\tAccuracy: 98.08%\n",
      "13\tValidation loss: 0.055059\tBest loss: 0.055059\tAccuracy: 98.67%\n",
      "14\tValidation loss: 0.054841\tBest loss: 0.054841\tAccuracy: 98.44%\n",
      "15\tValidation loss: 0.073230\tBest loss: 0.054841\tAccuracy: 98.12%\n",
      "16\tValidation loss: 0.075843\tBest loss: 0.054841\tAccuracy: 97.93%\n",
      "17\tValidation loss: 0.085240\tBest loss: 0.054841\tAccuracy: 97.58%\n",
      "18\tValidation loss: 0.082070\tBest loss: 0.054841\tAccuracy: 97.58%\n",
      "19\tValidation loss: 0.067190\tBest loss: 0.054841\tAccuracy: 98.48%\n",
      "20\tValidation loss: 0.074114\tBest loss: 0.054841\tAccuracy: 98.32%\n",
      "21\tValidation loss: 0.089502\tBest loss: 0.054841\tAccuracy: 98.01%\n",
      "22\tValidation loss: 0.068711\tBest loss: 0.054841\tAccuracy: 97.85%\n",
      "23\tValidation loss: 0.053166\tBest loss: 0.053166\tAccuracy: 98.71%\n",
      "24\tValidation loss: 0.077008\tBest loss: 0.053166\tAccuracy: 97.93%\n",
      "25\tValidation loss: 0.075803\tBest loss: 0.053166\tAccuracy: 98.40%\n",
      "26\tValidation loss: 0.070386\tBest loss: 0.053166\tAccuracy: 98.36%\n",
      "27\tValidation loss: 0.112602\tBest loss: 0.053166\tAccuracy: 97.42%\n",
      "28\tValidation loss: 0.081700\tBest loss: 0.053166\tAccuracy: 98.67%\n",
      "29\tValidation loss: 0.072907\tBest loss: 0.053166\tAccuracy: 98.48%\n",
      "30\tValidation loss: 0.073332\tBest loss: 0.053166\tAccuracy: 98.44%\n",
      "31\tValidation loss: 0.052037\tBest loss: 0.052037\tAccuracy: 98.75%\n",
      "32\tValidation loss: 0.086142\tBest loss: 0.052037\tAccuracy: 98.44%\n",
      "33\tValidation loss: 0.055712\tBest loss: 0.052037\tAccuracy: 98.79%\n",
      "34\tValidation loss: 0.088524\tBest loss: 0.052037\tAccuracy: 98.36%\n",
      "35\tValidation loss: 0.071593\tBest loss: 0.052037\tAccuracy: 98.12%\n",
      "36\tValidation loss: 0.070164\tBest loss: 0.052037\tAccuracy: 98.71%\n",
      "37\tValidation loss: 0.066247\tBest loss: 0.052037\tAccuracy: 98.63%\n",
      "38\tValidation loss: 0.071628\tBest loss: 0.052037\tAccuracy: 98.75%\n",
      "39\tValidation loss: 0.091164\tBest loss: 0.052037\tAccuracy: 97.81%\n",
      "40\tValidation loss: 0.043486\tBest loss: 0.043486\tAccuracy: 98.71%\n",
      "41\tValidation loss: 0.062468\tBest loss: 0.043486\tAccuracy: 98.83%\n",
      "42\tValidation loss: 0.057915\tBest loss: 0.043486\tAccuracy: 98.75%\n",
      "43\tValidation loss: 0.078069\tBest loss: 0.043486\tAccuracy: 98.32%\n",
      "44\tValidation loss: 0.050541\tBest loss: 0.043486\tAccuracy: 98.87%\n",
      "45\tValidation loss: 0.072999\tBest loss: 0.043486\tAccuracy: 98.40%\n",
      "46\tValidation loss: 0.107095\tBest loss: 0.043486\tAccuracy: 98.48%\n",
      "47\tValidation loss: 0.056919\tBest loss: 0.043486\tAccuracy: 98.79%\n",
      "48\tValidation loss: 0.071436\tBest loss: 0.043486\tAccuracy: 98.75%\n",
      "49\tValidation loss: 0.076202\tBest loss: 0.043486\tAccuracy: 98.83%\n",
      "50\tValidation loss: 0.074988\tBest loss: 0.043486\tAccuracy: 98.59%\n",
      "51\tValidation loss: 0.058843\tBest loss: 0.043486\tAccuracy: 98.63%\n",
      "52\tValidation loss: 0.108007\tBest loss: 0.043486\tAccuracy: 97.69%\n",
      "53\tValidation loss: 0.096465\tBest loss: 0.043486\tAccuracy: 97.97%\n",
      "54\tValidation loss: 0.088315\tBest loss: 0.043486\tAccuracy: 98.28%\n",
      "55\tValidation loss: 0.048888\tBest loss: 0.043486\tAccuracy: 98.87%\n",
      "56\tValidation loss: 0.118376\tBest loss: 0.043486\tAccuracy: 97.69%\n",
      "57\tValidation loss: 0.075397\tBest loss: 0.043486\tAccuracy: 98.55%\n",
      "58\tValidation loss: 0.058260\tBest loss: 0.043486\tAccuracy: 98.87%\n",
      "59\tValidation loss: 0.071644\tBest loss: 0.043486\tAccuracy: 98.16%\n",
      "60\tValidation loss: 0.055701\tBest loss: 0.043486\tAccuracy: 98.67%\n",
      "61\tValidation loss: 0.082753\tBest loss: 0.043486\tAccuracy: 98.20%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.99, activation=<function relu at 0x114259620>, total=21.5min\n",
      "[CV] n_neurons=140, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.99, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.151928\tBest loss: 0.151928\tAccuracy: 95.97%\n",
      "1\tValidation loss: 0.103345\tBest loss: 0.103345\tAccuracy: 97.34%\n",
      "2\tValidation loss: 0.126217\tBest loss: 0.103345\tAccuracy: 96.17%\n",
      "3\tValidation loss: 0.106712\tBest loss: 0.103345\tAccuracy: 97.46%\n",
      "4\tValidation loss: 0.090401\tBest loss: 0.090401\tAccuracy: 97.62%\n",
      "5\tValidation loss: 0.401533\tBest loss: 0.090401\tAccuracy: 83.62%\n",
      "6\tValidation loss: 0.127205\tBest loss: 0.090401\tAccuracy: 96.68%\n",
      "7\tValidation loss: 0.100839\tBest loss: 0.090401\tAccuracy: 97.46%\n",
      "8\tValidation loss: 0.058659\tBest loss: 0.058659\tAccuracy: 98.48%\n",
      "9\tValidation loss: 0.225176\tBest loss: 0.058659\tAccuracy: 93.08%\n",
      "10\tValidation loss: 0.222976\tBest loss: 0.058659\tAccuracy: 92.73%\n",
      "11\tValidation loss: 0.110334\tBest loss: 0.058659\tAccuracy: 96.44%\n",
      "12\tValidation loss: 0.064978\tBest loss: 0.058659\tAccuracy: 98.44%\n",
      "13\tValidation loss: 0.064189\tBest loss: 0.058659\tAccuracy: 98.55%\n",
      "14\tValidation loss: 0.076467\tBest loss: 0.058659\tAccuracy: 98.05%\n",
      "15\tValidation loss: 0.066498\tBest loss: 0.058659\tAccuracy: 98.28%\n",
      "16\tValidation loss: 0.081151\tBest loss: 0.058659\tAccuracy: 97.81%\n",
      "17\tValidation loss: 0.105496\tBest loss: 0.058659\tAccuracy: 97.54%\n",
      "18\tValidation loss: 0.068861\tBest loss: 0.058659\tAccuracy: 98.28%\n",
      "19\tValidation loss: 0.056263\tBest loss: 0.056263\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.056783\tBest loss: 0.056263\tAccuracy: 98.67%\n",
      "21\tValidation loss: 0.104479\tBest loss: 0.056263\tAccuracy: 97.34%\n",
      "22\tValidation loss: 0.056856\tBest loss: 0.056263\tAccuracy: 98.48%\n",
      "23\tValidation loss: 0.057567\tBest loss: 0.056263\tAccuracy: 98.40%\n",
      "24\tValidation loss: 0.056601\tBest loss: 0.056263\tAccuracy: 98.36%\n",
      "25\tValidation loss: 0.077706\tBest loss: 0.056263\tAccuracy: 98.28%\n",
      "26\tValidation loss: 0.060896\tBest loss: 0.056263\tAccuracy: 98.79%\n",
      "27\tValidation loss: 0.043950\tBest loss: 0.043950\tAccuracy: 98.87%\n",
      "28\tValidation loss: 0.068035\tBest loss: 0.043950\tAccuracy: 98.51%\n",
      "29\tValidation loss: 0.053495\tBest loss: 0.043950\tAccuracy: 98.87%\n",
      "30\tValidation loss: 0.085716\tBest loss: 0.043950\tAccuracy: 98.36%\n",
      "31\tValidation loss: 0.087280\tBest loss: 0.043950\tAccuracy: 97.58%\n",
      "32\tValidation loss: 0.060251\tBest loss: 0.043950\tAccuracy: 98.44%\n",
      "33\tValidation loss: 0.062791\tBest loss: 0.043950\tAccuracy: 98.59%\n",
      "34\tValidation loss: 0.069668\tBest loss: 0.043950\tAccuracy: 98.55%\n",
      "35\tValidation loss: 0.049437\tBest loss: 0.043950\tAccuracy: 98.98%\n",
      "36\tValidation loss: 0.061947\tBest loss: 0.043950\tAccuracy: 98.36%\n",
      "37\tValidation loss: 0.040639\tBest loss: 0.040639\tAccuracy: 99.10%\n",
      "38\tValidation loss: 0.056084\tBest loss: 0.040639\tAccuracy: 98.94%\n",
      "39\tValidation loss: 0.055936\tBest loss: 0.040639\tAccuracy: 99.02%\n",
      "40\tValidation loss: 0.072873\tBest loss: 0.040639\tAccuracy: 98.24%\n",
      "41\tValidation loss: 0.084247\tBest loss: 0.040639\tAccuracy: 98.20%\n",
      "42\tValidation loss: 0.063430\tBest loss: 0.040639\tAccuracy: 98.71%\n",
      "43\tValidation loss: 0.064206\tBest loss: 0.040639\tAccuracy: 98.59%\n",
      "44\tValidation loss: 0.089125\tBest loss: 0.040639\tAccuracy: 98.75%\n",
      "45\tValidation loss: 0.047728\tBest loss: 0.040639\tAccuracy: 98.79%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\tValidation loss: 0.051272\tBest loss: 0.040639\tAccuracy: 98.83%\n",
      "47\tValidation loss: 0.085616\tBest loss: 0.040639\tAccuracy: 98.48%\n",
      "48\tValidation loss: 0.070816\tBest loss: 0.040639\tAccuracy: 98.48%\n",
      "49\tValidation loss: 0.069700\tBest loss: 0.040639\tAccuracy: 98.44%\n",
      "50\tValidation loss: 0.111232\tBest loss: 0.040639\tAccuracy: 98.44%\n",
      "51\tValidation loss: 0.066756\tBest loss: 0.040639\tAccuracy: 98.63%\n",
      "52\tValidation loss: 0.069677\tBest loss: 0.040639\tAccuracy: 98.51%\n",
      "53\tValidation loss: 0.058425\tBest loss: 0.040639\tAccuracy: 98.79%\n",
      "54\tValidation loss: 0.091926\tBest loss: 0.040639\tAccuracy: 98.51%\n",
      "55\tValidation loss: 0.085832\tBest loss: 0.040639\tAccuracy: 98.48%\n",
      "56\tValidation loss: 0.090216\tBest loss: 0.040639\tAccuracy: 98.16%\n",
      "57\tValidation loss: 0.057388\tBest loss: 0.040639\tAccuracy: 99.06%\n",
      "58\tValidation loss: 0.119400\tBest loss: 0.040639\tAccuracy: 98.24%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, batch_size=10, batch_norm_momentum=0.99, activation=<function relu at 0x114259620>, total=17.9min\n",
      "[CV] n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.99, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.089492\tBest loss: 0.089492\tAccuracy: 97.54%\n",
      "1\tValidation loss: 0.078420\tBest loss: 0.078420\tAccuracy: 97.81%\n",
      "2\tValidation loss: 0.067449\tBest loss: 0.067449\tAccuracy: 98.20%\n",
      "3\tValidation loss: 0.063315\tBest loss: 0.063315\tAccuracy: 98.24%\n",
      "4\tValidation loss: 0.087854\tBest loss: 0.063315\tAccuracy: 97.42%\n",
      "5\tValidation loss: 0.067741\tBest loss: 0.063315\tAccuracy: 98.40%\n",
      "6\tValidation loss: 0.053390\tBest loss: 0.053390\tAccuracy: 98.44%\n",
      "7\tValidation loss: 0.053463\tBest loss: 0.053390\tAccuracy: 98.71%\n",
      "8\tValidation loss: 0.097208\tBest loss: 0.053390\tAccuracy: 97.50%\n",
      "9\tValidation loss: 0.066163\tBest loss: 0.053390\tAccuracy: 98.48%\n",
      "10\tValidation loss: 0.069395\tBest loss: 0.053390\tAccuracy: 98.40%\n",
      "11\tValidation loss: 0.050911\tBest loss: 0.050911\tAccuracy: 98.67%\n",
      "12\tValidation loss: 0.046522\tBest loss: 0.046522\tAccuracy: 98.79%\n",
      "13\tValidation loss: 0.073062\tBest loss: 0.046522\tAccuracy: 98.44%\n",
      "14\tValidation loss: 0.042134\tBest loss: 0.042134\tAccuracy: 98.87%\n",
      "15\tValidation loss: 0.061181\tBest loss: 0.042134\tAccuracy: 98.32%\n",
      "16\tValidation loss: 0.032153\tBest loss: 0.032153\tAccuracy: 99.14%\n",
      "17\tValidation loss: 0.077732\tBest loss: 0.032153\tAccuracy: 98.40%\n",
      "18\tValidation loss: 0.047066\tBest loss: 0.032153\tAccuracy: 98.94%\n",
      "19\tValidation loss: 0.072483\tBest loss: 0.032153\tAccuracy: 98.59%\n",
      "20\tValidation loss: 0.057545\tBest loss: 0.032153\tAccuracy: 99.06%\n",
      "21\tValidation loss: 0.075290\tBest loss: 0.032153\tAccuracy: 98.40%\n",
      "22\tValidation loss: 0.062218\tBest loss: 0.032153\tAccuracy: 98.63%\n",
      "23\tValidation loss: 0.059977\tBest loss: 0.032153\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.035085\tBest loss: 0.032153\tAccuracy: 99.22%\n",
      "25\tValidation loss: 0.051801\tBest loss: 0.032153\tAccuracy: 98.79%\n",
      "26\tValidation loss: 0.064193\tBest loss: 0.032153\tAccuracy: 98.71%\n",
      "27\tValidation loss: 0.189697\tBest loss: 0.032153\tAccuracy: 96.17%\n",
      "28\tValidation loss: 0.057258\tBest loss: 0.032153\tAccuracy: 98.79%\n",
      "29\tValidation loss: 0.062939\tBest loss: 0.032153\tAccuracy: 98.51%\n",
      "30\tValidation loss: 0.052643\tBest loss: 0.032153\tAccuracy: 98.87%\n",
      "31\tValidation loss: 0.066938\tBest loss: 0.032153\tAccuracy: 98.87%\n",
      "32\tValidation loss: 0.083942\tBest loss: 0.032153\tAccuracy: 98.59%\n",
      "33\tValidation loss: 0.057918\tBest loss: 0.032153\tAccuracy: 98.87%\n",
      "34\tValidation loss: 0.065877\tBest loss: 0.032153\tAccuracy: 98.67%\n",
      "35\tValidation loss: 0.057472\tBest loss: 0.032153\tAccuracy: 98.91%\n",
      "36\tValidation loss: 0.049325\tBest loss: 0.032153\tAccuracy: 98.94%\n",
      "37\tValidation loss: 0.084957\tBest loss: 0.032153\tAccuracy: 98.67%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.99, activation=<function elu at 0x114248840>, total= 2.3min\n",
      "[CV] n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.99, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.168724\tBest loss: 0.168724\tAccuracy: 96.01%\n",
      "1\tValidation loss: 0.071380\tBest loss: 0.071380\tAccuracy: 98.08%\n",
      "2\tValidation loss: 0.067582\tBest loss: 0.067582\tAccuracy: 98.40%\n",
      "3\tValidation loss: 0.106594\tBest loss: 0.067582\tAccuracy: 97.22%\n",
      "4\tValidation loss: 0.126168\tBest loss: 0.067582\tAccuracy: 97.11%\n",
      "5\tValidation loss: 0.057046\tBest loss: 0.057046\tAccuracy: 98.44%\n",
      "6\tValidation loss: 0.081331\tBest loss: 0.057046\tAccuracy: 98.12%\n",
      "7\tValidation loss: 0.058986\tBest loss: 0.057046\tAccuracy: 98.12%\n",
      "8\tValidation loss: 0.070929\tBest loss: 0.057046\tAccuracy: 98.63%\n",
      "9\tValidation loss: 0.150413\tBest loss: 0.057046\tAccuracy: 95.27%\n",
      "10\tValidation loss: 0.058630\tBest loss: 0.057046\tAccuracy: 98.67%\n",
      "11\tValidation loss: 0.075574\tBest loss: 0.057046\tAccuracy: 98.16%\n",
      "12\tValidation loss: 0.058158\tBest loss: 0.057046\tAccuracy: 98.71%\n",
      "13\tValidation loss: 0.067821\tBest loss: 0.057046\tAccuracy: 98.40%\n",
      "14\tValidation loss: 0.061580\tBest loss: 0.057046\tAccuracy: 98.44%\n",
      "15\tValidation loss: 0.117215\tBest loss: 0.057046\tAccuracy: 97.77%\n",
      "16\tValidation loss: 0.047510\tBest loss: 0.047510\tAccuracy: 98.75%\n",
      "17\tValidation loss: 0.043037\tBest loss: 0.043037\tAccuracy: 98.98%\n",
      "18\tValidation loss: 0.101441\tBest loss: 0.043037\tAccuracy: 98.24%\n",
      "19\tValidation loss: 0.053727\tBest loss: 0.043037\tAccuracy: 98.94%\n",
      "20\tValidation loss: 0.037772\tBest loss: 0.037772\tAccuracy: 98.91%\n",
      "21\tValidation loss: 0.069395\tBest loss: 0.037772\tAccuracy: 98.28%\n",
      "22\tValidation loss: 0.056553\tBest loss: 0.037772\tAccuracy: 98.91%\n",
      "23\tValidation loss: 0.064864\tBest loss: 0.037772\tAccuracy: 98.75%\n",
      "24\tValidation loss: 0.041661\tBest loss: 0.037772\tAccuracy: 99.18%\n",
      "25\tValidation loss: 0.048231\tBest loss: 0.037772\tAccuracy: 99.02%\n",
      "26\tValidation loss: 0.041923\tBest loss: 0.037772\tAccuracy: 99.10%\n",
      "27\tValidation loss: 0.072441\tBest loss: 0.037772\tAccuracy: 98.55%\n",
      "28\tValidation loss: 0.060276\tBest loss: 0.037772\tAccuracy: 98.91%\n",
      "29\tValidation loss: 0.058671\tBest loss: 0.037772\tAccuracy: 98.67%\n",
      "30\tValidation loss: 0.039658\tBest loss: 0.037772\tAccuracy: 99.10%\n",
      "31\tValidation loss: 0.042737\tBest loss: 0.037772\tAccuracy: 98.94%\n",
      "32\tValidation loss: 0.044391\tBest loss: 0.037772\tAccuracy: 99.02%\n",
      "33\tValidation loss: 0.046648\tBest loss: 0.037772\tAccuracy: 99.06%\n",
      "34\tValidation loss: 0.050100\tBest loss: 0.037772\tAccuracy: 98.98%\n",
      "35\tValidation loss: 0.046697\tBest loss: 0.037772\tAccuracy: 98.98%\n",
      "36\tValidation loss: 0.032014\tBest loss: 0.032014\tAccuracy: 99.06%\n",
      "37\tValidation loss: 0.046088\tBest loss: 0.032014\tAccuracy: 99.02%\n",
      "38\tValidation loss: 0.061937\tBest loss: 0.032014\tAccuracy: 98.79%\n",
      "39\tValidation loss: 0.030974\tBest loss: 0.030974\tAccuracy: 99.10%\n",
      "40\tValidation loss: 0.034767\tBest loss: 0.030974\tAccuracy: 99.37%\n",
      "41\tValidation loss: 0.040199\tBest loss: 0.030974\tAccuracy: 99.22%\n",
      "42\tValidation loss: 0.048829\tBest loss: 0.030974\tAccuracy: 98.98%\n",
      "43\tValidation loss: 0.060222\tBest loss: 0.030974\tAccuracy: 98.55%\n",
      "44\tValidation loss: 0.048853\tBest loss: 0.030974\tAccuracy: 99.10%\n",
      "45\tValidation loss: 0.060157\tBest loss: 0.030974\tAccuracy: 98.91%\n",
      "46\tValidation loss: 0.056279\tBest loss: 0.030974\tAccuracy: 99.02%\n",
      "47\tValidation loss: 0.056089\tBest loss: 0.030974\tAccuracy: 98.91%\n",
      "48\tValidation loss: 0.044033\tBest loss: 0.030974\tAccuracy: 99.06%\n",
      "49\tValidation loss: 0.049091\tBest loss: 0.030974\tAccuracy: 99.02%\n",
      "50\tValidation loss: 0.044489\tBest loss: 0.030974\tAccuracy: 99.18%\n",
      "51\tValidation loss: 0.061883\tBest loss: 0.030974\tAccuracy: 99.06%\n",
      "52\tValidation loss: 0.070086\tBest loss: 0.030974\tAccuracy: 98.67%\n",
      "53\tValidation loss: 0.063140\tBest loss: 0.030974\tAccuracy: 99.10%\n",
      "54\tValidation loss: 0.056177\tBest loss: 0.030974\tAccuracy: 98.87%\n",
      "55\tValidation loss: 0.045824\tBest loss: 0.030974\tAccuracy: 99.14%\n",
      "56\tValidation loss: 0.054655\tBest loss: 0.030974\tAccuracy: 99.10%\n",
      "57\tValidation loss: 0.057351\tBest loss: 0.030974\tAccuracy: 98.98%\n",
      "58\tValidation loss: 0.052297\tBest loss: 0.030974\tAccuracy: 99.10%\n",
      "59\tValidation loss: 0.051654\tBest loss: 0.030974\tAccuracy: 99.06%\n",
      "60\tValidation loss: 0.044159\tBest loss: 0.030974\tAccuracy: 99.22%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.99, activation=<function elu at 0x114248840>, total= 3.7min\n",
      "[CV] n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.99, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.126449\tBest loss: 0.126449\tAccuracy: 97.15%\n",
      "1\tValidation loss: 0.063312\tBest loss: 0.063312\tAccuracy: 97.97%\n",
      "2\tValidation loss: 0.070741\tBest loss: 0.063312\tAccuracy: 97.65%\n",
      "3\tValidation loss: 0.062400\tBest loss: 0.062400\tAccuracy: 98.32%\n",
      "4\tValidation loss: 0.058578\tBest loss: 0.058578\tAccuracy: 98.24%\n",
      "5\tValidation loss: 0.043288\tBest loss: 0.043288\tAccuracy: 98.67%\n",
      "6\tValidation loss: 0.056134\tBest loss: 0.043288\tAccuracy: 98.12%\n",
      "7\tValidation loss: 0.084191\tBest loss: 0.043288\tAccuracy: 98.44%\n",
      "8\tValidation loss: 0.041741\tBest loss: 0.041741\tAccuracy: 99.02%\n",
      "9\tValidation loss: 0.053930\tBest loss: 0.041741\tAccuracy: 98.83%\n",
      "10\tValidation loss: 0.046577\tBest loss: 0.041741\tAccuracy: 98.75%\n",
      "11\tValidation loss: 0.078240\tBest loss: 0.041741\tAccuracy: 98.12%\n",
      "12\tValidation loss: 0.063192\tBest loss: 0.041741\tAccuracy: 98.63%\n",
      "13\tValidation loss: 0.087300\tBest loss: 0.041741\tAccuracy: 98.05%\n",
      "14\tValidation loss: 0.054696\tBest loss: 0.041741\tAccuracy: 98.71%\n",
      "15\tValidation loss: 0.092762\tBest loss: 0.041741\tAccuracy: 98.28%\n",
      "16\tValidation loss: 0.100090\tBest loss: 0.041741\tAccuracy: 97.89%\n",
      "17\tValidation loss: 0.048771\tBest loss: 0.041741\tAccuracy: 99.02%\n",
      "18\tValidation loss: 0.051515\tBest loss: 0.041741\tAccuracy: 98.79%\n",
      "19\tValidation loss: 0.044245\tBest loss: 0.041741\tAccuracy: 99.06%\n",
      "20\tValidation loss: 0.061905\tBest loss: 0.041741\tAccuracy: 98.71%\n",
      "21\tValidation loss: 0.054046\tBest loss: 0.041741\tAccuracy: 98.83%\n",
      "22\tValidation loss: 0.060309\tBest loss: 0.041741\tAccuracy: 98.91%\n",
      "23\tValidation loss: 0.059579\tBest loss: 0.041741\tAccuracy: 98.79%\n",
      "24\tValidation loss: 0.067436\tBest loss: 0.041741\tAccuracy: 98.71%\n",
      "25\tValidation loss: 0.068541\tBest loss: 0.041741\tAccuracy: 98.55%\n",
      "26\tValidation loss: 0.078464\tBest loss: 0.041741\tAccuracy: 98.55%\n",
      "27\tValidation loss: 0.056997\tBest loss: 0.041741\tAccuracy: 98.48%\n",
      "28\tValidation loss: 0.060297\tBest loss: 0.041741\tAccuracy: 99.10%\n",
      "29\tValidation loss: 0.047985\tBest loss: 0.041741\tAccuracy: 99.18%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, batch_size=100, batch_norm_momentum=0.99, activation=<function elu at 0x114248840>, total= 1.8min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.066194\tBest loss: 0.066194\tAccuracy: 98.20%\n",
      "1\tValidation loss: 0.064328\tBest loss: 0.064328\tAccuracy: 98.16%\n",
      "2\tValidation loss: 0.053789\tBest loss: 0.053789\tAccuracy: 98.32%\n",
      "3\tValidation loss: 0.081234\tBest loss: 0.053789\tAccuracy: 97.89%\n",
      "4\tValidation loss: 0.033694\tBest loss: 0.033694\tAccuracy: 99.14%\n",
      "5\tValidation loss: 0.047896\tBest loss: 0.033694\tAccuracy: 98.91%\n",
      "6\tValidation loss: 0.043604\tBest loss: 0.033694\tAccuracy: 98.59%\n",
      "7\tValidation loss: 0.040487\tBest loss: 0.033694\tAccuracy: 98.83%\n",
      "8\tValidation loss: 0.041978\tBest loss: 0.033694\tAccuracy: 98.83%\n",
      "9\tValidation loss: 0.058786\tBest loss: 0.033694\tAccuracy: 98.67%\n",
      "10\tValidation loss: 0.039213\tBest loss: 0.033694\tAccuracy: 98.83%\n",
      "11\tValidation loss: 0.047281\tBest loss: 0.033694\tAccuracy: 98.75%\n",
      "12\tValidation loss: 0.050140\tBest loss: 0.033694\tAccuracy: 98.94%\n",
      "13\tValidation loss: 0.046341\tBest loss: 0.033694\tAccuracy: 98.87%\n",
      "14\tValidation loss: 0.053872\tBest loss: 0.033694\tAccuracy: 98.67%\n",
      "15\tValidation loss: 0.049673\tBest loss: 0.033694\tAccuracy: 99.02%\n",
      "16\tValidation loss: 0.046261\tBest loss: 0.033694\tAccuracy: 99.06%\n",
      "17\tValidation loss: 0.042072\tBest loss: 0.033694\tAccuracy: 99.02%\n",
      "18\tValidation loss: 0.052911\tBest loss: 0.033694\tAccuracy: 99.06%\n",
      "19\tValidation loss: 0.071966\tBest loss: 0.033694\tAccuracy: 98.75%\n",
      "20\tValidation loss: 0.055778\tBest loss: 0.033694\tAccuracy: 98.75%\n",
      "21\tValidation loss: 0.060208\tBest loss: 0.033694\tAccuracy: 98.98%\n",
      "22\tValidation loss: 0.047584\tBest loss: 0.033694\tAccuracy: 98.87%\n",
      "23\tValidation loss: 0.050916\tBest loss: 0.033694\tAccuracy: 98.94%\n",
      "24\tValidation loss: 0.077873\tBest loss: 0.033694\tAccuracy: 98.63%\n",
      "25\tValidation loss: 0.055820\tBest loss: 0.033694\tAccuracy: 98.91%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=<function relu at 0x114259620>, total= 2.3min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.073741\tBest loss: 0.073741\tAccuracy: 97.50%\n",
      "1\tValidation loss: 0.063030\tBest loss: 0.063030\tAccuracy: 98.16%\n",
      "2\tValidation loss: 0.049455\tBest loss: 0.049455\tAccuracy: 98.44%\n",
      "3\tValidation loss: 0.043830\tBest loss: 0.043830\tAccuracy: 98.63%\n",
      "4\tValidation loss: 0.046722\tBest loss: 0.043830\tAccuracy: 98.51%\n",
      "5\tValidation loss: 0.046087\tBest loss: 0.043830\tAccuracy: 98.67%\n",
      "6\tValidation loss: 0.066873\tBest loss: 0.043830\tAccuracy: 98.24%\n",
      "7\tValidation loss: 0.049264\tBest loss: 0.043830\tAccuracy: 98.63%\n",
      "8\tValidation loss: 0.047673\tBest loss: 0.043830\tAccuracy: 98.71%\n",
      "9\tValidation loss: 0.052714\tBest loss: 0.043830\tAccuracy: 98.48%\n",
      "10\tValidation loss: 0.042631\tBest loss: 0.042631\tAccuracy: 98.98%\n",
      "11\tValidation loss: 0.034635\tBest loss: 0.034635\tAccuracy: 99.10%\n",
      "12\tValidation loss: 0.045187\tBest loss: 0.034635\tAccuracy: 98.87%\n",
      "13\tValidation loss: 0.054426\tBest loss: 0.034635\tAccuracy: 98.48%\n",
      "14\tValidation loss: 0.047377\tBest loss: 0.034635\tAccuracy: 98.44%\n",
      "15\tValidation loss: 0.042875\tBest loss: 0.034635\tAccuracy: 99.10%\n",
      "16\tValidation loss: 0.045952\tBest loss: 0.034635\tAccuracy: 98.79%\n",
      "17\tValidation loss: 0.058628\tBest loss: 0.034635\tAccuracy: 98.75%\n",
      "18\tValidation loss: 0.033251\tBest loss: 0.033251\tAccuracy: 98.91%\n",
      "19\tValidation loss: 0.038607\tBest loss: 0.033251\tAccuracy: 99.06%\n",
      "20\tValidation loss: 0.053248\tBest loss: 0.033251\tAccuracy: 98.87%\n",
      "21\tValidation loss: 0.048570\tBest loss: 0.033251\tAccuracy: 98.91%\n",
      "22\tValidation loss: 0.054574\tBest loss: 0.033251\tAccuracy: 98.79%\n",
      "23\tValidation loss: 0.043607\tBest loss: 0.033251\tAccuracy: 99.18%\n",
      "24\tValidation loss: 0.039549\tBest loss: 0.033251\tAccuracy: 98.87%\n",
      "25\tValidation loss: 0.032648\tBest loss: 0.032648\tAccuracy: 99.22%\n",
      "26\tValidation loss: 0.042238\tBest loss: 0.032648\tAccuracy: 98.98%\n",
      "27\tValidation loss: 0.047384\tBest loss: 0.032648\tAccuracy: 98.87%\n",
      "28\tValidation loss: 0.063250\tBest loss: 0.032648\tAccuracy: 98.71%\n",
      "29\tValidation loss: 0.037806\tBest loss: 0.032648\tAccuracy: 99.18%\n",
      "30\tValidation loss: 0.035682\tBest loss: 0.032648\tAccuracy: 99.18%\n",
      "31\tValidation loss: 0.040567\tBest loss: 0.032648\tAccuracy: 99.02%\n",
      "32\tValidation loss: 0.042267\tBest loss: 0.032648\tAccuracy: 98.98%\n",
      "33\tValidation loss: 0.036344\tBest loss: 0.032648\tAccuracy: 99.02%\n",
      "34\tValidation loss: 0.040331\tBest loss: 0.032648\tAccuracy: 99.22%\n",
      "35\tValidation loss: 0.037624\tBest loss: 0.032648\tAccuracy: 99.22%\n",
      "36\tValidation loss: 0.053279\tBest loss: 0.032648\tAccuracy: 98.83%\n",
      "37\tValidation loss: 0.050403\tBest loss: 0.032648\tAccuracy: 98.79%\n",
      "38\tValidation loss: 0.050700\tBest loss: 0.032648\tAccuracy: 98.94%\n",
      "39\tValidation loss: 0.034293\tBest loss: 0.032648\tAccuracy: 99.10%\n",
      "40\tValidation loss: 0.035793\tBest loss: 0.032648\tAccuracy: 99.18%\n",
      "41\tValidation loss: 0.032523\tBest loss: 0.032523\tAccuracy: 99.41%\n",
      "42\tValidation loss: 0.051484\tBest loss: 0.032523\tAccuracy: 99.14%\n",
      "43\tValidation loss: 0.052248\tBest loss: 0.032523\tAccuracy: 99.10%\n",
      "44\tValidation loss: 0.043014\tBest loss: 0.032523\tAccuracy: 99.14%\n",
      "45\tValidation loss: 0.047287\tBest loss: 0.032523\tAccuracy: 99.22%\n",
      "46\tValidation loss: 0.125447\tBest loss: 0.032523\tAccuracy: 98.12%\n",
      "47\tValidation loss: 0.044963\tBest loss: 0.032523\tAccuracy: 99.06%\n",
      "48\tValidation loss: 0.058030\tBest loss: 0.032523\tAccuracy: 99.10%\n",
      "49\tValidation loss: 0.039964\tBest loss: 0.032523\tAccuracy: 99.22%\n",
      "50\tValidation loss: 0.047356\tBest loss: 0.032523\tAccuracy: 99.06%\n",
      "51\tValidation loss: 0.051581\tBest loss: 0.032523\tAccuracy: 99.34%\n",
      "52\tValidation loss: 0.051083\tBest loss: 0.032523\tAccuracy: 99.26%\n",
      "53\tValidation loss: 0.042424\tBest loss: 0.032523\tAccuracy: 99.26%\n",
      "54\tValidation loss: 0.046922\tBest loss: 0.032523\tAccuracy: 98.98%\n",
      "55\tValidation loss: 0.044635\tBest loss: 0.032523\tAccuracy: 99.10%\n",
      "56\tValidation loss: 0.058718\tBest loss: 0.032523\tAccuracy: 98.94%\n",
      "57\tValidation loss: 0.050127\tBest loss: 0.032523\tAccuracy: 99.18%\n",
      "58\tValidation loss: 0.059930\tBest loss: 0.032523\tAccuracy: 98.98%\n",
      "59\tValidation loss: 0.049998\tBest loss: 0.032523\tAccuracy: 98.94%\n",
      "60\tValidation loss: 0.048315\tBest loss: 0.032523\tAccuracy: 99.06%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\tValidation loss: 0.047479\tBest loss: 0.032523\tAccuracy: 99.06%\n",
      "62\tValidation loss: 0.043045\tBest loss: 0.032523\tAccuracy: 99.06%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=<function relu at 0x114259620>, total= 5.5min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.068138\tBest loss: 0.068138\tAccuracy: 97.89%\n",
      "1\tValidation loss: 0.053356\tBest loss: 0.053356\tAccuracy: 98.28%\n",
      "2\tValidation loss: 0.087664\tBest loss: 0.053356\tAccuracy: 97.54%\n",
      "3\tValidation loss: 0.037894\tBest loss: 0.037894\tAccuracy: 98.67%\n",
      "4\tValidation loss: 0.036544\tBest loss: 0.036544\tAccuracy: 98.67%\n",
      "5\tValidation loss: 0.042214\tBest loss: 0.036544\tAccuracy: 98.71%\n",
      "6\tValidation loss: 0.062696\tBest loss: 0.036544\tAccuracy: 98.16%\n",
      "7\tValidation loss: 0.055235\tBest loss: 0.036544\tAccuracy: 98.59%\n",
      "8\tValidation loss: 0.043901\tBest loss: 0.036544\tAccuracy: 98.79%\n",
      "9\tValidation loss: 0.037141\tBest loss: 0.036544\tAccuracy: 98.83%\n",
      "10\tValidation loss: 0.040108\tBest loss: 0.036544\tAccuracy: 99.06%\n",
      "11\tValidation loss: 0.034944\tBest loss: 0.034944\tAccuracy: 99.02%\n",
      "12\tValidation loss: 0.046064\tBest loss: 0.034944\tAccuracy: 98.79%\n",
      "13\tValidation loss: 0.043475\tBest loss: 0.034944\tAccuracy: 99.02%\n",
      "14\tValidation loss: 0.049359\tBest loss: 0.034944\tAccuracy: 98.79%\n",
      "15\tValidation loss: 0.026154\tBest loss: 0.026154\tAccuracy: 99.45%\n",
      "16\tValidation loss: 0.034257\tBest loss: 0.026154\tAccuracy: 99.06%\n",
      "17\tValidation loss: 0.038560\tBest loss: 0.026154\tAccuracy: 99.06%\n",
      "18\tValidation loss: 0.045164\tBest loss: 0.026154\tAccuracy: 98.87%\n",
      "19\tValidation loss: 0.046388\tBest loss: 0.026154\tAccuracy: 99.10%\n",
      "20\tValidation loss: 0.044342\tBest loss: 0.026154\tAccuracy: 99.02%\n",
      "21\tValidation loss: 0.029273\tBest loss: 0.026154\tAccuracy: 99.34%\n",
      "22\tValidation loss: 0.037263\tBest loss: 0.026154\tAccuracy: 99.26%\n",
      "23\tValidation loss: 0.035989\tBest loss: 0.026154\tAccuracy: 99.18%\n",
      "24\tValidation loss: 0.038977\tBest loss: 0.026154\tAccuracy: 99.10%\n",
      "25\tValidation loss: 0.039605\tBest loss: 0.026154\tAccuracy: 99.18%\n",
      "26\tValidation loss: 0.051387\tBest loss: 0.026154\tAccuracy: 98.98%\n",
      "27\tValidation loss: 0.028177\tBest loss: 0.026154\tAccuracy: 99.26%\n",
      "28\tValidation loss: 0.035832\tBest loss: 0.026154\tAccuracy: 99.10%\n",
      "29\tValidation loss: 0.032448\tBest loss: 0.026154\tAccuracy: 99.02%\n",
      "30\tValidation loss: 0.046125\tBest loss: 0.026154\tAccuracy: 98.71%\n",
      "31\tValidation loss: 0.029890\tBest loss: 0.026154\tAccuracy: 99.34%\n",
      "32\tValidation loss: 0.035327\tBest loss: 0.026154\tAccuracy: 99.26%\n",
      "33\tValidation loss: 0.040301\tBest loss: 0.026154\tAccuracy: 99.26%\n",
      "34\tValidation loss: 0.028757\tBest loss: 0.026154\tAccuracy: 99.34%\n",
      "35\tValidation loss: 0.034236\tBest loss: 0.026154\tAccuracy: 99.22%\n",
      "36\tValidation loss: 0.056746\tBest loss: 0.026154\tAccuracy: 98.75%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.98, activation=<function relu at 0x114259620>, total= 3.3min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8> \n",
      "0\tValidation loss: 0.070111\tBest loss: 0.070111\tAccuracy: 97.77%\n",
      "1\tValidation loss: 0.056609\tBest loss: 0.056609\tAccuracy: 98.44%\n",
      "2\tValidation loss: 0.055998\tBest loss: 0.055998\tAccuracy: 98.32%\n",
      "3\tValidation loss: 0.053492\tBest loss: 0.053492\tAccuracy: 98.12%\n",
      "4\tValidation loss: 0.048778\tBest loss: 0.048778\tAccuracy: 98.55%\n",
      "5\tValidation loss: 0.045055\tBest loss: 0.045055\tAccuracy: 98.79%\n",
      "6\tValidation loss: 0.042560\tBest loss: 0.042560\tAccuracy: 98.75%\n",
      "7\tValidation loss: 0.048982\tBest loss: 0.042560\tAccuracy: 98.40%\n",
      "8\tValidation loss: 0.038200\tBest loss: 0.038200\tAccuracy: 99.14%\n",
      "9\tValidation loss: 0.039850\tBest loss: 0.038200\tAccuracy: 99.02%\n",
      "10\tValidation loss: 0.040263\tBest loss: 0.038200\tAccuracy: 98.98%\n",
      "11\tValidation loss: 0.034019\tBest loss: 0.034019\tAccuracy: 99.06%\n",
      "12\tValidation loss: 0.046902\tBest loss: 0.034019\tAccuracy: 98.79%\n",
      "13\tValidation loss: 0.039085\tBest loss: 0.034019\tAccuracy: 99.06%\n",
      "14\tValidation loss: 0.043371\tBest loss: 0.034019\tAccuracy: 98.83%\n",
      "15\tValidation loss: 0.036156\tBest loss: 0.034019\tAccuracy: 99.14%\n",
      "16\tValidation loss: 0.038893\tBest loss: 0.034019\tAccuracy: 99.18%\n",
      "17\tValidation loss: 0.067872\tBest loss: 0.034019\tAccuracy: 98.36%\n",
      "18\tValidation loss: 0.047960\tBest loss: 0.034019\tAccuracy: 98.67%\n",
      "19\tValidation loss: 0.051387\tBest loss: 0.034019\tAccuracy: 99.14%\n",
      "20\tValidation loss: 0.050344\tBest loss: 0.034019\tAccuracy: 98.94%\n",
      "21\tValidation loss: 0.050675\tBest loss: 0.034019\tAccuracy: 99.02%\n",
      "22\tValidation loss: 0.059533\tBest loss: 0.034019\tAccuracy: 98.91%\n",
      "23\tValidation loss: 0.044152\tBest loss: 0.034019\tAccuracy: 99.30%\n",
      "24\tValidation loss: 0.040925\tBest loss: 0.034019\tAccuracy: 99.30%\n",
      "25\tValidation loss: 0.035895\tBest loss: 0.034019\tAccuracy: 99.06%\n",
      "26\tValidation loss: 0.043940\tBest loss: 0.034019\tAccuracy: 99.10%\n",
      "27\tValidation loss: 0.044809\tBest loss: 0.034019\tAccuracy: 99.22%\n",
      "28\tValidation loss: 0.048635\tBest loss: 0.034019\tAccuracy: 99.02%\n",
      "29\tValidation loss: 0.051509\tBest loss: 0.034019\tAccuracy: 98.98%\n",
      "30\tValidation loss: 0.060177\tBest loss: 0.034019\tAccuracy: 99.10%\n",
      "31\tValidation loss: 0.046120\tBest loss: 0.034019\tAccuracy: 98.98%\n",
      "32\tValidation loss: 0.052532\tBest loss: 0.034019\tAccuracy: 98.98%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>, total= 3.1min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8> \n",
      "0\tValidation loss: 0.065577\tBest loss: 0.065577\tAccuracy: 97.97%\n",
      "1\tValidation loss: 0.050574\tBest loss: 0.050574\tAccuracy: 98.48%\n",
      "2\tValidation loss: 0.058215\tBest loss: 0.050574\tAccuracy: 98.32%\n",
      "3\tValidation loss: 0.043839\tBest loss: 0.043839\tAccuracy: 98.59%\n",
      "4\tValidation loss: 0.048982\tBest loss: 0.043839\tAccuracy: 98.79%\n",
      "5\tValidation loss: 0.046814\tBest loss: 0.043839\tAccuracy: 98.32%\n",
      "6\tValidation loss: 0.043141\tBest loss: 0.043141\tAccuracy: 98.87%\n",
      "7\tValidation loss: 0.045821\tBest loss: 0.043141\tAccuracy: 98.75%\n",
      "8\tValidation loss: 0.046249\tBest loss: 0.043141\tAccuracy: 98.75%\n",
      "9\tValidation loss: 0.047837\tBest loss: 0.043141\tAccuracy: 98.63%\n",
      "10\tValidation loss: 0.045664\tBest loss: 0.043141\tAccuracy: 98.75%\n",
      "11\tValidation loss: 0.051028\tBest loss: 0.043141\tAccuracy: 98.71%\n",
      "12\tValidation loss: 0.043229\tBest loss: 0.043141\tAccuracy: 98.87%\n",
      "13\tValidation loss: 0.043305\tBest loss: 0.043141\tAccuracy: 98.79%\n",
      "14\tValidation loss: 0.055216\tBest loss: 0.043141\tAccuracy: 98.75%\n",
      "15\tValidation loss: 0.043005\tBest loss: 0.043005\tAccuracy: 98.91%\n",
      "16\tValidation loss: 0.049904\tBest loss: 0.043005\tAccuracy: 98.55%\n",
      "17\tValidation loss: 0.049603\tBest loss: 0.043005\tAccuracy: 98.79%\n",
      "18\tValidation loss: 0.045548\tBest loss: 0.043005\tAccuracy: 98.91%\n",
      "19\tValidation loss: 0.042529\tBest loss: 0.042529\tAccuracy: 98.83%\n",
      "20\tValidation loss: 0.045389\tBest loss: 0.042529\tAccuracy: 99.02%\n",
      "21\tValidation loss: 0.042503\tBest loss: 0.042503\tAccuracy: 99.10%\n",
      "22\tValidation loss: 0.045446\tBest loss: 0.042503\tAccuracy: 98.83%\n",
      "23\tValidation loss: 0.042854\tBest loss: 0.042503\tAccuracy: 98.87%\n",
      "24\tValidation loss: 0.047913\tBest loss: 0.042503\tAccuracy: 99.02%\n",
      "25\tValidation loss: 0.047841\tBest loss: 0.042503\tAccuracy: 98.87%\n",
      "26\tValidation loss: 0.055471\tBest loss: 0.042503\tAccuracy: 98.71%\n",
      "27\tValidation loss: 0.051828\tBest loss: 0.042503\tAccuracy: 98.98%\n",
      "28\tValidation loss: 0.050032\tBest loss: 0.042503\tAccuracy: 98.91%\n",
      "29\tValidation loss: 0.040561\tBest loss: 0.040561\tAccuracy: 99.06%\n",
      "30\tValidation loss: 0.057293\tBest loss: 0.040561\tAccuracy: 98.87%\n",
      "31\tValidation loss: 0.053938\tBest loss: 0.040561\tAccuracy: 98.94%\n",
      "32\tValidation loss: 0.046033\tBest loss: 0.040561\tAccuracy: 98.94%\n",
      "33\tValidation loss: 0.031979\tBest loss: 0.031979\tAccuracy: 99.14%\n",
      "34\tValidation loss: 0.042598\tBest loss: 0.031979\tAccuracy: 99.26%\n",
      "35\tValidation loss: 0.038800\tBest loss: 0.031979\tAccuracy: 99.10%\n",
      "36\tValidation loss: 0.035676\tBest loss: 0.031979\tAccuracy: 99.06%\n",
      "37\tValidation loss: 0.043783\tBest loss: 0.031979\tAccuracy: 98.94%\n",
      "38\tValidation loss: 0.042672\tBest loss: 0.031979\tAccuracy: 99.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\tValidation loss: 0.059288\tBest loss: 0.031979\tAccuracy: 99.06%\n",
      "40\tValidation loss: 0.043143\tBest loss: 0.031979\tAccuracy: 99.14%\n",
      "41\tValidation loss: 0.051988\tBest loss: 0.031979\tAccuracy: 98.98%\n",
      "42\tValidation loss: 0.039454\tBest loss: 0.031979\tAccuracy: 99.10%\n",
      "43\tValidation loss: 0.040026\tBest loss: 0.031979\tAccuracy: 98.87%\n",
      "44\tValidation loss: 0.048535\tBest loss: 0.031979\tAccuracy: 99.14%\n",
      "45\tValidation loss: 0.056372\tBest loss: 0.031979\tAccuracy: 99.02%\n",
      "46\tValidation loss: 0.060254\tBest loss: 0.031979\tAccuracy: 98.63%\n",
      "47\tValidation loss: 0.049157\tBest loss: 0.031979\tAccuracy: 99.06%\n",
      "48\tValidation loss: 0.046662\tBest loss: 0.031979\tAccuracy: 99.02%\n",
      "49\tValidation loss: 0.050189\tBest loss: 0.031979\tAccuracy: 99.10%\n",
      "50\tValidation loss: 0.047874\tBest loss: 0.031979\tAccuracy: 99.18%\n",
      "51\tValidation loss: 0.042758\tBest loss: 0.031979\tAccuracy: 99.22%\n",
      "52\tValidation loss: 0.039419\tBest loss: 0.031979\tAccuracy: 99.30%\n",
      "53\tValidation loss: 0.043819\tBest loss: 0.031979\tAccuracy: 99.02%\n",
      "54\tValidation loss: 0.035928\tBest loss: 0.031979\tAccuracy: 99.22%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>, total= 5.4min\n",
      "[CV] n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8> \n",
      "0\tValidation loss: 0.084152\tBest loss: 0.084152\tAccuracy: 97.89%\n",
      "1\tValidation loss: 0.052841\tBest loss: 0.052841\tAccuracy: 98.48%\n",
      "2\tValidation loss: 0.046733\tBest loss: 0.046733\tAccuracy: 98.51%\n",
      "3\tValidation loss: 0.034369\tBest loss: 0.034369\tAccuracy: 98.79%\n",
      "4\tValidation loss: 0.039295\tBest loss: 0.034369\tAccuracy: 98.98%\n",
      "5\tValidation loss: 0.036501\tBest loss: 0.034369\tAccuracy: 98.91%\n",
      "6\tValidation loss: 0.034356\tBest loss: 0.034356\tAccuracy: 98.98%\n",
      "7\tValidation loss: 0.034358\tBest loss: 0.034356\tAccuracy: 98.94%\n",
      "8\tValidation loss: 0.043479\tBest loss: 0.034356\tAccuracy: 98.83%\n",
      "9\tValidation loss: 0.038563\tBest loss: 0.034356\tAccuracy: 98.75%\n",
      "10\tValidation loss: 0.028579\tBest loss: 0.028579\tAccuracy: 99.30%\n",
      "11\tValidation loss: 0.032195\tBest loss: 0.028579\tAccuracy: 99.06%\n",
      "12\tValidation loss: 0.036103\tBest loss: 0.028579\tAccuracy: 98.98%\n",
      "13\tValidation loss: 0.038263\tBest loss: 0.028579\tAccuracy: 99.02%\n",
      "14\tValidation loss: 0.041794\tBest loss: 0.028579\tAccuracy: 98.94%\n",
      "15\tValidation loss: 0.032547\tBest loss: 0.028579\tAccuracy: 99.06%\n",
      "16\tValidation loss: 0.038408\tBest loss: 0.028579\tAccuracy: 98.98%\n",
      "17\tValidation loss: 0.034877\tBest loss: 0.028579\tAccuracy: 99.14%\n",
      "18\tValidation loss: 0.034269\tBest loss: 0.028579\tAccuracy: 98.91%\n",
      "19\tValidation loss: 0.030583\tBest loss: 0.028579\tAccuracy: 99.14%\n",
      "20\tValidation loss: 0.040356\tBest loss: 0.028579\tAccuracy: 98.87%\n",
      "21\tValidation loss: 0.029516\tBest loss: 0.028579\tAccuracy: 99.37%\n",
      "22\tValidation loss: 0.036210\tBest loss: 0.028579\tAccuracy: 99.10%\n",
      "23\tValidation loss: 0.026685\tBest loss: 0.026685\tAccuracy: 99.37%\n",
      "24\tValidation loss: 0.043867\tBest loss: 0.026685\tAccuracy: 99.22%\n",
      "25\tValidation loss: 0.033288\tBest loss: 0.026685\tAccuracy: 99.10%\n",
      "26\tValidation loss: 0.044957\tBest loss: 0.026685\tAccuracy: 99.14%\n",
      "27\tValidation loss: 0.043655\tBest loss: 0.026685\tAccuracy: 98.98%\n",
      "28\tValidation loss: 0.046256\tBest loss: 0.026685\tAccuracy: 98.98%\n",
      "29\tValidation loss: 0.043905\tBest loss: 0.026685\tAccuracy: 98.91%\n",
      "30\tValidation loss: 0.033134\tBest loss: 0.026685\tAccuracy: 99.06%\n",
      "31\tValidation loss: 0.053219\tBest loss: 0.026685\tAccuracy: 98.94%\n",
      "32\tValidation loss: 0.038281\tBest loss: 0.026685\tAccuracy: 98.94%\n",
      "33\tValidation loss: 0.036261\tBest loss: 0.026685\tAccuracy: 98.87%\n",
      "34\tValidation loss: 0.042288\tBest loss: 0.026685\tAccuracy: 99.18%\n",
      "35\tValidation loss: 0.038501\tBest loss: 0.026685\tAccuracy: 99.37%\n",
      "36\tValidation loss: 0.032818\tBest loss: 0.026685\tAccuracy: 99.14%\n",
      "37\tValidation loss: 0.032422\tBest loss: 0.026685\tAccuracy: 99.18%\n",
      "38\tValidation loss: 0.044814\tBest loss: 0.026685\tAccuracy: 99.22%\n",
      "39\tValidation loss: 0.054353\tBest loss: 0.026685\tAccuracy: 98.98%\n",
      "40\tValidation loss: 0.032920\tBest loss: 0.026685\tAccuracy: 99.37%\n",
      "41\tValidation loss: 0.041832\tBest loss: 0.026685\tAccuracy: 99.10%\n",
      "42\tValidation loss: 0.033053\tBest loss: 0.026685\tAccuracy: 99.22%\n",
      "43\tValidation loss: 0.034532\tBest loss: 0.026685\tAccuracy: 99.22%\n",
      "44\tValidation loss: 0.038104\tBest loss: 0.026685\tAccuracy: 99.41%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, batch_size=50, batch_norm_momentum=0.9, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>, total= 5.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 158.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.086505\tBest loss: 0.086505\tAccuracy: 97.69%\n",
      "1\tValidation loss: 0.051801\tBest loss: 0.051801\tAccuracy: 98.20%\n",
      "2\tValidation loss: 0.034452\tBest loss: 0.034452\tAccuracy: 99.02%\n",
      "3\tValidation loss: 0.042590\tBest loss: 0.034452\tAccuracy: 98.67%\n",
      "4\tValidation loss: 0.046777\tBest loss: 0.034452\tAccuracy: 98.44%\n",
      "5\tValidation loss: 0.033430\tBest loss: 0.033430\tAccuracy: 98.91%\n",
      "6\tValidation loss: 0.040356\tBest loss: 0.033430\tAccuracy: 99.02%\n",
      "7\tValidation loss: 0.038254\tBest loss: 0.033430\tAccuracy: 99.02%\n",
      "8\tValidation loss: 0.047743\tBest loss: 0.033430\tAccuracy: 98.71%\n",
      "9\tValidation loss: 0.035001\tBest loss: 0.033430\tAccuracy: 98.75%\n",
      "10\tValidation loss: 0.043047\tBest loss: 0.033430\tAccuracy: 99.06%\n",
      "11\tValidation loss: 0.048549\tBest loss: 0.033430\tAccuracy: 99.14%\n",
      "12\tValidation loss: 0.034620\tBest loss: 0.033430\tAccuracy: 99.10%\n",
      "13\tValidation loss: 0.039632\tBest loss: 0.033430\tAccuracy: 98.98%\n",
      "14\tValidation loss: 0.037619\tBest loss: 0.033430\tAccuracy: 99.10%\n",
      "15\tValidation loss: 0.040361\tBest loss: 0.033430\tAccuracy: 98.98%\n",
      "16\tValidation loss: 0.040829\tBest loss: 0.033430\tAccuracy: 99.02%\n",
      "17\tValidation loss: 0.041895\tBest loss: 0.033430\tAccuracy: 98.98%\n",
      "18\tValidation loss: 0.035153\tBest loss: 0.033430\tAccuracy: 99.10%\n",
      "19\tValidation loss: 0.035009\tBest loss: 0.033430\tAccuracy: 99.34%\n",
      "20\tValidation loss: 0.050117\tBest loss: 0.033430\tAccuracy: 98.79%\n",
      "21\tValidation loss: 0.039771\tBest loss: 0.033430\tAccuracy: 98.87%\n",
      "22\tValidation loss: 0.026098\tBest loss: 0.026098\tAccuracy: 99.14%\n",
      "23\tValidation loss: 0.033777\tBest loss: 0.026098\tAccuracy: 99.10%\n",
      "24\tValidation loss: 0.044168\tBest loss: 0.026098\tAccuracy: 98.94%\n",
      "25\tValidation loss: 0.029046\tBest loss: 0.026098\tAccuracy: 99.30%\n",
      "26\tValidation loss: 0.031794\tBest loss: 0.026098\tAccuracy: 99.30%\n",
      "27\tValidation loss: 0.033537\tBest loss: 0.026098\tAccuracy: 99.26%\n",
      "28\tValidation loss: 0.048033\tBest loss: 0.026098\tAccuracy: 99.10%\n",
      "29\tValidation loss: 0.037148\tBest loss: 0.026098\tAccuracy: 99.34%\n",
      "30\tValidation loss: 0.056691\tBest loss: 0.026098\tAccuracy: 98.87%\n",
      "31\tValidation loss: 0.040974\tBest loss: 0.026098\tAccuracy: 98.75%\n",
      "32\tValidation loss: 0.036115\tBest loss: 0.026098\tAccuracy: 99.45%\n",
      "33\tValidation loss: 0.034313\tBest loss: 0.026098\tAccuracy: 99.45%\n",
      "34\tValidation loss: 0.036399\tBest loss: 0.026098\tAccuracy: 99.18%\n",
      "35\tValidation loss: 0.054454\tBest loss: 0.026098\tAccuracy: 98.98%\n",
      "36\tValidation loss: 0.037326\tBest loss: 0.026098\tAccuracy: 99.37%\n",
      "37\tValidation loss: 0.042152\tBest loss: 0.026098\tAccuracy: 99.37%\n",
      "38\tValidation loss: 0.036092\tBest loss: 0.026098\tAccuracy: 99.18%\n",
      "39\tValidation loss: 0.035338\tBest loss: 0.026098\tAccuracy: 99.41%\n",
      "40\tValidation loss: 0.033661\tBest loss: 0.026098\tAccuracy: 99.30%\n",
      "41\tValidation loss: 0.032619\tBest loss: 0.026098\tAccuracy: 99.14%\n",
      "42\tValidation loss: 0.047852\tBest loss: 0.026098\tAccuracy: 99.18%\n",
      "43\tValidation loss: 0.029562\tBest loss: 0.026098\tAccuracy: 99.45%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=DNNClassifier(activation=<function elu at 0x114248840>,\n",
       "                                           batch_norm_momentum=None,\n",
       "                                           batch_size=20, dropout_rate=None,\n",
       "                                           initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "                                           learning_rate=0.01,\n",
       "                                           n_hidden_layers=5, n_neurons=100,\n",
       "                                           optimizer_class=<class 'tensorflow.python.tra...\n",
       "                                                       <function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>,\n",
       "                                                       <function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25488>],\n",
       "                                        'batch_norm_momentum': [0.9, 0.95, 0.98,\n",
       "                                                                0.99, 0.999],\n",
       "                                        'batch_size': [10, 50, 100, 500],\n",
       "                                        'learning_rate': [0.01, 0.02, 0.05,\n",
       "                                                          0.1],\n",
       "                                        'n_neurons': [10, 30, 50, 70, 90, 100,\n",
       "                                                      120, 140, 160]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {\"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "                  \"batch_size\": [10, 50, 100, 500],\n",
    "                  \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "                  \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "                  \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999]}\n",
    "                  # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "                  # \"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "                  # \"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)]\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs,\n",
    "                                   n_iter=10, cv=3, random_state=42, verbose=2) # default: n_iter=50\n",
    "rnd_search_bn.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best parameters and check the accuracy on the test set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 120, 'learning_rate': 0.01, 'batch_size': 50, 'batch_norm_momentum': 0.9, 'activation': <function leaky_relu.<locals>.parametrized_leaky_relu at 0x125d25bf8>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9918272037361354"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rnd_search_bn.best_params_)\n",
    "y_pred = rnd_search_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... (really good) and on the entire training set (should be better – and it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985376988372923"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf.predict(X_train1)\n",
    "accuracy_score(y_train1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for dropout!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.115947\tBest loss: 0.115947\tAccuracy: 96.87%\n",
      "1\tValidation loss: 0.092185\tBest loss: 0.092185\tAccuracy: 97.54%\n",
      "2\tValidation loss: 0.090862\tBest loss: 0.090862\tAccuracy: 97.38%\n",
      "3\tValidation loss: 0.080346\tBest loss: 0.080346\tAccuracy: 97.73%\n",
      "4\tValidation loss: 0.081874\tBest loss: 0.080346\tAccuracy: 98.12%\n",
      "5\tValidation loss: 0.079668\tBest loss: 0.079668\tAccuracy: 98.01%\n",
      "6\tValidation loss: 0.074473\tBest loss: 0.074473\tAccuracy: 97.97%\n",
      "7\tValidation loss: 0.077893\tBest loss: 0.074473\tAccuracy: 98.08%\n",
      "8\tValidation loss: 0.073062\tBest loss: 0.073062\tAccuracy: 98.05%\n",
      "9\tValidation loss: 0.075687\tBest loss: 0.073062\tAccuracy: 98.12%\n",
      "10\tValidation loss: 0.074898\tBest loss: 0.073062\tAccuracy: 97.89%\n",
      "11\tValidation loss: 0.067346\tBest loss: 0.067346\tAccuracy: 98.32%\n",
      "12\tValidation loss: 0.069648\tBest loss: 0.067346\tAccuracy: 98.16%\n",
      "13\tValidation loss: 0.066984\tBest loss: 0.066984\tAccuracy: 98.28%\n",
      "14\tValidation loss: 0.076502\tBest loss: 0.066984\tAccuracy: 97.97%\n",
      "15\tValidation loss: 0.077080\tBest loss: 0.066984\tAccuracy: 98.20%\n",
      "16\tValidation loss: 0.067103\tBest loss: 0.066984\tAccuracy: 98.12%\n",
      "17\tValidation loss: 0.065436\tBest loss: 0.065436\tAccuracy: 98.16%\n",
      "18\tValidation loss: 0.065094\tBest loss: 0.065094\tAccuracy: 98.32%\n",
      "19\tValidation loss: 0.066693\tBest loss: 0.065094\tAccuracy: 98.08%\n",
      "20\tValidation loss: 0.076955\tBest loss: 0.065094\tAccuracy: 97.73%\n",
      "21\tValidation loss: 0.067998\tBest loss: 0.065094\tAccuracy: 98.08%\n",
      "22\tValidation loss: 0.066570\tBest loss: 0.065094\tAccuracy: 98.51%\n",
      "23\tValidation loss: 0.071400\tBest loss: 0.065094\tAccuracy: 98.05%\n",
      "24\tValidation loss: 0.075756\tBest loss: 0.065094\tAccuracy: 97.97%\n",
      "25\tValidation loss: 0.068179\tBest loss: 0.065094\tAccuracy: 98.44%\n",
      "26\tValidation loss: 0.074532\tBest loss: 0.065094\tAccuracy: 98.05%\n",
      "27\tValidation loss: 0.076975\tBest loss: 0.065094\tAccuracy: 97.97%\n",
      "28\tValidation loss: 0.086087\tBest loss: 0.065094\tAccuracy: 97.77%\n",
      "29\tValidation loss: 0.103882\tBest loss: 0.065094\tAccuracy: 97.62%\n",
      "30\tValidation loss: 0.146061\tBest loss: 0.065094\tAccuracy: 95.78%\n",
      "31\tValidation loss: 0.135519\tBest loss: 0.065094\tAccuracy: 96.40%\n",
      "32\tValidation loss: 0.115818\tBest loss: 0.065094\tAccuracy: 96.52%\n",
      "33\tValidation loss: 0.141345\tBest loss: 0.065094\tAccuracy: 96.95%\n",
      "34\tValidation loss: 0.133706\tBest loss: 0.065094\tAccuracy: 96.52%\n",
      "35\tValidation loss: 0.161927\tBest loss: 0.065094\tAccuracy: 95.97%\n",
      "36\tValidation loss: 0.587136\tBest loss: 0.065094\tAccuracy: 85.77%\n",
      "37\tValidation loss: 0.160235\tBest loss: 0.065094\tAccuracy: 95.00%\n",
      "38\tValidation loss: 0.179055\tBest loss: 0.065094\tAccuracy: 93.20%\n",
      "39\tValidation loss: 0.154282\tBest loss: 0.065094\tAccuracy: 93.98%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DNNClassifier(activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x1427821e0>,\n",
       "              batch_norm_momentum=None, batch_size=500, dropout_rate=0.5,\n",
       "              initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "              learning_rate=0.01, n_hidden_layers=5, n_neurons=90,\n",
       "              optimizer_class=<class 'tensorflow.python.training.adam.AdamOptimizer'>,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42, dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we can roughly expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873516248297334"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fair enough. Let's run the randomized search, now including the `dropout_rate` as a hyperparameter. Batch normalization is discontinued, here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 2.038991\tBest loss: 2.038991\tAccuracy: 18.73%\n",
      "1\tValidation loss: 2.394311\tBest loss: 2.038991\tAccuracy: 18.73%\n",
      "2\tValidation loss: 2.661509\tBest loss: 2.038991\tAccuracy: 19.27%\n",
      "3\tValidation loss: 3.156009\tBest loss: 2.038991\tAccuracy: 22.01%\n",
      "4\tValidation loss: 2.579127\tBest loss: 2.038991\tAccuracy: 18.73%\n",
      "5\tValidation loss: 2.009854\tBest loss: 2.009854\tAccuracy: 22.01%\n",
      "6\tValidation loss: 2.297004\tBest loss: 2.009854\tAccuracy: 19.08%\n",
      "7\tValidation loss: 2.015415\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "8\tValidation loss: 2.098555\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "9\tValidation loss: 2.229317\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "10\tValidation loss: 2.435627\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "11\tValidation loss: 3.293192\tBest loss: 2.009854\tAccuracy: 19.08%\n",
      "12\tValidation loss: 2.197180\tBest loss: 2.009854\tAccuracy: 19.08%\n",
      "13\tValidation loss: 2.357098\tBest loss: 2.009854\tAccuracy: 22.01%\n",
      "14\tValidation loss: 3.546944\tBest loss: 2.009854\tAccuracy: 18.73%\n",
      "15\tValidation loss: 2.195999\tBest loss: 2.009854\tAccuracy: 22.01%\n",
      "16\tValidation loss: 2.542377\tBest loss: 2.009854\tAccuracy: 18.73%\n",
      "17\tValidation loss: 2.755081\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "18\tValidation loss: 3.244885\tBest loss: 2.009854\tAccuracy: 22.01%\n",
      "19\tValidation loss: 3.688083\tBest loss: 2.009854\tAccuracy: 22.01%\n",
      "20\tValidation loss: 2.531843\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "21\tValidation loss: 2.160419\tBest loss: 2.009854\tAccuracy: 20.91%\n",
      "22\tValidation loss: 1.723818\tBest loss: 1.723818\tAccuracy: 18.73%\n",
      "23\tValidation loss: 2.545223\tBest loss: 1.723818\tAccuracy: 20.91%\n",
      "24\tValidation loss: 2.034534\tBest loss: 1.723818\tAccuracy: 20.91%\n",
      "25\tValidation loss: 2.485674\tBest loss: 1.723818\tAccuracy: 19.27%\n",
      "26\tValidation loss: 2.657391\tBest loss: 1.723818\tAccuracy: 20.91%\n",
      "27\tValidation loss: 1.869182\tBest loss: 1.723818\tAccuracy: 19.08%\n",
      "28\tValidation loss: 2.967032\tBest loss: 1.723818\tAccuracy: 19.08%\n",
      "29\tValidation loss: 3.408942\tBest loss: 1.723818\tAccuracy: 19.08%\n",
      "30\tValidation loss: 1.849644\tBest loss: 1.723818\tAccuracy: 20.91%\n",
      "31\tValidation loss: 2.778171\tBest loss: 1.723818\tAccuracy: 19.08%\n",
      "32\tValidation loss: 2.714841\tBest loss: 1.723818\tAccuracy: 19.27%\n",
      "33\tValidation loss: 3.779350\tBest loss: 1.723818\tAccuracy: 19.08%\n",
      "34\tValidation loss: 1.962069\tBest loss: 1.723818\tAccuracy: 22.01%\n",
      "35\tValidation loss: 4.478134\tBest loss: 1.723818\tAccuracy: 20.91%\n",
      "36\tValidation loss: 3.063630\tBest loss: 1.723818\tAccuracy: 22.01%\n",
      "37\tValidation loss: 2.285158\tBest loss: 1.723818\tAccuracy: 18.73%\n",
      "38\tValidation loss: 2.102220\tBest loss: 1.723818\tAccuracy: 18.73%\n",
      "39\tValidation loss: 3.406659\tBest loss: 1.723818\tAccuracy: 19.27%\n",
      "40\tValidation loss: 2.547287\tBest loss: 1.723818\tAccuracy: 22.01%\n",
      "41\tValidation loss: 2.616271\tBest loss: 1.723818\tAccuracy: 20.91%\n",
      "42\tValidation loss: 2.598369\tBest loss: 1.723818\tAccuracy: 19.27%\n",
      "43\tValidation loss: 3.057254\tBest loss: 1.723818\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=<function elu at 0x114248840>, total= 5.1min\n",
      "[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 1.664894\tBest loss: 1.664894\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.927515\tBest loss: 1.664894\tAccuracy: 22.01%\n",
      "2\tValidation loss: 1.814280\tBest loss: 1.664894\tAccuracy: 19.08%\n",
      "3\tValidation loss: 2.437774\tBest loss: 1.664894\tAccuracy: 22.01%\n",
      "4\tValidation loss: 2.357125\tBest loss: 1.664894\tAccuracy: 22.01%\n",
      "5\tValidation loss: 2.299808\tBest loss: 1.664894\tAccuracy: 18.73%\n",
      "6\tValidation loss: 3.094177\tBest loss: 1.664894\tAccuracy: 18.73%\n",
      "7\tValidation loss: 2.089147\tBest loss: 1.664894\tAccuracy: 19.08%\n",
      "8\tValidation loss: 3.215371\tBest loss: 1.664894\tAccuracy: 18.73%\n",
      "9\tValidation loss: 2.244811\tBest loss: 1.664894\tAccuracy: 22.01%\n",
      "10\tValidation loss: 2.779113\tBest loss: 1.664894\tAccuracy: 20.91%\n",
      "11\tValidation loss: 4.502943\tBest loss: 1.664894\tAccuracy: 19.08%\n",
      "12\tValidation loss: 3.149274\tBest loss: 1.664894\tAccuracy: 19.08%\n",
      "13\tValidation loss: 3.544765\tBest loss: 1.664894\tAccuracy: 20.91%\n",
      "14\tValidation loss: 3.421386\tBest loss: 1.664894\tAccuracy: 20.91%\n",
      "15\tValidation loss: 3.655596\tBest loss: 1.664894\tAccuracy: 20.91%\n",
      "16\tValidation loss: 2.053710\tBest loss: 1.664894\tAccuracy: 20.91%\n",
      "17\tValidation loss: 3.057725\tBest loss: 1.664894\tAccuracy: 19.08%\n",
      "18\tValidation loss: 2.568397\tBest loss: 1.664894\tAccuracy: 19.27%\n",
      "19\tValidation loss: 2.575650\tBest loss: 1.664894\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.747947\tBest loss: 1.664894\tAccuracy: 18.73%\n",
      "21\tValidation loss: 1.959564\tBest loss: 1.664894\tAccuracy: 19.27%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=<function elu at 0x114248840>, total= 2.7min\n",
      "[CV] n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 1.842584\tBest loss: 1.842584\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1.920646\tBest loss: 1.842584\tAccuracy: 18.73%\n",
      "2\tValidation loss: 2.576388\tBest loss: 1.842584\tAccuracy: 19.08%\n",
      "3\tValidation loss: 2.238076\tBest loss: 1.842584\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.798920\tBest loss: 1.798920\tAccuracy: 20.91%\n",
      "5\tValidation loss: 3.233273\tBest loss: 1.798920\tAccuracy: 20.91%\n",
      "6\tValidation loss: 2.017239\tBest loss: 1.798920\tAccuracy: 19.08%\n",
      "7\tValidation loss: 2.240717\tBest loss: 1.798920\tAccuracy: 19.08%\n",
      "8\tValidation loss: 1.954389\tBest loss: 1.798920\tAccuracy: 22.01%\n",
      "9\tValidation loss: 3.409244\tBest loss: 1.798920\tAccuracy: 22.01%\n",
      "10\tValidation loss: 3.239856\tBest loss: 1.798920\tAccuracy: 18.73%\n",
      "11\tValidation loss: 3.047362\tBest loss: 1.798920\tAccuracy: 18.73%\n",
      "12\tValidation loss: 2.501650\tBest loss: 1.798920\tAccuracy: 18.73%\n",
      "13\tValidation loss: 4.068747\tBest loss: 1.798920\tAccuracy: 19.08%\n",
      "14\tValidation loss: 2.169908\tBest loss: 1.798920\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.779742\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "16\tValidation loss: 3.085422\tBest loss: 1.779742\tAccuracy: 20.91%\n",
      "17\tValidation loss: 2.651036\tBest loss: 1.779742\tAccuracy: 18.73%\n",
      "18\tValidation loss: 2.696090\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "19\tValidation loss: 2.872313\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "20\tValidation loss: 2.941484\tBest loss: 1.779742\tAccuracy: 22.01%\n",
      "21\tValidation loss: 2.165803\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "22\tValidation loss: 2.554505\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "23\tValidation loss: 3.051398\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "24\tValidation loss: 2.215387\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "25\tValidation loss: 2.184484\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "26\tValidation loss: 1.998534\tBest loss: 1.779742\tAccuracy: 18.73%\n",
      "27\tValidation loss: 2.507367\tBest loss: 1.779742\tAccuracy: 20.91%\n",
      "28\tValidation loss: 2.411195\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "29\tValidation loss: 4.259584\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "30\tValidation loss: 1.947190\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "31\tValidation loss: 2.283673\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "32\tValidation loss: 2.749190\tBest loss: 1.779742\tAccuracy: 19.27%\n",
      "33\tValidation loss: 3.109326\tBest loss: 1.779742\tAccuracy: 22.01%\n",
      "34\tValidation loss: 2.356573\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "35\tValidation loss: 2.384276\tBest loss: 1.779742\tAccuracy: 19.08%\n",
      "36\tValidation loss: 2.583924\tBest loss: 1.779742\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.1, dropout_rate=0.5, batch_size=10, activation=<function elu at 0x114248840>, total= 4.4min\n",
      "[CV] n_neurons=140, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 2.677152\tBest loss: 2.677152\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.611978\tBest loss: 1.611978\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.613561\tBest loss: 1.611978\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.624212\tBest loss: 1.611978\tAccuracy: 18.73%\n",
      "4\tValidation loss: 1.629560\tBest loss: 1.611978\tAccuracy: 18.73%\n",
      "5\tValidation loss: 1.611452\tBest loss: 1.611452\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.620830\tBest loss: 1.611452\tAccuracy: 22.01%\n",
      "7\tValidation loss: 1.642277\tBest loss: 1.611452\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.614585\tBest loss: 1.611452\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.624631\tBest loss: 1.611452\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.618108\tBest loss: 1.611452\tAccuracy: 20.91%\n",
      "11\tValidation loss: 1.628746\tBest loss: 1.611452\tAccuracy: 18.73%\n",
      "12\tValidation loss: 1.617237\tBest loss: 1.611452\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.673980\tBest loss: 1.611452\tAccuracy: 19.27%\n",
      "14\tValidation loss: 1.636072\tBest loss: 1.611452\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.642241\tBest loss: 1.611452\tAccuracy: 19.27%\n",
      "16\tValidation loss: 1.629208\tBest loss: 1.611452\tAccuracy: 20.91%\n",
      "17\tValidation loss: 1.618958\tBest loss: 1.611452\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.630114\tBest loss: 1.611452\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.609817\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.659108\tBest loss: 1.609817\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.652593\tBest loss: 1.609817\tAccuracy: 18.73%\n",
      "22\tValidation loss: 1.677958\tBest loss: 1.609817\tAccuracy: 18.73%\n",
      "23\tValidation loss: 1.629297\tBest loss: 1.609817\tAccuracy: 19.08%\n",
      "24\tValidation loss: 1.624437\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "25\tValidation loss: 1.632437\tBest loss: 1.609817\tAccuracy: 19.08%\n",
      "26\tValidation loss: 1.624675\tBest loss: 1.609817\tAccuracy: 18.73%\n",
      "27\tValidation loss: 1.676263\tBest loss: 1.609817\tAccuracy: 19.08%\n",
      "28\tValidation loss: 1.621148\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "29\tValidation loss: 1.689653\tBest loss: 1.609817\tAccuracy: 18.73%\n",
      "30\tValidation loss: 1.644436\tBest loss: 1.609817\tAccuracy: 19.27%\n",
      "31\tValidation loss: 1.644208\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "32\tValidation loss: 1.610551\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "33\tValidation loss: 1.628053\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "34\tValidation loss: 1.656008\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.624421\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "36\tValidation loss: 1.646134\tBest loss: 1.609817\tAccuracy: 20.91%\n",
      "37\tValidation loss: 1.647799\tBest loss: 1.609817\tAccuracy: 18.73%\n",
      "38\tValidation loss: 1.721433\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "39\tValidation loss: 1.625620\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "40\tValidation loss: 1.681150\tBest loss: 1.609817\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=<function elu at 0x114248840>, total= 1.3min\n",
      "[CV] n_neurons=140, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 2.506119\tBest loss: 2.506119\tAccuracy: 24.55%\n",
      "1\tValidation loss: 1.624185\tBest loss: 1.624185\tAccuracy: 18.73%\n",
      "2\tValidation loss: 1.459566\tBest loss: 1.459566\tAccuracy: 28.42%\n",
      "3\tValidation loss: 1.414171\tBest loss: 1.414171\tAccuracy: 30.81%\n",
      "4\tValidation loss: 1.614910\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.609340\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.620297\tBest loss: 1.414171\tAccuracy: 19.08%\n",
      "7\tValidation loss: 1.609780\tBest loss: 1.414171\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.614756\tBest loss: 1.414171\tAccuracy: 20.91%\n",
      "9\tValidation loss: 1.635073\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.633376\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.633779\tBest loss: 1.414171\tAccuracy: 18.73%\n",
      "12\tValidation loss: 1.637240\tBest loss: 1.414171\tAccuracy: 19.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\tValidation loss: 1.635910\tBest loss: 1.414171\tAccuracy: 19.27%\n",
      "14\tValidation loss: 1.624897\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.619846\tBest loss: 1.414171\tAccuracy: 20.91%\n",
      "16\tValidation loss: 1.661598\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.632120\tBest loss: 1.414171\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.653340\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.621119\tBest loss: 1.414171\tAccuracy: 20.91%\n",
      "20\tValidation loss: 1.634872\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.647739\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.622865\tBest loss: 1.414171\tAccuracy: 22.01%\n",
      "23\tValidation loss: 1.658472\tBest loss: 1.414171\tAccuracy: 19.27%\n",
      "24\tValidation loss: 1.624927\tBest loss: 1.414171\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=<function elu at 0x114248840>, total=  49.7s\n",
      "[CV] n_neurons=140, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 1.793705\tBest loss: 1.793705\tAccuracy: 20.91%\n",
      "1\tValidation loss: 1.624446\tBest loss: 1.624446\tAccuracy: 20.91%\n",
      "2\tValidation loss: 1.613189\tBest loss: 1.613189\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.620390\tBest loss: 1.613189\tAccuracy: 18.73%\n",
      "4\tValidation loss: 1.610375\tBest loss: 1.610375\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.614338\tBest loss: 1.610375\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.608895\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "7\tValidation loss: 1.630360\tBest loss: 1.608895\tAccuracy: 18.73%\n",
      "8\tValidation loss: 1.631692\tBest loss: 1.608895\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.635393\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.619120\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.657873\tBest loss: 1.608895\tAccuracy: 18.73%\n",
      "12\tValidation loss: 1.616619\tBest loss: 1.608895\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.643754\tBest loss: 1.608895\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.623652\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.646042\tBest loss: 1.608895\tAccuracy: 19.08%\n",
      "16\tValidation loss: 1.634822\tBest loss: 1.608895\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.659235\tBest loss: 1.608895\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.614273\tBest loss: 1.608895\tAccuracy: 20.91%\n",
      "19\tValidation loss: 1.617036\tBest loss: 1.608895\tAccuracy: 20.91%\n",
      "20\tValidation loss: 1.622355\tBest loss: 1.608895\tAccuracy: 19.08%\n",
      "21\tValidation loss: 1.645830\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.641108\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "23\tValidation loss: 1.665024\tBest loss: 1.608895\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.653067\tBest loss: 1.608895\tAccuracy: 20.91%\n",
      "25\tValidation loss: 1.662871\tBest loss: 1.608895\tAccuracy: 19.27%\n",
      "26\tValidation loss: 1.687609\tBest loss: 1.608895\tAccuracy: 18.73%\n",
      "27\tValidation loss: 1.666242\tBest loss: 1.608895\tAccuracy: 19.27%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, dropout_rate=0.2, batch_size=500, activation=<function elu at 0x114248840>, total=  55.5s\n",
      "[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.143885\tBest loss: 0.143885\tAccuracy: 96.52%\n",
      "1\tValidation loss: 0.139493\tBest loss: 0.139493\tAccuracy: 96.29%\n",
      "2\tValidation loss: 1.788617\tBest loss: 0.139493\tAccuracy: 19.08%\n",
      "3\tValidation loss: 1.624425\tBest loss: 0.139493\tAccuracy: 20.91%\n",
      "4\tValidation loss: 1.709311\tBest loss: 0.139493\tAccuracy: 19.27%\n",
      "5\tValidation loss: 1.614482\tBest loss: 0.139493\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.657638\tBest loss: 0.139493\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.645140\tBest loss: 0.139493\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.691236\tBest loss: 0.139493\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.847037\tBest loss: 0.139493\tAccuracy: 19.08%\n",
      "10\tValidation loss: 1.658911\tBest loss: 0.139493\tAccuracy: 19.08%\n",
      "11\tValidation loss: 1.664693\tBest loss: 0.139493\tAccuracy: 18.73%\n",
      "12\tValidation loss: 1.623274\tBest loss: 0.139493\tAccuracy: 18.73%\n",
      "13\tValidation loss: 1.622542\tBest loss: 0.139493\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.651223\tBest loss: 0.139493\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.682043\tBest loss: 0.139493\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.658643\tBest loss: 0.139493\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.681453\tBest loss: 0.139493\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.659770\tBest loss: 0.139493\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.643169\tBest loss: 0.139493\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.681604\tBest loss: 0.139493\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.634471\tBest loss: 0.139493\tAccuracy: 19.08%\n",
      "22\tValidation loss: 1.631107\tBest loss: 0.139493\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=<function elu at 0x114248840>, total=  44.9s\n",
      "[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.138758\tBest loss: 0.138758\tAccuracy: 95.82%\n",
      "1\tValidation loss: 0.133157\tBest loss: 0.133157\tAccuracy: 97.19%\n",
      "2\tValidation loss: 1.297018\tBest loss: 0.133157\tAccuracy: 35.26%\n",
      "3\tValidation loss: 1.632372\tBest loss: 0.133157\tAccuracy: 20.95%\n",
      "4\tValidation loss: 1.698204\tBest loss: 0.133157\tAccuracy: 19.08%\n",
      "5\tValidation loss: 1.652505\tBest loss: 0.133157\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.701141\tBest loss: 0.133157\tAccuracy: 19.08%\n",
      "7\tValidation loss: 1.627677\tBest loss: 0.133157\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.644778\tBest loss: 0.133157\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.654257\tBest loss: 0.133157\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.667630\tBest loss: 0.133157\tAccuracy: 20.91%\n",
      "11\tValidation loss: 1.614572\tBest loss: 0.133157\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.654010\tBest loss: 0.133157\tAccuracy: 19.27%\n",
      "13\tValidation loss: 1.623109\tBest loss: 0.133157\tAccuracy: 18.73%\n",
      "14\tValidation loss: 1.680204\tBest loss: 0.133157\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.641296\tBest loss: 0.133157\tAccuracy: 18.73%\n",
      "16\tValidation loss: 1.661939\tBest loss: 0.133157\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.724656\tBest loss: 0.133157\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.685988\tBest loss: 0.133157\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.630729\tBest loss: 0.133157\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.676465\tBest loss: 0.133157\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.632461\tBest loss: 0.133157\tAccuracy: 18.73%\n",
      "22\tValidation loss: 1.655675\tBest loss: 0.133157\tAccuracy: 19.08%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=<function elu at 0x114248840>, total=  46.1s\n",
      "[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.127127\tBest loss: 0.127127\tAccuracy: 96.76%\n",
      "1\tValidation loss: 0.164128\tBest loss: 0.127127\tAccuracy: 96.76%\n",
      "2\tValidation loss: 1.695437\tBest loss: 0.127127\tAccuracy: 19.08%\n",
      "3\tValidation loss: 1.609973\tBest loss: 0.127127\tAccuracy: 19.08%\n",
      "4\tValidation loss: 1.644292\tBest loss: 0.127127\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.631322\tBest loss: 0.127127\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.687310\tBest loss: 0.127127\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.644475\tBest loss: 0.127127\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.642976\tBest loss: 0.127127\tAccuracy: 18.73%\n",
      "9\tValidation loss: 1.671119\tBest loss: 0.127127\tAccuracy: 19.27%\n",
      "10\tValidation loss: 1.629837\tBest loss: 0.127127\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.653883\tBest loss: 0.127127\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.643555\tBest loss: 0.127127\tAccuracy: 18.73%\n",
      "13\tValidation loss: 1.649819\tBest loss: 0.127127\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.654130\tBest loss: 0.127127\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.641571\tBest loss: 0.127127\tAccuracy: 20.91%\n",
      "16\tValidation loss: 1.699764\tBest loss: 0.127127\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.663347\tBest loss: 0.127127\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.673045\tBest loss: 0.127127\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.655063\tBest loss: 0.127127\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.639333\tBest loss: 0.127127\tAccuracy: 19.08%\n",
      "21\tValidation loss: 1.628095\tBest loss: 0.127127\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.3, batch_size=100, activation=<function elu at 0x114248840>, total=  43.2s\n",
      "[CV] n_neurons=120, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=<function elu at 0x114248840> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.142880\tBest loss: 0.142880\tAccuracy: 96.44%\n",
      "1\tValidation loss: 0.094105\tBest loss: 0.094105\tAccuracy: 97.26%\n",
      "2\tValidation loss: 0.085623\tBest loss: 0.085623\tAccuracy: 97.77%\n",
      "3\tValidation loss: 1.682621\tBest loss: 0.085623\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.739742\tBest loss: 0.085623\tAccuracy: 19.27%\n",
      "5\tValidation loss: 1.613065\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.666334\tBest loss: 0.085623\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.673351\tBest loss: 0.085623\tAccuracy: 19.08%\n",
      "8\tValidation loss: 1.700847\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.840350\tBest loss: 0.085623\tAccuracy: 19.08%\n",
      "10\tValidation loss: 1.683697\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.653740\tBest loss: 0.085623\tAccuracy: 18.73%\n",
      "12\tValidation loss: 1.663890\tBest loss: 0.085623\tAccuracy: 19.08%\n",
      "13\tValidation loss: 1.662506\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.671051\tBest loss: 0.085623\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.679654\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.696455\tBest loss: 0.085623\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.687108\tBest loss: 0.085623\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.676131\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.657697\tBest loss: 0.085623\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.698648\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.641137\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.662353\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "23\tValidation loss: 1.633925\tBest loss: 0.085623\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=<function elu at 0x114248840>, total=  53.6s\n",
      "[CV] n_neurons=120, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.137455\tBest loss: 0.137455\tAccuracy: 96.48%\n",
      "1\tValidation loss: 0.129374\tBest loss: 0.129374\tAccuracy: 97.30%\n",
      "2\tValidation loss: 0.264659\tBest loss: 0.129374\tAccuracy: 94.29%\n",
      "3\tValidation loss: 1.629945\tBest loss: 0.129374\tAccuracy: 22.01%\n",
      "4\tValidation loss: 1.665254\tBest loss: 0.129374\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.655261\tBest loss: 0.129374\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.730310\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.665054\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.668211\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.706906\tBest loss: 0.129374\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.683832\tBest loss: 0.129374\tAccuracy: 20.91%\n",
      "11\tValidation loss: 1.619793\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.715887\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "13\tValidation loss: 1.621925\tBest loss: 0.129374\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.670904\tBest loss: 0.129374\tAccuracy: 20.91%\n",
      "15\tValidation loss: 1.696239\tBest loss: 0.129374\tAccuracy: 18.73%\n",
      "16\tValidation loss: 1.647805\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.680746\tBest loss: 0.129374\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.656347\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "19\tValidation loss: 1.622423\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "20\tValidation loss: 1.696086\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.613970\tBest loss: 0.129374\tAccuracy: 18.73%\n",
      "22\tValidation loss: 1.667004\tBest loss: 0.129374\tAccuracy: 19.27%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=<function elu at 0x114248840>, total=  51.8s\n",
      "[CV] n_neurons=120, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.154935\tBest loss: 0.154935\tAccuracy: 96.40%\n",
      "1\tValidation loss: 0.099151\tBest loss: 0.099151\tAccuracy: 97.42%\n",
      "2\tValidation loss: 0.092193\tBest loss: 0.092193\tAccuracy: 97.62%\n",
      "3\tValidation loss: 0.177961\tBest loss: 0.092193\tAccuracy: 95.23%\n",
      "4\tValidation loss: 1.701957\tBest loss: 0.092193\tAccuracy: 20.91%\n",
      "5\tValidation loss: 1.643710\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "6\tValidation loss: 1.879745\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.642756\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.638637\tBest loss: 0.092193\tAccuracy: 18.73%\n",
      "9\tValidation loss: 1.654263\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "10\tValidation loss: 1.643770\tBest loss: 0.092193\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.709708\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.658341\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "13\tValidation loss: 1.668294\tBest loss: 0.092193\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.636261\tBest loss: 0.092193\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.625336\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "16\tValidation loss: 1.731852\tBest loss: 0.092193\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.666938\tBest loss: 0.092193\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.688980\tBest loss: 0.092193\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.691713\tBest loss: 0.092193\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.641732\tBest loss: 0.092193\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.689792\tBest loss: 0.092193\tAccuracy: 18.73%\n",
      "22\tValidation loss: 1.652560\tBest loss: 0.092193\tAccuracy: 19.08%\n",
      "23\tValidation loss: 1.681135\tBest loss: 0.092193\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.02, dropout_rate=0.2, batch_size=100, activation=<function elu at 0x114248840>, total=  54.2s\n",
      "[CV] n_neurons=10, learning_rate=0.05, dropout_rate=0.2, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8> \n",
      "0\tValidation loss: 0.459265\tBest loss: 0.459265\tAccuracy: 78.77%\n",
      "1\tValidation loss: 0.420448\tBest loss: 0.420448\tAccuracy: 79.01%\n",
      "2\tValidation loss: 0.403573\tBest loss: 0.403573\tAccuracy: 85.97%\n",
      "3\tValidation loss: 0.391198\tBest loss: 0.391198\tAccuracy: 87.84%\n",
      "4\tValidation loss: 0.314873\tBest loss: 0.314873\tAccuracy: 90.85%\n",
      "5\tValidation loss: 0.236153\tBest loss: 0.236153\tAccuracy: 93.51%\n",
      "6\tValidation loss: 0.345663\tBest loss: 0.236153\tAccuracy: 83.97%\n",
      "7\tValidation loss: 0.349608\tBest loss: 0.236153\tAccuracy: 88.94%\n",
      "8\tValidation loss: 0.240107\tBest loss: 0.236153\tAccuracy: 93.78%\n",
      "9\tValidation loss: 0.287622\tBest loss: 0.236153\tAccuracy: 92.89%\n",
      "10\tValidation loss: 0.287230\tBest loss: 0.236153\tAccuracy: 92.03%\n",
      "11\tValidation loss: 0.285926\tBest loss: 0.236153\tAccuracy: 93.67%\n",
      "12\tValidation loss: 0.302629\tBest loss: 0.236153\tAccuracy: 92.49%\n",
      "13\tValidation loss: 0.326606\tBest loss: 0.236153\tAccuracy: 89.29%\n",
      "14\tValidation loss: 0.266710\tBest loss: 0.236153\tAccuracy: 92.53%\n",
      "15\tValidation loss: 0.265517\tBest loss: 0.236153\tAccuracy: 94.88%\n",
      "16\tValidation loss: 0.247545\tBest loss: 0.236153\tAccuracy: 94.02%\n",
      "17\tValidation loss: 0.318705\tBest loss: 0.236153\tAccuracy: 92.06%\n",
      "18\tValidation loss: 0.296763\tBest loss: 0.236153\tAccuracy: 93.08%\n",
      "19\tValidation loss: 0.242578\tBest loss: 0.236153\tAccuracy: 93.28%\n",
      "20\tValidation loss: 0.251325\tBest loss: 0.236153\tAccuracy: 93.12%\n",
      "21\tValidation loss: 0.255719\tBest loss: 0.236153\tAccuracy: 94.61%\n",
      "22\tValidation loss: 0.270566\tBest loss: 0.236153\tAccuracy: 93.94%\n",
      "23\tValidation loss: 0.294852\tBest loss: 0.236153\tAccuracy: 94.84%\n",
      "24\tValidation loss: 0.244336\tBest loss: 0.236153\tAccuracy: 94.49%\n",
      "25\tValidation loss: 0.269744\tBest loss: 0.236153\tAccuracy: 93.43%\n",
      "26\tValidation loss: 0.261058\tBest loss: 0.236153\tAccuracy: 93.90%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, dropout_rate=0.2, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>, total=  34.6s\n",
      "[CV] n_neurons=10, learning_rate=0.05, dropout_rate=0.2, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8> \n",
      "0\tValidation loss: 0.531083\tBest loss: 0.531083\tAccuracy: 83.97%\n",
      "1\tValidation loss: 0.317157\tBest loss: 0.317157\tAccuracy: 93.08%\n",
      "2\tValidation loss: 0.302048\tBest loss: 0.302048\tAccuracy: 92.65%\n",
      "3\tValidation loss: 0.424043\tBest loss: 0.302048\tAccuracy: 85.89%\n",
      "4\tValidation loss: 0.549716\tBest loss: 0.302048\tAccuracy: 88.74%\n",
      "5\tValidation loss: 0.511211\tBest loss: 0.302048\tAccuracy: 88.62%\n",
      "6\tValidation loss: 0.395137\tBest loss: 0.302048\tAccuracy: 91.91%\n",
      "7\tValidation loss: 0.381966\tBest loss: 0.302048\tAccuracy: 91.63%\n",
      "8\tValidation loss: 0.532627\tBest loss: 0.302048\tAccuracy: 86.59%\n",
      "9\tValidation loss: 0.368285\tBest loss: 0.302048\tAccuracy: 93.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\tValidation loss: 0.281358\tBest loss: 0.281358\tAccuracy: 95.15%\n",
      "11\tValidation loss: 0.326868\tBest loss: 0.281358\tAccuracy: 93.47%\n",
      "12\tValidation loss: 0.620927\tBest loss: 0.281358\tAccuracy: 92.42%\n",
      "13\tValidation loss: 0.529614\tBest loss: 0.281358\tAccuracy: 75.45%\n",
      "14\tValidation loss: 0.493146\tBest loss: 0.281358\tAccuracy: 80.30%\n",
      "15\tValidation loss: 0.341381\tBest loss: 0.281358\tAccuracy: 90.77%\n",
      "16\tValidation loss: 0.331195\tBest loss: 0.281358\tAccuracy: 91.83%\n",
      "17\tValidation loss: 0.302034\tBest loss: 0.281358\tAccuracy: 93.24%\n",
      "18\tValidation loss: 0.251850\tBest loss: 0.251850\tAccuracy: 93.71%\n",
      "19\tValidation loss: 0.250452\tBest loss: 0.250452\tAccuracy: 94.53%\n",
      "20\tValidation loss: 0.254106\tBest loss: 0.250452\tAccuracy: 94.76%\n",
      "21\tValidation loss: 0.260853\tBest loss: 0.250452\tAccuracy: 93.28%\n",
      "22\tValidation loss: 0.270098\tBest loss: 0.250452\tAccuracy: 93.08%\n",
      "23\tValidation loss: 0.266075\tBest loss: 0.250452\tAccuracy: 94.18%\n",
      "24\tValidation loss: 0.277862\tBest loss: 0.250452\tAccuracy: 95.35%\n",
      "25\tValidation loss: 0.332727\tBest loss: 0.250452\tAccuracy: 93.71%\n",
      "26\tValidation loss: 0.579320\tBest loss: 0.250452\tAccuracy: 92.49%\n",
      "27\tValidation loss: 0.253735\tBest loss: 0.250452\tAccuracy: 94.61%\n",
      "28\tValidation loss: 0.273793\tBest loss: 0.250452\tAccuracy: 94.61%\n",
      "29\tValidation loss: 0.419035\tBest loss: 0.250452\tAccuracy: 95.00%\n",
      "30\tValidation loss: 0.325248\tBest loss: 0.250452\tAccuracy: 93.47%\n",
      "31\tValidation loss: 0.281835\tBest loss: 0.250452\tAccuracy: 94.02%\n",
      "32\tValidation loss: 0.247861\tBest loss: 0.247861\tAccuracy: 94.57%\n",
      "33\tValidation loss: 0.442221\tBest loss: 0.247861\tAccuracy: 82.25%\n",
      "34\tValidation loss: 0.275137\tBest loss: 0.247861\tAccuracy: 92.81%\n",
      "35\tValidation loss: 0.294848\tBest loss: 0.247861\tAccuracy: 93.71%\n",
      "36\tValidation loss: 0.254667\tBest loss: 0.247861\tAccuracy: 93.86%\n",
      "37\tValidation loss: 0.230119\tBest loss: 0.230119\tAccuracy: 94.61%\n",
      "38\tValidation loss: 0.276814\tBest loss: 0.230119\tAccuracy: 93.55%\n",
      "39\tValidation loss: 0.330460\tBest loss: 0.230119\tAccuracy: 93.94%\n",
      "40\tValidation loss: 0.309970\tBest loss: 0.230119\tAccuracy: 91.95%\n",
      "41\tValidation loss: 0.237878\tBest loss: 0.230119\tAccuracy: 94.76%\n",
      "42\tValidation loss: 0.240836\tBest loss: 0.230119\tAccuracy: 94.29%\n",
      "43\tValidation loss: 0.329478\tBest loss: 0.230119\tAccuracy: 91.05%\n",
      "44\tValidation loss: 0.270134\tBest loss: 0.230119\tAccuracy: 94.53%\n",
      "45\tValidation loss: 0.303453\tBest loss: 0.230119\tAccuracy: 93.04%\n",
      "46\tValidation loss: 0.250392\tBest loss: 0.230119\tAccuracy: 94.21%\n",
      "47\tValidation loss: 0.238984\tBest loss: 0.230119\tAccuracy: 94.37%\n",
      "48\tValidation loss: 0.247488\tBest loss: 0.230119\tAccuracy: 95.39%\n",
      "49\tValidation loss: 0.317683\tBest loss: 0.230119\tAccuracy: 92.34%\n",
      "50\tValidation loss: 0.297675\tBest loss: 0.230119\tAccuracy: 92.92%\n",
      "51\tValidation loss: 0.240150\tBest loss: 0.230119\tAccuracy: 94.68%\n",
      "52\tValidation loss: 0.254165\tBest loss: 0.230119\tAccuracy: 94.02%\n",
      "53\tValidation loss: 0.571046\tBest loss: 0.230119\tAccuracy: 81.04%\n",
      "54\tValidation loss: 0.294259\tBest loss: 0.230119\tAccuracy: 93.94%\n",
      "55\tValidation loss: 0.331489\tBest loss: 0.230119\tAccuracy: 92.10%\n",
      "56\tValidation loss: 0.243301\tBest loss: 0.230119\tAccuracy: 93.82%\n",
      "57\tValidation loss: 0.250620\tBest loss: 0.230119\tAccuracy: 94.45%\n",
      "58\tValidation loss: 0.248422\tBest loss: 0.230119\tAccuracy: 93.71%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, dropout_rate=0.2, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>, total= 1.2min\n",
      "[CV] n_neurons=10, learning_rate=0.05, dropout_rate=0.2, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8> \n",
      "0\tValidation loss: 0.480061\tBest loss: 0.480061\tAccuracy: 78.34%\n",
      "1\tValidation loss: 0.415146\tBest loss: 0.415146\tAccuracy: 78.38%\n",
      "2\tValidation loss: 0.405157\tBest loss: 0.405157\tAccuracy: 82.53%\n",
      "3\tValidation loss: 0.389358\tBest loss: 0.389358\tAccuracy: 84.87%\n",
      "4\tValidation loss: 0.371753\tBest loss: 0.371753\tAccuracy: 88.47%\n",
      "5\tValidation loss: 0.626105\tBest loss: 0.371753\tAccuracy: 71.03%\n",
      "6\tValidation loss: 0.487277\tBest loss: 0.371753\tAccuracy: 82.25%\n",
      "7\tValidation loss: 1.326782\tBest loss: 0.371753\tAccuracy: 79.75%\n",
      "8\tValidation loss: 0.526798\tBest loss: 0.371753\tAccuracy: 74.98%\n",
      "9\tValidation loss: 0.445819\tBest loss: 0.371753\tAccuracy: 80.26%\n",
      "10\tValidation loss: 0.427201\tBest loss: 0.371753\tAccuracy: 80.10%\n",
      "11\tValidation loss: 0.388448\tBest loss: 0.371753\tAccuracy: 83.11%\n",
      "12\tValidation loss: 0.408028\tBest loss: 0.371753\tAccuracy: 84.83%\n",
      "13\tValidation loss: 0.416301\tBest loss: 0.371753\tAccuracy: 84.60%\n",
      "14\tValidation loss: 0.459428\tBest loss: 0.371753\tAccuracy: 79.48%\n",
      "15\tValidation loss: 0.416901\tBest loss: 0.371753\tAccuracy: 78.34%\n",
      "16\tValidation loss: 0.428556\tBest loss: 0.371753\tAccuracy: 80.73%\n",
      "17\tValidation loss: 0.714176\tBest loss: 0.371753\tAccuracy: 65.87%\n",
      "18\tValidation loss: 0.450378\tBest loss: 0.371753\tAccuracy: 85.22%\n",
      "19\tValidation loss: 0.519231\tBest loss: 0.371753\tAccuracy: 84.36%\n",
      "20\tValidation loss: 0.434950\tBest loss: 0.371753\tAccuracy: 85.54%\n",
      "21\tValidation loss: 0.435246\tBest loss: 0.371753\tAccuracy: 84.75%\n",
      "22\tValidation loss: 0.409789\tBest loss: 0.371753\tAccuracy: 86.59%\n",
      "23\tValidation loss: 0.392875\tBest loss: 0.371753\tAccuracy: 86.51%\n",
      "24\tValidation loss: 0.444876\tBest loss: 0.371753\tAccuracy: 83.97%\n",
      "25\tValidation loss: 0.416177\tBest loss: 0.371753\tAccuracy: 85.54%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.05, dropout_rate=0.2, batch_size=50, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>, total=  34.1s\n",
      "[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8> \n",
      "0\tValidation loss: 0.217926\tBest loss: 0.217926\tAccuracy: 93.00%\n",
      "1\tValidation loss: 0.162938\tBest loss: 0.162938\tAccuracy: 94.76%\n",
      "2\tValidation loss: 0.196905\tBest loss: 0.162938\tAccuracy: 95.15%\n",
      "3\tValidation loss: 0.177099\tBest loss: 0.162938\tAccuracy: 95.04%\n",
      "4\tValidation loss: 0.149658\tBest loss: 0.149658\tAccuracy: 95.54%\n",
      "5\tValidation loss: 0.206930\tBest loss: 0.149658\tAccuracy: 95.82%\n",
      "6\tValidation loss: 0.210281\tBest loss: 0.149658\tAccuracy: 94.96%\n",
      "7\tValidation loss: 0.168016\tBest loss: 0.149658\tAccuracy: 95.62%\n",
      "8\tValidation loss: 0.170895\tBest loss: 0.149658\tAccuracy: 95.35%\n",
      "9\tValidation loss: 0.205004\tBest loss: 0.149658\tAccuracy: 95.78%\n",
      "10\tValidation loss: 0.210330\tBest loss: 0.149658\tAccuracy: 94.92%\n",
      "11\tValidation loss: 0.136582\tBest loss: 0.136582\tAccuracy: 95.86%\n",
      "12\tValidation loss: 0.162577\tBest loss: 0.136582\tAccuracy: 96.05%\n",
      "13\tValidation loss: 0.190299\tBest loss: 0.136582\tAccuracy: 94.10%\n",
      "14\tValidation loss: 0.177672\tBest loss: 0.136582\tAccuracy: 96.09%\n",
      "15\tValidation loss: 0.165752\tBest loss: 0.136582\tAccuracy: 95.86%\n",
      "16\tValidation loss: 0.175824\tBest loss: 0.136582\tAccuracy: 95.00%\n",
      "17\tValidation loss: 0.190955\tBest loss: 0.136582\tAccuracy: 94.80%\n",
      "18\tValidation loss: 0.172758\tBest loss: 0.136582\tAccuracy: 94.53%\n",
      "19\tValidation loss: 0.162129\tBest loss: 0.136582\tAccuracy: 95.86%\n",
      "20\tValidation loss: 0.172946\tBest loss: 0.136582\tAccuracy: 96.13%\n",
      "21\tValidation loss: 0.144957\tBest loss: 0.136582\tAccuracy: 96.09%\n",
      "22\tValidation loss: 0.170393\tBest loss: 0.136582\tAccuracy: 94.80%\n",
      "23\tValidation loss: 0.145690\tBest loss: 0.136582\tAccuracy: 95.97%\n",
      "24\tValidation loss: 0.189756\tBest loss: 0.136582\tAccuracy: 95.15%\n",
      "25\tValidation loss: 0.121440\tBest loss: 0.121440\tAccuracy: 96.13%\n",
      "26\tValidation loss: 0.223515\tBest loss: 0.121440\tAccuracy: 92.81%\n",
      "27\tValidation loss: 0.156064\tBest loss: 0.121440\tAccuracy: 96.17%\n",
      "28\tValidation loss: 0.163009\tBest loss: 0.121440\tAccuracy: 95.93%\n",
      "29\tValidation loss: 0.147410\tBest loss: 0.121440\tAccuracy: 96.21%\n",
      "30\tValidation loss: 0.185732\tBest loss: 0.121440\tAccuracy: 94.37%\n",
      "31\tValidation loss: 0.163900\tBest loss: 0.121440\tAccuracy: 95.27%\n",
      "32\tValidation loss: 0.419527\tBest loss: 0.121440\tAccuracy: 89.44%\n",
      "33\tValidation loss: 0.146496\tBest loss: 0.121440\tAccuracy: 96.17%\n",
      "34\tValidation loss: 0.143875\tBest loss: 0.121440\tAccuracy: 96.33%\n",
      "35\tValidation loss: 0.174747\tBest loss: 0.121440\tAccuracy: 95.43%\n",
      "36\tValidation loss: 0.156366\tBest loss: 0.121440\tAccuracy: 95.74%\n",
      "37\tValidation loss: 0.188948\tBest loss: 0.121440\tAccuracy: 95.43%\n",
      "38\tValidation loss: 0.133922\tBest loss: 0.121440\tAccuracy: 96.17%\n",
      "39\tValidation loss: 0.156235\tBest loss: 0.121440\tAccuracy: 95.74%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\tValidation loss: 0.181255\tBest loss: 0.121440\tAccuracy: 96.05%\n",
      "41\tValidation loss: 0.160781\tBest loss: 0.121440\tAccuracy: 95.07%\n",
      "42\tValidation loss: 0.134399\tBest loss: 0.121440\tAccuracy: 96.13%\n",
      "43\tValidation loss: 0.211088\tBest loss: 0.121440\tAccuracy: 95.43%\n",
      "44\tValidation loss: 0.130202\tBest loss: 0.121440\tAccuracy: 96.56%\n",
      "45\tValidation loss: 0.174957\tBest loss: 0.121440\tAccuracy: 95.39%\n",
      "46\tValidation loss: 0.161183\tBest loss: 0.121440\tAccuracy: 95.86%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8>, total= 3.6min\n",
      "[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8> \n",
      "0\tValidation loss: 0.218841\tBest loss: 0.218841\tAccuracy: 94.21%\n",
      "1\tValidation loss: 0.312006\tBest loss: 0.218841\tAccuracy: 93.71%\n",
      "2\tValidation loss: 0.240037\tBest loss: 0.218841\tAccuracy: 93.35%\n",
      "3\tValidation loss: 0.226268\tBest loss: 0.218841\tAccuracy: 94.33%\n",
      "4\tValidation loss: 0.304461\tBest loss: 0.218841\tAccuracy: 90.73%\n",
      "5\tValidation loss: 0.229038\tBest loss: 0.218841\tAccuracy: 93.63%\n",
      "6\tValidation loss: 0.217931\tBest loss: 0.217931\tAccuracy: 93.71%\n",
      "7\tValidation loss: 0.262553\tBest loss: 0.217931\tAccuracy: 93.82%\n",
      "8\tValidation loss: 0.225369\tBest loss: 0.217931\tAccuracy: 94.25%\n",
      "9\tValidation loss: 0.215941\tBest loss: 0.215941\tAccuracy: 94.06%\n",
      "10\tValidation loss: 0.194457\tBest loss: 0.194457\tAccuracy: 95.04%\n",
      "11\tValidation loss: 0.204735\tBest loss: 0.194457\tAccuracy: 95.93%\n",
      "12\tValidation loss: 0.225299\tBest loss: 0.194457\tAccuracy: 95.58%\n",
      "13\tValidation loss: 0.216817\tBest loss: 0.194457\tAccuracy: 94.68%\n",
      "14\tValidation loss: 0.247609\tBest loss: 0.194457\tAccuracy: 93.98%\n",
      "15\tValidation loss: 0.211506\tBest loss: 0.194457\tAccuracy: 94.61%\n",
      "16\tValidation loss: 0.269835\tBest loss: 0.194457\tAccuracy: 91.83%\n",
      "17\tValidation loss: 0.207829\tBest loss: 0.194457\tAccuracy: 94.88%\n",
      "18\tValidation loss: 0.199658\tBest loss: 0.194457\tAccuracy: 94.96%\n",
      "19\tValidation loss: 0.239033\tBest loss: 0.194457\tAccuracy: 94.57%\n",
      "20\tValidation loss: 0.226135\tBest loss: 0.194457\tAccuracy: 94.53%\n",
      "21\tValidation loss: 0.227185\tBest loss: 0.194457\tAccuracy: 95.31%\n",
      "22\tValidation loss: 0.181957\tBest loss: 0.181957\tAccuracy: 95.62%\n",
      "23\tValidation loss: 0.183247\tBest loss: 0.181957\tAccuracy: 95.43%\n",
      "24\tValidation loss: 0.204813\tBest loss: 0.181957\tAccuracy: 96.01%\n",
      "25\tValidation loss: 0.291236\tBest loss: 0.181957\tAccuracy: 95.50%\n",
      "26\tValidation loss: 0.254212\tBest loss: 0.181957\tAccuracy: 94.72%\n",
      "27\tValidation loss: 0.200032\tBest loss: 0.181957\tAccuracy: 95.50%\n",
      "28\tValidation loss: 0.165439\tBest loss: 0.165439\tAccuracy: 95.86%\n",
      "29\tValidation loss: 0.179296\tBest loss: 0.165439\tAccuracy: 96.17%\n",
      "30\tValidation loss: 0.200789\tBest loss: 0.165439\tAccuracy: 94.61%\n",
      "31\tValidation loss: 0.294281\tBest loss: 0.165439\tAccuracy: 94.80%\n",
      "32\tValidation loss: 0.182170\tBest loss: 0.165439\tAccuracy: 95.54%\n",
      "33\tValidation loss: 0.232951\tBest loss: 0.165439\tAccuracy: 95.43%\n",
      "34\tValidation loss: 0.195017\tBest loss: 0.165439\tAccuracy: 95.70%\n",
      "35\tValidation loss: 0.187781\tBest loss: 0.165439\tAccuracy: 95.74%\n",
      "36\tValidation loss: 0.197488\tBest loss: 0.165439\tAccuracy: 95.86%\n",
      "37\tValidation loss: 0.240433\tBest loss: 0.165439\tAccuracy: 94.88%\n",
      "38\tValidation loss: 0.198531\tBest loss: 0.165439\tAccuracy: 94.45%\n",
      "39\tValidation loss: 0.249401\tBest loss: 0.165439\tAccuracy: 94.72%\n",
      "40\tValidation loss: 0.228721\tBest loss: 0.165439\tAccuracy: 95.15%\n",
      "41\tValidation loss: 0.179956\tBest loss: 0.165439\tAccuracy: 96.36%\n",
      "42\tValidation loss: 0.172271\tBest loss: 0.165439\tAccuracy: 95.86%\n",
      "43\tValidation loss: 0.183184\tBest loss: 0.165439\tAccuracy: 95.70%\n",
      "44\tValidation loss: 0.296215\tBest loss: 0.165439\tAccuracy: 95.31%\n",
      "45\tValidation loss: 0.214093\tBest loss: 0.165439\tAccuracy: 95.11%\n",
      "46\tValidation loss: 0.216590\tBest loss: 0.165439\tAccuracy: 93.71%\n",
      "47\tValidation loss: 0.362201\tBest loss: 0.165439\tAccuracy: 94.84%\n",
      "48\tValidation loss: 0.198425\tBest loss: 0.165439\tAccuracy: 95.23%\n",
      "49\tValidation loss: 0.287795\tBest loss: 0.165439\tAccuracy: 93.32%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8>, total= 4.4min\n",
      "[CV] n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8> \n",
      "0\tValidation loss: 0.223935\tBest loss: 0.223935\tAccuracy: 94.37%\n",
      "1\tValidation loss: 0.264702\tBest loss: 0.223935\tAccuracy: 91.40%\n",
      "2\tValidation loss: 0.209528\tBest loss: 0.209528\tAccuracy: 95.04%\n",
      "3\tValidation loss: 0.200510\tBest loss: 0.200510\tAccuracy: 95.23%\n",
      "4\tValidation loss: 0.243087\tBest loss: 0.200510\tAccuracy: 94.92%\n",
      "5\tValidation loss: 0.198530\tBest loss: 0.198530\tAccuracy: 94.49%\n",
      "6\tValidation loss: 0.219470\tBest loss: 0.198530\tAccuracy: 95.31%\n",
      "7\tValidation loss: 0.222031\tBest loss: 0.198530\tAccuracy: 94.57%\n",
      "8\tValidation loss: 0.242493\tBest loss: 0.198530\tAccuracy: 93.71%\n",
      "9\tValidation loss: 0.261525\tBest loss: 0.198530\tAccuracy: 94.61%\n",
      "10\tValidation loss: 0.217334\tBest loss: 0.198530\tAccuracy: 94.80%\n",
      "11\tValidation loss: 0.248632\tBest loss: 0.198530\tAccuracy: 95.35%\n",
      "12\tValidation loss: 0.199853\tBest loss: 0.198530\tAccuracy: 94.92%\n",
      "13\tValidation loss: 0.193139\tBest loss: 0.193139\tAccuracy: 95.31%\n",
      "14\tValidation loss: 0.244702\tBest loss: 0.193139\tAccuracy: 94.02%\n",
      "15\tValidation loss: 0.246536\tBest loss: 0.193139\tAccuracy: 94.61%\n",
      "16\tValidation loss: 0.258220\tBest loss: 0.193139\tAccuracy: 94.96%\n",
      "17\tValidation loss: 0.202301\tBest loss: 0.193139\tAccuracy: 95.35%\n",
      "18\tValidation loss: 0.213821\tBest loss: 0.193139\tAccuracy: 95.74%\n",
      "19\tValidation loss: 0.191546\tBest loss: 0.191546\tAccuracy: 94.41%\n",
      "20\tValidation loss: 0.269663\tBest loss: 0.191546\tAccuracy: 94.25%\n",
      "21\tValidation loss: 0.227571\tBest loss: 0.191546\tAccuracy: 95.07%\n",
      "22\tValidation loss: 0.308983\tBest loss: 0.191546\tAccuracy: 93.43%\n",
      "23\tValidation loss: 0.228657\tBest loss: 0.191546\tAccuracy: 94.72%\n",
      "24\tValidation loss: 0.212130\tBest loss: 0.191546\tAccuracy: 95.23%\n",
      "25\tValidation loss: 0.267084\tBest loss: 0.191546\tAccuracy: 92.73%\n",
      "26\tValidation loss: 0.241569\tBest loss: 0.191546\tAccuracy: 95.58%\n",
      "27\tValidation loss: 0.426039\tBest loss: 0.191546\tAccuracy: 86.83%\n",
      "28\tValidation loss: 0.301845\tBest loss: 0.191546\tAccuracy: 89.99%\n",
      "29\tValidation loss: 0.251248\tBest loss: 0.191546\tAccuracy: 94.76%\n",
      "30\tValidation loss: 0.203535\tBest loss: 0.191546\tAccuracy: 95.70%\n",
      "31\tValidation loss: 0.244598\tBest loss: 0.191546\tAccuracy: 94.21%\n",
      "32\tValidation loss: 0.189895\tBest loss: 0.189895\tAccuracy: 95.07%\n",
      "33\tValidation loss: 0.253287\tBest loss: 0.189895\tAccuracy: 94.21%\n",
      "34\tValidation loss: 0.189599\tBest loss: 0.189599\tAccuracy: 94.57%\n",
      "35\tValidation loss: 0.191412\tBest loss: 0.189599\tAccuracy: 94.88%\n",
      "36\tValidation loss: 0.224345\tBest loss: 0.189599\tAccuracy: 95.07%\n",
      "37\tValidation loss: 0.259543\tBest loss: 0.189599\tAccuracy: 95.86%\n",
      "38\tValidation loss: 0.193443\tBest loss: 0.189599\tAccuracy: 95.43%\n",
      "39\tValidation loss: 0.322665\tBest loss: 0.189599\tAccuracy: 94.06%\n",
      "40\tValidation loss: 0.230245\tBest loss: 0.189599\tAccuracy: 93.98%\n",
      "41\tValidation loss: 0.235555\tBest loss: 0.189599\tAccuracy: 93.78%\n",
      "42\tValidation loss: 0.346126\tBest loss: 0.189599\tAccuracy: 91.09%\n",
      "43\tValidation loss: 0.314256\tBest loss: 0.189599\tAccuracy: 93.39%\n",
      "44\tValidation loss: 0.240367\tBest loss: 0.189599\tAccuracy: 94.61%\n",
      "45\tValidation loss: 0.216025\tBest loss: 0.189599\tAccuracy: 96.33%\n",
      "46\tValidation loss: 0.215694\tBest loss: 0.189599\tAccuracy: 94.33%\n",
      "47\tValidation loss: 0.236779\tBest loss: 0.189599\tAccuracy: 94.45%\n",
      "48\tValidation loss: 0.208992\tBest loss: 0.189599\tAccuracy: 95.07%\n",
      "49\tValidation loss: 0.435713\tBest loss: 0.189599\tAccuracy: 88.35%\n",
      "50\tValidation loss: 0.234650\tBest loss: 0.189599\tAccuracy: 94.68%\n",
      "51\tValidation loss: 0.345454\tBest loss: 0.189599\tAccuracy: 91.63%\n",
      "52\tValidation loss: 0.390544\tBest loss: 0.189599\tAccuracy: 93.55%\n",
      "53\tValidation loss: 0.249099\tBest loss: 0.189599\tAccuracy: 95.31%\n",
      "54\tValidation loss: 0.374941\tBest loss: 0.189599\tAccuracy: 93.86%\n",
      "55\tValidation loss: 0.361741\tBest loss: 0.189599\tAccuracy: 90.97%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=10, learning_rate=0.02, dropout_rate=0.2, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8>, total= 5.1min\n",
      "[CV] n_neurons=140, learning_rate=0.1, dropout_rate=0.4, batch_size=100, activation=<function relu at 0x114259620> \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 1.611193\tBest loss: 1.611193\tAccuracy: 19.08%\n",
      "1\tValidation loss: 1.613134\tBest loss: 1.611193\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.609273\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.609834\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "4\tValidation loss: 1.618567\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.612659\tBest loss: 1.609273\tAccuracy: 19.27%\n",
      "6\tValidation loss: 1.609880\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "7\tValidation loss: 1.615101\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.621918\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.610810\tBest loss: 1.609273\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.608223\tBest loss: 1.608223\tAccuracy: 20.91%\n",
      "11\tValidation loss: 1.619413\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.608466\tBest loss: 1.608223\tAccuracy: 20.91%\n",
      "13\tValidation loss: 1.618590\tBest loss: 1.608223\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.613458\tBest loss: 1.608223\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.611298\tBest loss: 1.608223\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.611331\tBest loss: 1.608223\tAccuracy: 19.08%\n",
      "17\tValidation loss: 1.611217\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "18\tValidation loss: 1.614755\tBest loss: 1.608223\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.610596\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "20\tValidation loss: 1.611315\tBest loss: 1.608223\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.614836\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "22\tValidation loss: 1.615952\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "23\tValidation loss: 1.618060\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "24\tValidation loss: 1.614472\tBest loss: 1.608223\tAccuracy: 22.01%\n",
      "25\tValidation loss: 1.609641\tBest loss: 1.608223\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.610522\tBest loss: 1.608223\tAccuracy: 22.01%\n",
      "27\tValidation loss: 1.612529\tBest loss: 1.608223\tAccuracy: 19.08%\n",
      "28\tValidation loss: 1.610727\tBest loss: 1.608223\tAccuracy: 19.08%\n",
      "29\tValidation loss: 1.614612\tBest loss: 1.608223\tAccuracy: 18.73%\n",
      "30\tValidation loss: 1.622660\tBest loss: 1.608223\tAccuracy: 19.27%\n",
      "31\tValidation loss: 1.608783\tBest loss: 1.608223\tAccuracy: 22.01%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, dropout_rate=0.4, batch_size=100, activation=<function relu at 0x114259620>, total= 1.5min\n",
      "[CV] n_neurons=140, learning_rate=0.1, dropout_rate=0.4, batch_size=100, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 1.609520\tBest loss: 1.609520\tAccuracy: 22.01%\n",
      "1\tValidation loss: 1.609790\tBest loss: 1.609520\tAccuracy: 19.08%\n",
      "2\tValidation loss: 1.615085\tBest loss: 1.609520\tAccuracy: 19.27%\n",
      "3\tValidation loss: 1.624225\tBest loss: 1.609520\tAccuracy: 18.73%\n",
      "4\tValidation loss: 1.614231\tBest loss: 1.609520\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.614035\tBest loss: 1.609520\tAccuracy: 19.27%\n",
      "6\tValidation loss: 1.613835\tBest loss: 1.609520\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.611883\tBest loss: 1.609520\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.609731\tBest loss: 1.609520\tAccuracy: 20.91%\n",
      "9\tValidation loss: 1.609540\tBest loss: 1.609520\tAccuracy: 20.91%\n",
      "10\tValidation loss: 1.618950\tBest loss: 1.609520\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.612220\tBest loss: 1.609520\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.609187\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "13\tValidation loss: 1.613467\tBest loss: 1.609187\tAccuracy: 19.27%\n",
      "14\tValidation loss: 1.613068\tBest loss: 1.609187\tAccuracy: 19.27%\n",
      "15\tValidation loss: 1.612362\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.612057\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "17\tValidation loss: 1.618463\tBest loss: 1.609187\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.610904\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.610653\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.612269\tBest loss: 1.609187\tAccuracy: 19.08%\n",
      "21\tValidation loss: 1.610902\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.617970\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "23\tValidation loss: 1.616940\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.612192\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "25\tValidation loss: 1.615077\tBest loss: 1.609187\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.607681\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "27\tValidation loss: 1.611637\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "28\tValidation loss: 1.626016\tBest loss: 1.607681\tAccuracy: 18.73%\n",
      "29\tValidation loss: 1.609979\tBest loss: 1.607681\tAccuracy: 18.73%\n",
      "30\tValidation loss: 1.611088\tBest loss: 1.607681\tAccuracy: 19.08%\n",
      "31\tValidation loss: 1.611650\tBest loss: 1.607681\tAccuracy: 19.27%\n",
      "32\tValidation loss: 1.612802\tBest loss: 1.607681\tAccuracy: 18.73%\n",
      "33\tValidation loss: 1.608831\tBest loss: 1.607681\tAccuracy: 20.91%\n",
      "34\tValidation loss: 1.613915\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.610115\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "36\tValidation loss: 1.613153\tBest loss: 1.607681\tAccuracy: 20.91%\n",
      "37\tValidation loss: 1.609837\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "38\tValidation loss: 1.620759\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "39\tValidation loss: 1.623756\tBest loss: 1.607681\tAccuracy: 19.27%\n",
      "40\tValidation loss: 1.609526\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "41\tValidation loss: 1.609149\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "42\tValidation loss: 1.622812\tBest loss: 1.607681\tAccuracy: 19.27%\n",
      "43\tValidation loss: 1.617259\tBest loss: 1.607681\tAccuracy: 22.01%\n",
      "44\tValidation loss: 1.616075\tBest loss: 1.607681\tAccuracy: 19.08%\n",
      "45\tValidation loss: 1.611638\tBest loss: 1.607681\tAccuracy: 19.08%\n",
      "46\tValidation loss: 1.610517\tBest loss: 1.607681\tAccuracy: 19.27%\n",
      "47\tValidation loss: 1.611001\tBest loss: 1.607681\tAccuracy: 19.08%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, dropout_rate=0.4, batch_size=100, activation=<function relu at 0x114259620>, total= 2.3min\n",
      "[CV] n_neurons=140, learning_rate=0.1, dropout_rate=0.4, batch_size=100, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 1.614856\tBest loss: 1.614856\tAccuracy: 19.27%\n",
      "1\tValidation loss: 1.609661\tBest loss: 1.609661\tAccuracy: 19.08%\n",
      "2\tValidation loss: 1.618164\tBest loss: 1.609661\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.619649\tBest loss: 1.609661\tAccuracy: 19.27%\n",
      "4\tValidation loss: 1.608994\tBest loss: 1.608994\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.610180\tBest loss: 1.608994\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.615118\tBest loss: 1.608994\tAccuracy: 22.01%\n",
      "7\tValidation loss: 1.615765\tBest loss: 1.608994\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.611767\tBest loss: 1.608994\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.614178\tBest loss: 1.608994\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.612346\tBest loss: 1.608994\tAccuracy: 18.73%\n",
      "11\tValidation loss: 1.623494\tBest loss: 1.608994\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.608287\tBest loss: 1.608287\tAccuracy: 22.01%\n",
      "13\tValidation loss: 1.611858\tBest loss: 1.608287\tAccuracy: 19.27%\n",
      "14\tValidation loss: 1.608453\tBest loss: 1.608287\tAccuracy: 20.91%\n",
      "15\tValidation loss: 1.618121\tBest loss: 1.608287\tAccuracy: 19.08%\n",
      "16\tValidation loss: 1.618649\tBest loss: 1.608287\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.615217\tBest loss: 1.608287\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.608069\tBest loss: 1.608069\tAccuracy: 20.91%\n",
      "19\tValidation loss: 1.608647\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.620139\tBest loss: 1.608069\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.611839\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.613161\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "23\tValidation loss: 1.613678\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "24\tValidation loss: 1.613880\tBest loss: 1.608069\tAccuracy: 20.91%\n",
      "25\tValidation loss: 1.617808\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "26\tValidation loss: 1.613083\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "27\tValidation loss: 1.614649\tBest loss: 1.608069\tAccuracy: 19.27%\n",
      "28\tValidation loss: 1.630548\tBest loss: 1.608069\tAccuracy: 18.73%\n",
      "29\tValidation loss: 1.611475\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "30\tValidation loss: 1.611855\tBest loss: 1.608069\tAccuracy: 19.08%\n",
      "31\tValidation loss: 1.611834\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "32\tValidation loss: 1.615368\tBest loss: 1.608069\tAccuracy: 18.73%\n",
      "33\tValidation loss: 1.608478\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "34\tValidation loss: 1.613686\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "35\tValidation loss: 1.609649\tBest loss: 1.608069\tAccuracy: 19.27%\n",
      "36\tValidation loss: 1.609904\tBest loss: 1.608069\tAccuracy: 18.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\tValidation loss: 1.608631\tBest loss: 1.608069\tAccuracy: 22.01%\n",
      "38\tValidation loss: 1.617385\tBest loss: 1.608069\tAccuracy: 19.27%\n",
      "39\tValidation loss: 1.625768\tBest loss: 1.608069\tAccuracy: 19.27%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=140, learning_rate=0.1, dropout_rate=0.4, batch_size=100, activation=<function relu at 0x114259620>, total= 1.9min\n",
      "[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.982988\tBest loss: 0.982988\tAccuracy: 54.85%\n",
      "1\tValidation loss: 1.616800\tBest loss: 0.982988\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.677744\tBest loss: 0.982988\tAccuracy: 19.08%\n",
      "3\tValidation loss: 1.645243\tBest loss: 0.982988\tAccuracy: 19.08%\n",
      "4\tValidation loss: 1.739493\tBest loss: 0.982988\tAccuracy: 19.27%\n",
      "5\tValidation loss: 1.609320\tBest loss: 0.982988\tAccuracy: 20.91%\n",
      "6\tValidation loss: 1.672261\tBest loss: 0.982988\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.641659\tBest loss: 0.982988\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.675502\tBest loss: 0.982988\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.846345\tBest loss: 0.982988\tAccuracy: 19.08%\n",
      "10\tValidation loss: 1.654606\tBest loss: 0.982988\tAccuracy: 19.08%\n",
      "11\tValidation loss: 1.668866\tBest loss: 0.982988\tAccuracy: 18.73%\n",
      "12\tValidation loss: 1.623947\tBest loss: 0.982988\tAccuracy: 18.73%\n",
      "13\tValidation loss: 1.619253\tBest loss: 0.982988\tAccuracy: 22.01%\n",
      "14\tValidation loss: 1.649346\tBest loss: 0.982988\tAccuracy: 19.08%\n",
      "15\tValidation loss: 1.676062\tBest loss: 0.982988\tAccuracy: 22.01%\n",
      "16\tValidation loss: 1.657656\tBest loss: 0.982988\tAccuracy: 18.73%\n",
      "17\tValidation loss: 1.678110\tBest loss: 0.982988\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.655594\tBest loss: 0.982988\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.641048\tBest loss: 0.982988\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.673525\tBest loss: 0.982988\tAccuracy: 19.27%\n",
      "21\tValidation loss: 1.633983\tBest loss: 0.982988\tAccuracy: 19.08%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=100, activation=<function elu at 0x114248840>, total=  50.7s\n",
      "[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 0.549149\tBest loss: 0.549149\tAccuracy: 76.27%\n",
      "1\tValidation loss: 1.639674\tBest loss: 0.549149\tAccuracy: 19.08%\n",
      "2\tValidation loss: 1.613329\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "3\tValidation loss: 1.636968\tBest loss: 0.549149\tAccuracy: 20.91%\n",
      "4\tValidation loss: 1.696659\tBest loss: 0.549149\tAccuracy: 19.08%\n",
      "5\tValidation loss: 1.626238\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.689894\tBest loss: 0.549149\tAccuracy: 19.08%\n",
      "7\tValidation loss: 1.623678\tBest loss: 0.549149\tAccuracy: 19.27%\n",
      "8\tValidation loss: 1.643784\tBest loss: 0.549149\tAccuracy: 19.27%\n",
      "9\tValidation loss: 1.656041\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "10\tValidation loss: 1.660336\tBest loss: 0.549149\tAccuracy: 20.91%\n",
      "11\tValidation loss: 1.617647\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.652147\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "13\tValidation loss: 1.629888\tBest loss: 0.549149\tAccuracy: 18.73%\n",
      "14\tValidation loss: 1.682820\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "15\tValidation loss: 1.637699\tBest loss: 0.549149\tAccuracy: 20.91%\n",
      "16\tValidation loss: 1.661600\tBest loss: 0.549149\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.720633\tBest loss: 0.549149\tAccuracy: 20.91%\n",
      "18\tValidation loss: 1.691965\tBest loss: 0.549149\tAccuracy: 22.01%\n",
      "19\tValidation loss: 1.629816\tBest loss: 0.549149\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.676904\tBest loss: 0.549149\tAccuracy: 20.91%\n",
      "21\tValidation loss: 1.634137\tBest loss: 0.549149\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=100, activation=<function elu at 0x114248840>, total=  50.1s\n",
      "[CV] n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=100, activation=<function elu at 0x114248840> \n",
      "0\tValidation loss: 1.255272\tBest loss: 1.255272\tAccuracy: 45.93%\n",
      "1\tValidation loss: 1.703405\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "2\tValidation loss: 1.656301\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "3\tValidation loss: 1.614034\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "4\tValidation loss: 1.649281\tBest loss: 1.255272\tAccuracy: 22.01%\n",
      "5\tValidation loss: 1.666734\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "6\tValidation loss: 1.728681\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.626946\tBest loss: 1.255272\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.655724\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "9\tValidation loss: 1.660618\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "10\tValidation loss: 1.643039\tBest loss: 1.255272\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.658842\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "12\tValidation loss: 1.631801\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "13\tValidation loss: 1.650679\tBest loss: 1.255272\tAccuracy: 19.08%\n",
      "14\tValidation loss: 1.658858\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "15\tValidation loss: 1.627393\tBest loss: 1.255272\tAccuracy: 20.91%\n",
      "16\tValidation loss: 1.693971\tBest loss: 1.255272\tAccuracy: 19.27%\n",
      "17\tValidation loss: 1.663250\tBest loss: 1.255272\tAccuracy: 19.08%\n",
      "18\tValidation loss: 1.684988\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "19\tValidation loss: 1.664343\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "20\tValidation loss: 1.644386\tBest loss: 1.255272\tAccuracy: 19.08%\n",
      "21\tValidation loss: 1.651659\tBest loss: 1.255272\tAccuracy: 18.73%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=100, learning_rate=0.02, dropout_rate=0.6, batch_size=100, activation=<function elu at 0x114248840>, total=  49.6s\n",
      "[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.487550\tBest loss: 0.487550\tAccuracy: 90.50%\n",
      "1\tValidation loss: 0.313071\tBest loss: 0.313071\tAccuracy: 91.87%\n",
      "2\tValidation loss: 0.432742\tBest loss: 0.313071\tAccuracy: 89.60%\n",
      "3\tValidation loss: 0.418456\tBest loss: 0.313071\tAccuracy: 90.15%\n",
      "4\tValidation loss: 0.415420\tBest loss: 0.313071\tAccuracy: 91.05%\n",
      "5\tValidation loss: 0.556706\tBest loss: 0.313071\tAccuracy: 90.19%\n",
      "6\tValidation loss: 0.819029\tBest loss: 0.313071\tAccuracy: 72.71%\n",
      "7\tValidation loss: 0.564644\tBest loss: 0.313071\tAccuracy: 86.94%\n",
      "8\tValidation loss: 0.557972\tBest loss: 0.313071\tAccuracy: 87.37%\n",
      "9\tValidation loss: 0.634276\tBest loss: 0.313071\tAccuracy: 82.33%\n",
      "10\tValidation loss: 0.578568\tBest loss: 0.313071\tAccuracy: 88.31%\n",
      "11\tValidation loss: 0.758397\tBest loss: 0.313071\tAccuracy: 77.48%\n",
      "12\tValidation loss: 0.666332\tBest loss: 0.313071\tAccuracy: 82.88%\n",
      "13\tValidation loss: 0.689233\tBest loss: 0.313071\tAccuracy: 81.20%\n",
      "14\tValidation loss: 0.577498\tBest loss: 0.313071\tAccuracy: 85.85%\n",
      "15\tValidation loss: 0.527506\tBest loss: 0.313071\tAccuracy: 87.57%\n",
      "16\tValidation loss: 0.650863\tBest loss: 0.313071\tAccuracy: 81.67%\n",
      "17\tValidation loss: 0.614898\tBest loss: 0.313071\tAccuracy: 84.95%\n",
      "18\tValidation loss: 0.586841\tBest loss: 0.313071\tAccuracy: 85.14%\n",
      "19\tValidation loss: 0.611174\tBest loss: 0.313071\tAccuracy: 87.37%\n",
      "20\tValidation loss: 0.538882\tBest loss: 0.313071\tAccuracy: 89.33%\n",
      "21\tValidation loss: 0.867321\tBest loss: 0.313071\tAccuracy: 72.01%\n",
      "22\tValidation loss: 0.685248\tBest loss: 0.313071\tAccuracy: 82.06%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=<function relu at 0x114259620>, total= 1.2min\n",
      "[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.650309\tBest loss: 0.650309\tAccuracy: 73.10%\n",
      "1\tValidation loss: 0.495792\tBest loss: 0.495792\tAccuracy: 87.88%\n",
      "2\tValidation loss: 0.532481\tBest loss: 0.495792\tAccuracy: 84.09%\n",
      "3\tValidation loss: 0.569103\tBest loss: 0.495792\tAccuracy: 83.11%\n",
      "4\tValidation loss: 0.462426\tBest loss: 0.462426\tAccuracy: 89.17%\n",
      "5\tValidation loss: 0.520466\tBest loss: 0.462426\tAccuracy: 86.63%\n",
      "6\tValidation loss: 0.558128\tBest loss: 0.462426\tAccuracy: 72.60%\n",
      "7\tValidation loss: 0.776551\tBest loss: 0.462426\tAccuracy: 63.80%\n",
      "8\tValidation loss: 0.709665\tBest loss: 0.462426\tAccuracy: 70.33%\n",
      "9\tValidation loss: 0.702396\tBest loss: 0.462426\tAccuracy: 72.17%\n",
      "10\tValidation loss: 1.018061\tBest loss: 0.462426\tAccuracy: 52.58%\n",
      "11\tValidation loss: 0.857541\tBest loss: 0.462426\tAccuracy: 66.46%\n",
      "12\tValidation loss: 0.894968\tBest loss: 0.462426\tAccuracy: 56.41%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\tValidation loss: 0.900971\tBest loss: 0.462426\tAccuracy: 50.94%\n",
      "14\tValidation loss: 0.974442\tBest loss: 0.462426\tAccuracy: 52.93%\n",
      "15\tValidation loss: 0.907088\tBest loss: 0.462426\tAccuracy: 55.90%\n",
      "16\tValidation loss: 1.266824\tBest loss: 0.462426\tAccuracy: 40.23%\n",
      "17\tValidation loss: 1.199741\tBest loss: 0.462426\tAccuracy: 33.54%\n",
      "18\tValidation loss: 1.201727\tBest loss: 0.462426\tAccuracy: 40.42%\n",
      "19\tValidation loss: 1.189006\tBest loss: 0.462426\tAccuracy: 40.70%\n",
      "20\tValidation loss: 1.173951\tBest loss: 0.462426\tAccuracy: 40.50%\n",
      "21\tValidation loss: 1.167526\tBest loss: 0.462426\tAccuracy: 40.30%\n",
      "22\tValidation loss: 1.184311\tBest loss: 0.462426\tAccuracy: 40.42%\n",
      "23\tValidation loss: 1.163244\tBest loss: 0.462426\tAccuracy: 40.38%\n",
      "24\tValidation loss: 1.154678\tBest loss: 0.462426\tAccuracy: 40.54%\n",
      "25\tValidation loss: 1.216409\tBest loss: 0.462426\tAccuracy: 40.30%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=<function relu at 0x114259620>, total= 1.4min\n",
      "[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=<function relu at 0x114259620> \n",
      "0\tValidation loss: 0.440500\tBest loss: 0.440500\tAccuracy: 88.90%\n",
      "1\tValidation loss: 0.563169\tBest loss: 0.440500\tAccuracy: 83.50%\n",
      "2\tValidation loss: 0.439882\tBest loss: 0.439882\tAccuracy: 89.68%\n",
      "3\tValidation loss: 0.456205\tBest loss: 0.439882\tAccuracy: 88.74%\n",
      "4\tValidation loss: 0.468300\tBest loss: 0.439882\tAccuracy: 88.35%\n",
      "5\tValidation loss: 0.482293\tBest loss: 0.439882\tAccuracy: 88.62%\n",
      "6\tValidation loss: 0.538590\tBest loss: 0.439882\tAccuracy: 85.61%\n",
      "7\tValidation loss: 0.524719\tBest loss: 0.439882\tAccuracy: 87.06%\n",
      "8\tValidation loss: 0.464246\tBest loss: 0.439882\tAccuracy: 89.84%\n",
      "9\tValidation loss: 0.573970\tBest loss: 0.439882\tAccuracy: 76.51%\n",
      "10\tValidation loss: 0.941323\tBest loss: 0.439882\tAccuracy: 58.33%\n",
      "11\tValidation loss: 0.882266\tBest loss: 0.439882\tAccuracy: 58.05%\n",
      "12\tValidation loss: 0.874446\tBest loss: 0.439882\tAccuracy: 58.17%\n",
      "13\tValidation loss: 0.857365\tBest loss: 0.439882\tAccuracy: 59.38%\n",
      "14\tValidation loss: 0.890767\tBest loss: 0.439882\tAccuracy: 57.62%\n",
      "15\tValidation loss: 0.877077\tBest loss: 0.439882\tAccuracy: 57.74%\n",
      "16\tValidation loss: 0.887945\tBest loss: 0.439882\tAccuracy: 58.05%\n",
      "17\tValidation loss: 0.881088\tBest loss: 0.439882\tAccuracy: 57.62%\n",
      "18\tValidation loss: 0.868666\tBest loss: 0.439882\tAccuracy: 58.25%\n",
      "19\tValidation loss: 1.182800\tBest loss: 0.439882\tAccuracy: 46.25%\n",
      "20\tValidation loss: 0.922772\tBest loss: 0.439882\tAccuracy: 56.22%\n",
      "21\tValidation loss: 0.996547\tBest loss: 0.439882\tAccuracy: 51.80%\n",
      "22\tValidation loss: 0.943119\tBest loss: 0.439882\tAccuracy: 55.39%\n",
      "23\tValidation loss: 0.939977\tBest loss: 0.439882\tAccuracy: 56.49%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.6, batch_size=50, activation=<function relu at 0x114259620>, total= 1.3min\n",
      "[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.3, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8> \n",
      "0\tValidation loss: 0.445774\tBest loss: 0.445774\tAccuracy: 89.37%\n",
      "1\tValidation loss: 0.594517\tBest loss: 0.445774\tAccuracy: 76.90%\n",
      "2\tValidation loss: 0.484971\tBest loss: 0.445774\tAccuracy: 84.36%\n",
      "3\tValidation loss: 1.737769\tBest loss: 0.445774\tAccuracy: 67.28%\n",
      "4\tValidation loss: 0.721004\tBest loss: 0.445774\tAccuracy: 73.42%\n",
      "5\tValidation loss: 0.524216\tBest loss: 0.445774\tAccuracy: 73.96%\n",
      "6\tValidation loss: 9.188629\tBest loss: 0.445774\tAccuracy: 34.52%\n",
      "7\tValidation loss: 0.930458\tBest loss: 0.445774\tAccuracy: 62.67%\n",
      "8\tValidation loss: 0.724275\tBest loss: 0.445774\tAccuracy: 69.35%\n",
      "9\tValidation loss: 3.188725\tBest loss: 0.445774\tAccuracy: 32.60%\n",
      "10\tValidation loss: 3.140197\tBest loss: 0.445774\tAccuracy: 43.75%\n",
      "11\tValidation loss: 4.909357\tBest loss: 0.445774\tAccuracy: 43.35%\n",
      "12\tValidation loss: 31.245270\tBest loss: 0.445774\tAccuracy: 45.58%\n",
      "13\tValidation loss: 1.162680\tBest loss: 0.445774\tAccuracy: 63.84%\n",
      "14\tValidation loss: 2.040795\tBest loss: 0.445774\tAccuracy: 47.46%\n",
      "15\tValidation loss: 1.653627\tBest loss: 0.445774\tAccuracy: 59.03%\n",
      "16\tValidation loss: 20.142824\tBest loss: 0.445774\tAccuracy: 39.56%\n",
      "17\tValidation loss: 4.365786\tBest loss: 0.445774\tAccuracy: 36.86%\n",
      "18\tValidation loss: 2.848572\tBest loss: 0.445774\tAccuracy: 45.86%\n",
      "19\tValidation loss: 1.801063\tBest loss: 0.445774\tAccuracy: 48.12%\n",
      "20\tValidation loss: 5.429787\tBest loss: 0.445774\tAccuracy: 37.41%\n",
      "21\tValidation loss: 4.630970\tBest loss: 0.445774\tAccuracy: 45.97%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.3, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>, total= 3.8min\n",
      "[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.3, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8> \n",
      "0\tValidation loss: 0.222590\tBest loss: 0.222590\tAccuracy: 93.08%\n",
      "1\tValidation loss: 0.793083\tBest loss: 0.222590\tAccuracy: 60.99%\n",
      "2\tValidation loss: 0.593737\tBest loss: 0.222590\tAccuracy: 79.48%\n",
      "3\tValidation loss: 0.444314\tBest loss: 0.222590\tAccuracy: 86.51%\n",
      "4\tValidation loss: 1.244497\tBest loss: 0.222590\tAccuracy: 49.88%\n",
      "5\tValidation loss: 0.590018\tBest loss: 0.222590\tAccuracy: 74.59%\n",
      "6\tValidation loss: 1.061385\tBest loss: 0.222590\tAccuracy: 62.16%\n",
      "7\tValidation loss: 0.641479\tBest loss: 0.222590\tAccuracy: 70.91%\n",
      "8\tValidation loss: 0.838878\tBest loss: 0.222590\tAccuracy: 62.86%\n",
      "9\tValidation loss: 1.389336\tBest loss: 0.222590\tAccuracy: 53.17%\n",
      "10\tValidation loss: 1.134461\tBest loss: 0.222590\tAccuracy: 66.03%\n",
      "11\tValidation loss: 2.932945\tBest loss: 0.222590\tAccuracy: 48.48%\n",
      "12\tValidation loss: 2.112404\tBest loss: 0.222590\tAccuracy: 41.91%\n",
      "13\tValidation loss: 1.508848\tBest loss: 0.222590\tAccuracy: 56.49%\n",
      "14\tValidation loss: 4.174708\tBest loss: 0.222590\tAccuracy: 26.39%\n",
      "15\tValidation loss: 1.177441\tBest loss: 0.222590\tAccuracy: 52.85%\n",
      "16\tValidation loss: 1.232143\tBest loss: 0.222590\tAccuracy: 55.43%\n",
      "17\tValidation loss: 1.817497\tBest loss: 0.222590\tAccuracy: 52.35%\n",
      "18\tValidation loss: 4.894596\tBest loss: 0.222590\tAccuracy: 40.97%\n",
      "19\tValidation loss: 7.268151\tBest loss: 0.222590\tAccuracy: 47.54%\n",
      "20\tValidation loss: 3.158352\tBest loss: 0.222590\tAccuracy: 50.35%\n",
      "21\tValidation loss: 29.709890\tBest loss: 0.222590\tAccuracy: 40.03%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.3, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>, total= 3.9min\n",
      "[CV] n_neurons=120, learning_rate=0.01, dropout_rate=0.3, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8> \n",
      "0\tValidation loss: 0.280394\tBest loss: 0.280394\tAccuracy: 92.14%\n",
      "1\tValidation loss: 0.352513\tBest loss: 0.280394\tAccuracy: 90.66%\n",
      "2\tValidation loss: 0.413851\tBest loss: 0.280394\tAccuracy: 84.64%\n",
      "3\tValidation loss: 35.916664\tBest loss: 0.280394\tAccuracy: 37.92%\n",
      "4\tValidation loss: 1.246031\tBest loss: 0.280394\tAccuracy: 43.78%\n",
      "5\tValidation loss: 1.184425\tBest loss: 0.280394\tAccuracy: 58.80%\n",
      "6\tValidation loss: 1.801612\tBest loss: 0.280394\tAccuracy: 34.64%\n",
      "7\tValidation loss: 2.302244\tBest loss: 0.280394\tAccuracy: 28.97%\n",
      "8\tValidation loss: 1.198271\tBest loss: 0.280394\tAccuracy: 57.23%\n",
      "9\tValidation loss: 1.389328\tBest loss: 0.280394\tAccuracy: 40.62%\n",
      "10\tValidation loss: 1.202407\tBest loss: 0.280394\tAccuracy: 51.13%\n",
      "11\tValidation loss: 1.388487\tBest loss: 0.280394\tAccuracy: 48.48%\n",
      "12\tValidation loss: 9.313537\tBest loss: 0.280394\tAccuracy: 24.55%\n",
      "13\tValidation loss: 7.562560\tBest loss: 0.280394\tAccuracy: 23.38%\n",
      "14\tValidation loss: 1.899565\tBest loss: 0.280394\tAccuracy: 31.86%\n",
      "15\tValidation loss: 1.258204\tBest loss: 0.280394\tAccuracy: 44.29%\n",
      "16\tValidation loss: 6.741638\tBest loss: 0.280394\tAccuracy: 29.91%\n",
      "17\tValidation loss: 7.628091\tBest loss: 0.280394\tAccuracy: 21.19%\n",
      "18\tValidation loss: 13.890924\tBest loss: 0.280394\tAccuracy: 26.39%\n",
      "19\tValidation loss: 4.916584\tBest loss: 0.280394\tAccuracy: 34.21%\n",
      "20\tValidation loss: 4.437963\tBest loss: 0.280394\tAccuracy: 35.30%\n",
      "21\tValidation loss: 6.798740\tBest loss: 0.280394\tAccuracy: 25.33%\n",
      "Early stopping!\n",
      "[CV]  n_neurons=120, learning_rate=0.01, dropout_rate=0.3, batch_size=10, activation=<function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>, total= 3.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 59.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.136687\tBest loss: 0.136687\tAccuracy: 96.79%\n",
      "1\tValidation loss: 0.091669\tBest loss: 0.091669\tAccuracy: 97.19%\n",
      "2\tValidation loss: 1.634807\tBest loss: 0.091669\tAccuracy: 20.91%\n",
      "3\tValidation loss: 1.685752\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "4\tValidation loss: 1.637896\tBest loss: 0.091669\tAccuracy: 19.27%\n",
      "5\tValidation loss: 1.638454\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "6\tValidation loss: 1.671915\tBest loss: 0.091669\tAccuracy: 19.27%\n",
      "7\tValidation loss: 1.649942\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "8\tValidation loss: 1.706077\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "9\tValidation loss: 1.648731\tBest loss: 0.091669\tAccuracy: 18.73%\n",
      "10\tValidation loss: 1.642693\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "11\tValidation loss: 1.688579\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "12\tValidation loss: 1.667935\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "13\tValidation loss: 1.691968\tBest loss: 0.091669\tAccuracy: 18.73%\n",
      "14\tValidation loss: 1.676282\tBest loss: 0.091669\tAccuracy: 19.27%\n",
      "15\tValidation loss: 1.620430\tBest loss: 0.091669\tAccuracy: 19.27%\n",
      "16\tValidation loss: 1.705971\tBest loss: 0.091669\tAccuracy: 19.08%\n",
      "17\tValidation loss: 1.858445\tBest loss: 0.091669\tAccuracy: 19.27%\n",
      "18\tValidation loss: 1.815565\tBest loss: 0.091669\tAccuracy: 19.08%\n",
      "19\tValidation loss: 1.664230\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "20\tValidation loss: 1.653978\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "21\tValidation loss: 1.634153\tBest loss: 0.091669\tAccuracy: 22.01%\n",
      "22\tValidation loss: 1.675761\tBest loss: 0.091669\tAccuracy: 18.73%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=DNNClassifier(activation=<function elu at 0x114248840>,\n",
       "                                           batch_norm_momentum=None,\n",
       "                                           batch_size=20, dropout_rate=None,\n",
       "                                           initializer=<tensorflow.python.ops.init_ops.VarianceScaling object at 0x1247673c8>,\n",
       "                                           learning_rate=0.01,\n",
       "                                           n_hidden_layers=5, n_neurons=100,\n",
       "                                           optimizer_class=<class 'tensorflow.python.tra...\n",
       "                                                       <function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344ebf8>,\n",
       "                                                       <function leaky_relu.<locals>.parametrized_leaky_relu at 0x14344e6a8>],\n",
       "                                        'batch_size': [10, 50, 100, 500],\n",
       "                                        'dropout_rate': [0.2, 0.3, 0.4, 0.5,\n",
       "                                                         0.6],\n",
       "                                        'learning_rate': [0.01, 0.02, 0.05,\n",
       "                                                          0.1],\n",
       "                                        'n_neurons': [10, 30, 50, 70, 90, 100,\n",
       "                                                      120, 140, 160]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {\"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "                  \"batch_size\": [10, 50, 100, 500],\n",
    "                  \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "                  \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "                  \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6]}\n",
    "                  # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "                  # \"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "                  # \"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)]\n",
    "rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs,\n",
    "                                        n_iter=10, cv=3, random_state=42, verbose=2) # default: n_iter=50\n",
    "rnd_search_dropout.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, dropout should bring an improvement. But the discontinuation of batch normalization might be an issue because batch normalization is really important. Also, the randomized parameter search might just be unlucky. Increasing `n_iter` increases the likelihood to find a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 120, 'learning_rate': 0.02, 'dropout_rate': 0.2, 'batch_size': 100, 'activation': <function elu at 0x114248840>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9785950574041642"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rnd_search_dropout.best_params_)\n",
    "y_pred = rnd_search_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that shall be it. Let's continue with the next task!\n",
    "### 9. Transfer Learning.\n",
    "- Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
    "- Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "- Try caching the frozen layers, and train the model again: how much faster is it now?\n",
    "- Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "- Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\n",
    "\n",
    "We start with the first two tasks in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.786745\tBest loss: 0.786745\tAccuracy: 72.67%\n",
      "1\tValidation loss: 0.727068\tBest loss: 0.727068\tAccuracy: 77.33%\n",
      "2\tValidation loss: 0.671649\tBest loss: 0.671649\tAccuracy: 78.67%\n",
      "3\tValidation loss: 0.689113\tBest loss: 0.671649\tAccuracy: 72.00%\n",
      "4\tValidation loss: 0.732169\tBest loss: 0.671649\tAccuracy: 74.67%\n",
      "5\tValidation loss: 0.750821\tBest loss: 0.671649\tAccuracy: 67.33%\n",
      "6\tValidation loss: 0.741337\tBest loss: 0.671649\tAccuracy: 72.00%\n",
      "7\tValidation loss: 0.605306\tBest loss: 0.605306\tAccuracy: 79.33%\n",
      "8\tValidation loss: 0.757117\tBest loss: 0.605306\tAccuracy: 68.67%\n",
      "9\tValidation loss: 0.612283\tBest loss: 0.605306\tAccuracy: 78.00%\n",
      "10\tValidation loss: 0.673644\tBest loss: 0.605306\tAccuracy: 76.00%\n",
      "11\tValidation loss: 0.726546\tBest loss: 0.605306\tAccuracy: 74.67%\n",
      "12\tValidation loss: 0.733611\tBest loss: 0.605306\tAccuracy: 75.33%\n",
      "Early stopping!\n",
      "Total training time: 1.6s\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 71.18%\n"
     ]
    }
   ],
   "source": [
    "# github everything but comments\n",
    "# reset the graph and restore the best model from the previous exercise\n",
    "reset_graph()\n",
    "restore_saver=tf.train.import_meta_graph(\"./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4.meta\") # path adapted\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0] # logits are not restored but built \"from scratch\"\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")\n",
    "# building the training operation\n",
    "# !!! note that only the \"TRAINABLE_VARIABLES\" of the \"logits\" scope (output layer) are taken into account, thus ...\n",
    "# ... effectively freezing everything below (as demanded by the exercise)\n",
    "learning_rate = 0.01\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "# build the new remaining parts, including a new \"saver\"\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()\n",
    "# for digits 5-9, subtract 5 so the labels are integers starting from 0 (0-4), as expected by tensorflow; but ...\n",
    "# ... this does not change the classification: the network will be retrained and thus is \"new\": it does not mix up ...\n",
    "# ... actual \"3s\" with \"8s\", for example\n",
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5\n",
    "# make batches of n (default: n=100) instances and return both features (X) and labels (y)\n",
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)\n",
    "# use the above function to get training of 100 instances and validation data for 30 instances\n",
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)\n",
    "# learning schedule\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 5 # adapted from default 20 for brevity\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # restore the best model from the previous exercise (exercise 8)\n",
    "    restore_saver.restore(sess, \"./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\")\n",
    "    # time keeping\n",
    "    t0 = time.time()\n",
    "    # loop through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # make randomized batches of the (actually very small) training data and feed them one by one for training\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            # do the training\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        # check performance on validation set after training, log the model, and update quantities for early stopping\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        # if applicable, stop early\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        # still in the current epoch, print the accuracy on the validation set\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "    # when done, print the time it took until now\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "# restore the best model and check how it performs on the test set\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further speed-up, cache the frozen layer (calculate their output only once and feed this output when needed instead of calculating it from scratch every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.943396\tBest loss: 0.943396\tAccuracy: 64.67%\n",
      "1\tValidation loss: 0.780935\tBest loss: 0.780935\tAccuracy: 72.67%\n",
      "2\tValidation loss: 0.722074\tBest loss: 0.722074\tAccuracy: 74.67%\n",
      "3\tValidation loss: 0.884344\tBest loss: 0.722074\tAccuracy: 66.67%\n",
      "4\tValidation loss: 0.793261\tBest loss: 0.722074\tAccuracy: 71.33%\n",
      "5\tValidation loss: 0.690417\tBest loss: 0.690417\tAccuracy: 78.00%\n",
      "6\tValidation loss: 0.687980\tBest loss: 0.687980\tAccuracy: 74.67%\n",
      "7\tValidation loss: 0.634309\tBest loss: 0.634309\tAccuracy: 77.33%\n",
      "8\tValidation loss: 0.663460\tBest loss: 0.634309\tAccuracy: 73.33%\n",
      "9\tValidation loss: 0.645926\tBest loss: 0.634309\tAccuracy: 79.33%\n",
      "10\tValidation loss: 0.659033\tBest loss: 0.634309\tAccuracy: 76.00%\n",
      "11\tValidation loss: 0.668498\tBest loss: 0.634309\tAccuracy: 74.67%\n",
      "12\tValidation loss: 0.672734\tBest loss: 0.634309\tAccuracy: 78.67%\n",
      "Early stopping!\n",
      "Total training time: 1.7s\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_five_frozen\n",
      "Final test accuracy: 69.43%\n"
     ]
    }
   ],
   "source": [
    "# get the fifth layer (see definition of \"dnn_clf\" at the beginning of exercise 8)\n",
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")\n",
    "# learning schedule\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 5 # adapted from default 20 for brevity\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "# run session; unless commented, things are basically as before\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\")\n",
    "    t0 = time.time()\n",
    "    # cache output (both for training and validation sets) of the 5th hidden layer\n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices] # get cached output of layer 5 ...\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})  # ... (and labels); run training\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the third task. Indeed, caching the frozen layers did speed up the training significantly.<br>\n",
    "For the fourth task, we shall load the model again but reuse only layer 1 through 4 and finish off with a fully connected layer using softmax output on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\n",
      "0\tValidation loss: 0.757380\tBest loss: 0.757380\tAccuracy: 73.33%\n",
      "1\tValidation loss: 0.801554\tBest loss: 0.757380\tAccuracy: 74.00%\n",
      "2\tValidation loss: 0.608344\tBest loss: 0.608344\tAccuracy: 82.00%\n",
      "3\tValidation loss: 0.626240\tBest loss: 0.608344\tAccuracy: 80.00%\n",
      "4\tValidation loss: 0.711040\tBest loss: 0.608344\tAccuracy: 76.67%\n",
      "5\tValidation loss: 0.652290\tBest loss: 0.608344\tAccuracy: 72.67%\n",
      "6\tValidation loss: 0.677738\tBest loss: 0.608344\tAccuracy: 80.00%\n",
      "7\tValidation loss: 0.562025\tBest loss: 0.562025\tAccuracy: 80.00%\n",
      "8\tValidation loss: 0.613854\tBest loss: 0.562025\tAccuracy: 78.00%\n",
      "9\tValidation loss: 0.547622\tBest loss: 0.547622\tAccuracy: 83.33%\n",
      "10\tValidation loss: 0.624438\tBest loss: 0.547622\tAccuracy: 79.33%\n",
      "11\tValidation loss: 0.677553\tBest loss: 0.547622\tAccuracy: 76.00%\n",
      "12\tValidation loss: 0.693222\tBest loss: 0.547622\tAccuracy: 77.33%\n",
      "13\tValidation loss: 0.686333\tBest loss: 0.547622\tAccuracy: 80.00%\n",
      "14\tValidation loss: 0.579084\tBest loss: 0.547622\tAccuracy: 79.33%\n",
      "15\tValidation loss: 0.544784\tBest loss: 0.544784\tAccuracy: 83.33%\n",
      "16\tValidation loss: 0.603957\tBest loss: 0.544784\tAccuracy: 80.00%\n",
      "17\tValidation loss: 0.677100\tBest loss: 0.544784\tAccuracy: 73.33%\n",
      "18\tValidation loss: 0.603562\tBest loss: 0.544784\tAccuracy: 80.00%\n",
      "19\tValidation loss: 0.558580\tBest loss: 0.544784\tAccuracy: 81.33%\n",
      "20\tValidation loss: 0.551155\tBest loss: 0.544784\tAccuracy: 80.67%\n",
      "21\tValidation loss: 0.533252\tBest loss: 0.533252\tAccuracy: 82.67%\n",
      "22\tValidation loss: 0.574093\tBest loss: 0.533252\tAccuracy: 80.67%\n",
      "23\tValidation loss: 0.581057\tBest loss: 0.533252\tAccuracy: 80.67%\n",
      "24\tValidation loss: 0.594089\tBest loss: 0.533252\tAccuracy: 77.33%\n",
      "25\tValidation loss: 0.604753\tBest loss: 0.533252\tAccuracy: 74.67%\n",
      "26\tValidation loss: 0.595235\tBest loss: 0.533252\tAccuracy: 81.33%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_four_frozen\n",
      "Final test accuracy: 77.91%\n"
     ]
    }
   ],
   "source": [
    "# basically as before unless commented\n",
    "reset_graph()\n",
    "n_outputs = 5\n",
    "restore_saver = tf.train.import_meta_graph(\"./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4.meta\")\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"hidden4_out:0\")\n",
    "# after the fourth layer, use a fully connected layer with softmax output\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "# basically as before\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "learning_rate = 0.01\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver() # new saver\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 5 # adapted from default 20 for brevity\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "with tf.Session() as sess: # no timekeeping\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./tf_logs/11_Training/Ex_8/my_best_mnist_model_0_to_4\")\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2)) # no caching\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, # use the new aaver\n",
    "                                               \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_four_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "with tf.Session() as sess: # use the new aaver\n",
    "    four_frozen_saver.restore(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_four_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last task: unfreeze the two top layers and retrain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_four_frozen\n",
      "0\tValidation loss: 0.951549\tBest loss: 0.951549\tAccuracy: 77.33%\n",
      "1\tValidation loss: 0.497489\tBest loss: 0.497489\tAccuracy: 82.00%\n",
      "2\tValidation loss: 0.526501\tBest loss: 0.497489\tAccuracy: 83.33%\n",
      "3\tValidation loss: 0.453755\tBest loss: 0.453755\tAccuracy: 86.00%\n",
      "4\tValidation loss: 0.447970\tBest loss: 0.447970\tAccuracy: 84.67%\n",
      "5\tValidation loss: 0.414435\tBest loss: 0.414435\tAccuracy: 86.00%\n",
      "6\tValidation loss: 0.423408\tBest loss: 0.414435\tAccuracy: 88.00%\n",
      "7\tValidation loss: 0.449138\tBest loss: 0.414435\tAccuracy: 88.00%\n",
      "8\tValidation loss: 0.868403\tBest loss: 0.414435\tAccuracy: 80.00%\n",
      "9\tValidation loss: 0.559651\tBest loss: 0.414435\tAccuracy: 86.00%\n",
      "10\tValidation loss: 0.508976\tBest loss: 0.414435\tAccuracy: 88.00%\n",
      "11\tValidation loss: 0.398155\tBest loss: 0.398155\tAccuracy: 89.33%\n",
      "12\tValidation loss: 0.466651\tBest loss: 0.398155\tAccuracy: 89.33%\n",
      "13\tValidation loss: 0.449792\tBest loss: 0.398155\tAccuracy: 87.33%\n",
      "14\tValidation loss: 0.595343\tBest loss: 0.398155\tAccuracy: 88.00%\n",
      "15\tValidation loss: 0.866855\tBest loss: 0.398155\tAccuracy: 84.67%\n",
      "16\tValidation loss: 0.732987\tBest loss: 0.398155\tAccuracy: 85.33%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_two_frozen\n",
      "Final test accuracy: 85.35%\n"
     ]
    }
   ],
   "source": [
    "# comments only where necessary\n",
    "learning_rate = 0.01\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, # use regular expression to get the two top layers\n",
    "                                  scope=\"hidden[34]|new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n",
    "training_op = optimizer.minimize(loss,var_list=unfrozen_vars) # trainer for top two layers; see \"Args\" under the link:\n",
    "                                                       # https://www.tensorflow.org/api_docs/python/tf/train/Optimizer\n",
    "init = tf.global_variables_initializer()\n",
    "two_frozen_saver = tf.train.Saver() # new saver\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 5 # adapted from default 20 for brevity\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    four_frozen_saver.restore(sess, # restore with former saver\n",
    "                              \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_four_frozen\")\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            # save with new saver\n",
    "            save_path = two_frozen_saver.save(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_two_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "with tf.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_two_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also unfreeze and retrain everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_two_frozen\n",
      "0\tValidation loss: 0.946693\tBest loss: 0.946693\tAccuracy: 68.00%\n",
      "1\tValidation loss: 0.370303\tBest loss: 0.370303\tAccuracy: 87.33%\n",
      "2\tValidation loss: 0.363243\tBest loss: 0.363243\tAccuracy: 90.00%\n",
      "3\tValidation loss: 0.571133\tBest loss: 0.363243\tAccuracy: 89.33%\n",
      "4\tValidation loss: 0.479860\tBest loss: 0.363243\tAccuracy: 90.00%\n",
      "5\tValidation loss: 0.885732\tBest loss: 0.363243\tAccuracy: 83.33%\n",
      "6\tValidation loss: 1.668353\tBest loss: 0.363243\tAccuracy: 79.33%\n",
      "7\tValidation loss: 2.226420\tBest loss: 0.363243\tAccuracy: 74.00%\n",
      "Early stopping!\n",
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_no_frozen\n",
      "Final test accuracy: 88.99%\n"
     ]
    }
   ],
   "source": [
    "# comments only where appropriate\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam4\")\n",
    "training_op = optimizer.minimize(loss) # (re-)train everything\n",
    "init = tf.global_variables_initializer()\n",
    "no_frozen_saver = tf.train.Saver() # new saver\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "max_checks_without_progress = 5 # adapted from default 20 for brevity\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # restore previous model\n",
    "    two_frozen_saver.restore(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_two_frozen\")\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            # save with new saver\n",
    "            save_path = no_frozen_saver.save(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_no_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "with tf.Session() as sess:\n",
    "    no_frozen_saver.restore(sess, \"./tf_logs/11_Training/Ex_9/my_mnist_model_5_to_9_no_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result fo far. Let's compare it to a DNN trained from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tValidation loss: 0.647539\tBest loss: 0.647539\tAccuracy: 77.33%\n",
      "1\tValidation loss: 0.495276\tBest loss: 0.495276\tAccuracy: 87.33%\n",
      "2\tValidation loss: 0.523561\tBest loss: 0.495276\tAccuracy: 87.33%\n",
      "3\tValidation loss: 0.561938\tBest loss: 0.495276\tAccuracy: 90.00%\n",
      "4\tValidation loss: 0.747551\tBest loss: 0.495276\tAccuracy: 90.67%\n",
      "5\tValidation loss: 0.555371\tBest loss: 0.495276\tAccuracy: 81.33%\n",
      "6\tValidation loss: 0.669283\tBest loss: 0.495276\tAccuracy: 90.67%\n",
      "7\tValidation loss: 0.604886\tBest loss: 0.495276\tAccuracy: 86.67%\n",
      "8\tValidation loss: 1.084157\tBest loss: 0.495276\tAccuracy: 85.33%\n",
      "9\tValidation loss: 1.497895\tBest loss: 0.495276\tAccuracy: 73.33%\n",
      "10\tValidation loss: 9.466480\tBest loss: 0.495276\tAccuracy: 80.00%\n",
      "11\tValidation loss: 6.965297\tBest loss: 0.495276\tAccuracy: 82.67%\n",
      "12\tValidation loss: 11.336102\tBest loss: 0.495276\tAccuracy: 86.67%\n",
      "13\tValidation loss: 10.460723\tBest loss: 0.495276\tAccuracy: 90.67%\n",
      "14\tValidation loss: 4.851905\tBest loss: 0.495276\tAccuracy: 72.00%\n",
      "15\tValidation loss: 5.485624\tBest loss: 0.495276\tAccuracy: 90.00%\n",
      "16\tValidation loss: 6.054422\tBest loss: 0.495276\tAccuracy: 91.33%\n",
      "17\tValidation loss: 6.651939\tBest loss: 0.495276\tAccuracy: 90.67%\n",
      "18\tValidation loss: 7.734695\tBest loss: 0.495276\tAccuracy: 91.33%\n",
      "19\tValidation loss: 7.287059\tBest loss: 0.495276\tAccuracy: 91.33%\n",
      "20\tValidation loss: 7.545192\tBest loss: 0.495276\tAccuracy: 90.67%\n",
      "21\tValidation loss: 8.272553\tBest loss: 0.495276\tAccuracy: 90.00%\n",
      "22\tValidation loss: 7.456333\tBest loss: 0.495276\tAccuracy: 89.33%\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.843859288212302"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\n",
    "dnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)\n",
    "y_pred = dnn_clf_5_to_9.predict(X_test2)\n",
    "accuracy_score(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transferred and retrained model works significantly better, at very similar training time. So transfer learning can definitely be helpful.\n",
    "### 10. Pretraining on an auxiliary task.\n",
    "- In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let's call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add a single output layer on top of both DNNs. You should use TensorFlow's `concat()` function with `axis=1` to concateneate the outputs of both DNNs along the horizontal axis, then feed the result to the output layer. This output layer should contain a single neuron using the logistic activation function.\n",
    "- Split the MNIST training set in two sets: split #1 should contain 55000 images, and split #2 should contain 5000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be imagess from different classes, For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
    "- Train the DNN on this training set. For each image pair, you can simultaneously feed te first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
    "- Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer with 10 neurons on top. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.\n",
    "\n",
    "As pointed out on github, a hidden top layer is going to be required. Otherwise, the network might not start training. This circumstance is not mentioned in the above task (which has been taken from the book).<br>\n",
    "We start with the first and second task: feed two (batches of) MNIST instances to two separate networks, have the networks interpret them, and then stack calculations on top of both outputs to decide whether they represent the same digits. We also display a small batch and establish the labels (no neural network output, yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100)\n",
      "(?, 100)\n",
      "(?, 200)\n",
      "(5, 2, 784) float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### first task\n",
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "# use the function \"dnn\" for constructing deep neural nets (defined further above)\n",
    "dnn1 = dnn(X1, name=\"DNN_A\")\n",
    "dnn2 = dnn(X2, name=\"DNN_B\")\n",
    "# combine the outputs in one list and check that it works (via .shape)\n",
    "dnn_outputs = tf.concat([dnn1, dnn2], axis=1)\n",
    "print(dnn1.shape)\n",
    "print(dnn2.shape)\n",
    "print(dnn_outputs.shape)\n",
    "# add an extra hidden layer with 10 neurons and an output layer with 1 neuron (sigmoid); for details, see ...\n",
    "# ... https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)\n",
    "# the network classifies according to y_proba > 0.5 (True or False) but this is equivalent to (and slower than) ...\n",
    "# ... logits > 0 (True or False); so for classification, we use the faster logits\n",
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n",
    "# we still need \"y_proba\" to calculate the loss function (cross entropy)\n",
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "# training details\n",
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "# accuracy\n",
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))\n",
    "# initializer and saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "### second task\n",
    "# assign training sets and test set\n",
    "X_train1 = X_train\n",
    "y_train1 = y_train\n",
    "X_train2 = X_valid\n",
    "y_train2 = y_valid\n",
    "X_test = X_test\n",
    "y_test = y_test\n",
    "# function for batches\n",
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]\n",
    "# test the function\n",
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "print(X_batch.shape, X_batch.dtype)\n",
    "# show examples\n",
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# and check whether the labels match\n",
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels work: different numbers have \"0\" label and equal numbers have a \"1\" label. Now, proceed to task 3 and train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.6923601\n",
      "0 Test accuracy: 0.5031\n",
      "1 Train loss: 0.6937516\n",
      "2 Train loss: 0.68896145\n",
      "3 Train loss: 0.6279489\n",
      "4 Train loss: 0.52220035\n",
      "5 Train loss: 0.53943956\n",
      "5 Test accuracy: 0.7317\n",
      "6 Train loss: 0.540305\n",
      "7 Train loss: 0.4539397\n",
      "8 Train loss: 0.45064873\n",
      "9 Train loss: 0.46825847\n",
      "10 Train loss: 0.35033742\n",
      "10 Test accuracy: 0.8226\n",
      "11 Train loss: 0.41334054\n",
      "12 Train loss: 0.36943892\n",
      "13 Train loss: 0.3761535\n",
      "14 Train loss: 0.32001543\n",
      "15 Train loss: 0.3276174\n",
      "15 Test accuracy: 0.8599\n"
     ]
    }
   ],
   "source": [
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))\n",
    "n_epochs = 16 # changed from default 100 for brevity\n",
    "batch_size = 500\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train loss:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "    save_path = saver.save(sess, \"./tf_logs/11_Training/Ex_10/my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fourth task, restore the previous model and freeze its lower layers via the `tf.stop_gradient()` function. Train the output layer on only 500 images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_logs/11_Training/Ex_10/my_digit_comparison_model.ckpt\n",
      "0 Test accuracy: 0.3087\n",
      "10 Test accuracy: 0.7337\n",
      "20 Test accuracy: 0.7637\n",
      "30 Test accuracy: 0.7821\n",
      "40 Test accuracy: 0.7881\n",
      "50 Test accuracy: 0.7922\n",
      "60 Test accuracy: 0.7959\n",
      "70 Test accuracy: 0.8\n",
      "80 Test accuracy: 0.8022\n",
      "90 Test accuracy: 0.8041\n",
      "100 Test accuracy: 0.8067\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "logits = tf.layers.dense(frozen_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 101 # adapted from default 100\n",
    "batch_size = 50\n",
    "X_train3 = X_train2[:500] # own code: use only 500 instances\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./tf_logs/11_Training/Ex_10/my_digit_comparison_model.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train3)) # own code: use X_train3\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train3) // batch_size): # own code: use X_train3\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "    # \"saver\" shown on github but deleted here (deleted only in this session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the above model with a new network trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.4664\n",
      "10 Test accuracy: 0.8242\n",
      "20 Test accuracy: 0.8286\n",
      "30 Test accuracy: 0.8287\n",
      "40 Test accuracy: 0.8278\n",
      "50 Test accuracy: 0.8274\n",
      "60 Test accuracy: 0.8275\n",
      "70 Test accuracy: 0.8273\n",
      "80 Test accuracy: 0.8271\n",
      "90 Test accuracy: 0.8272\n",
      "100 Test accuracy: 0.827\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "# \"saver\" shown on github but deleted here and in session\n",
    "n_epochs = 101 # adapted from default 150 (for better comparison with above model)\n",
    "batch_size = 50\n",
    "X_train3 = X_train2[:500] # own code: use only 500 instances\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train3)) # own code: use X_train3\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train3) // batch_size): # own code: use X_train3\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the newly trained model performs better!<br>\n",
    "Bottom line: **transfer learning** can be **extremely useful** but it is **not guaranteed to work better** for every project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
